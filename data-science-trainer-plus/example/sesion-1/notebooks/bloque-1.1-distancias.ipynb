{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "md_d4fb6b",
   "metadata": {},
   "source": "# Bloque 1.1 ‚Äî Fundamentos y M√©tricas de Distancia\n**M√°ster en Ciencia de Datos ¬∑ M√≥dulo: Algoritmos de Clustering**\n**Sesi√≥n 1 ¬∑ Duraci√≥n: 55 min**\n\n---\n> üìå **C√≥mo usar este notebook:**\n> Ejecuta las celdas **en orden**. Cada secci√≥n comienza con explicaci√≥n te√≥rica (en Markdown) seguida del c√≥digo correspondiente.\n> Los comentarios `# ---` delimitan ejercicios opcionales para profundizar.\n"
  },
  {
   "cell_type": "markdown",
   "id": "md_a3e26a",
   "metadata": {},
   "source": "## üîß Setup y verificaci√≥n del entorno"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "code_b81cdd",
   "metadata": {},
   "outputs": [],
   "source": "# ============================================================\n# SETUP ‚Äî ejecutar siempre en primer lugar\n# ============================================================\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Verificar librer√≠as clave\nimport importlib, sys\n\nrequired = {\n    'numpy': 'numpy',\n    'pandas': 'pandas',\n    'matplotlib': 'matplotlib',\n    'seaborn': 'seaborn',\n    'sklearn': 'scikit-learn',\n    'scipy': 'scipy',\n}\n\noptional = {\n    'sklearn_extra': 'scikit-learn-extra  # pip install scikit-learn-extra',\n    'minisom': 'minisom               # pip install minisom',\n    'umap': 'umap-learn             # pip install umap-learn',\n    'hdbscan': 'hdbscan               # pip install hdbscan',\n    'yellowbrick': 'yellowbrick          # pip install yellowbrick',\n}\n\nprint(\"Librer√≠as requeridas:\")\nfor mod, pkg in required.items():\n    ok = importlib.util.find_spec(mod) is not None\n    print(f\"  {'‚úÖ' if ok else '‚ùå'} {pkg}\")\n\nprint(\"\\nLibrer√≠as opcionales:\")\nfor mod, pkg in optional.items():\n    ok = importlib.util.find_spec(mod) is not None\n    print(f\"  {'‚úÖ' if ok else '‚ö†Ô∏è '} {pkg}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "code_d8ef42",
   "metadata": {},
   "outputs": [],
   "source": "# ============================================================\n# BLOQUE 1.1 ‚Äî Introducci√≥n al Clustering y Espacio de Datos\n# M√°ster en Ciencia de Datos\n# ============================================================\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom scipy.spatial import distance\n\n# Generadores de datos sint√©ticos\nfrom sklearn.datasets import make_blobs, make_moons, make_circles\n\n# Configuraci√≥n visual\nplt.rcParams['figure.figsize'] = (10, 6)\nplt.rcParams['font.size'] = 12\nsns.set_style(\"whitegrid\")\nnp.random.seed(42)\n\nprint(\"‚úì Librer√≠as cargadas correctamente\")"
  },
  {
   "cell_type": "markdown",
   "id": "md_c55995",
   "metadata": {},
   "source": "**Nota al instructor:** Aprovechar esta celda para verificar que todos los alumnos tienen el entorno funcionando. Pedir que ejecuten y confirmen el mensaje de OK.\n\n---\n\n#### Celda 2 ‚Äî Datasets sint√©ticos: los tres escenarios cl√°sicos"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "code_9f97f1",
   "metadata": {},
   "outputs": [],
   "source": "# Generamos tres tipos de distribuciones de datos\n# Cada una plantea un reto diferente para los algoritmos de clustering\n\n# Escenario A: Blobs bien separados (el caso \"f√°cil\")\nX_blobs, y_blobs = make_blobs(n_samples=300, centers=4, cluster_std=0.8)\n\n# Escenario B: Lunas entrelazadas (clusters no convexos)\nX_moons, y_moons = make_moons(n_samples=300, noise=0.05)\n\n# Escenario C: C√≠rculos conc√©ntricos (clusters anidados)\nX_circles, y_circles = make_circles(n_samples=300, noise=0.05, factor=0.5)\n\n# Visualizaci√≥n\nfig, axes = plt.subplots(1, 3, figsize=(15, 5))\n\ndatasets = [\n    (X_blobs,   y_blobs,   \"A ‚Äî Blobs separados\\n(caso ideal para K-Means)\"),\n    (X_moons,   y_moons,   \"B ‚Äî Lunas entrelazadas\\n(requiere densidad o kernels)\"),\n    (X_circles, y_circles, \"C ‚Äî C√≠rculos conc√©ntricos\\n(K-Means fallar√° aqu√≠)\")\n]\n\nfor ax, (X, y, title) in zip(axes, datasets):\n    scatter = ax.scatter(X[:, 0], X[:, 1], c=y, cmap='tab10', alpha=0.7, s=30)\n    ax.set_title(title, fontsize=11)\n    ax.set_xlabel(\"Caracter√≠stica 1\")\n    ax.set_ylabel(\"Caracter√≠stica 2\")\n\nplt.suptitle(\"Tres morfolog√≠as de datos ‚Äî ¬øUn solo algoritmo puede con todos?\",\n             fontsize=13, fontweight='bold')\nplt.tight_layout()\nplt.savefig(\"img_datasets_sinteticos.png\", dpi=150, bbox_inches='tight')\nplt.show()"
  },
  {
   "cell_type": "markdown",
   "id": "md_44deba",
   "metadata": {},
   "source": "**Script de explicaci√≥n para el instructor:**\n\n*\"Fijaos en estos tres escenarios. El escenario A es el sue√±o de cualquier algoritmo: los grupos son compactos, esf√©ricos y bien separados. Aqu√≠ cualquier m√©todo funciona. El B y el C son m√°s interesantes: tienen estructura clara (los humanos los identificamos perfectamente), pero esa estructura no es esf√©rica. En los pr√≥ximos bloques vamos a ver qu√© algoritmos fallan aqu√≠ y por qu√©.\"*\n\n*\"Cuando trabaj√©is con datos reales nunca sabr√©is en qu√© escenario est√°is. Por eso el primer paso siempre es visualizar ‚Äîsi la dimensionalidad lo permite‚Äî y entender la forma de los datos.\"*\n\n---\n\n#### Celda 3 ‚Äî El impacto de la escala en las distancias"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "code_9ef6b1",
   "metadata": {},
   "outputs": [],
   "source": "# Demostraci√≥n: por qu√© hay que normalizar antes de clusterizar\n\n# Dataset artificial: dos clientes descritos por edad e ingresos anuales\n# Cliente A: 25 a√±os, 25.000‚Ç¨ anuales\n# Cliente B: 26 a√±os, 80.000‚Ç¨ anuales\n# Cliente C: 55 a√±os, 27.000‚Ç¨ anuales\n# ¬øQui√©n est√° m√°s \"cerca\" de A?\n\nclientes = np.array([\n    [25,  25000],   # Cliente A (referencia)\n    [26,  80000],   # Cliente B (1 a√±o m√°s, mucho m√°s rico)\n    [55,  27000],   # Cliente C (30 a√±os m√°s, ingresos similares)\n])\n\nnombres = [\"A (referencia)\", \"B\", \"C\"]\n\n# Calculamos distancias euclidianas sin normalizar\nprint(\"=== Distancias EUCLIDIANAS (sin normalizar) ===\")\nd_AB = distance.euclidean(clientes[0], clientes[1])\nd_AC = distance.euclidean(clientes[0], clientes[2])\nprint(f\"  d(A, B) = {d_AB:,.0f}  ‚Üê 1 a√±o de diferencia, 55k‚Ç¨ de diferencia\")\nprint(f\"  d(A, C) = {d_AC:,.0f}  ‚Üê 30 a√±os de diferencia, 2k‚Ç¨ de diferencia\")\nprint(f\"\\n  ‚Üí Seg√∫n distancia euclidiana, B est√° m√°s cerca de A que C\")\nprint(f\"  ‚Üí ¬øTiene sentido para negocio? B tiene ingresos 3x mayores que A...\")\n\n# Normalizamos con StandardScaler\nfrom sklearn.preprocessing import StandardScaler\n\nscaler = StandardScaler()\nclientes_norm = scaler.fit_transform(clientes)\n\nprint(\"\\n=== Distancias EUCLIDIANAS (tras normalizaci√≥n z-score) ===\")\nd_AB_n = distance.euclidean(clientes_norm[0], clientes_norm[1])\nd_AC_n = distance.euclidean(clientes_norm[0], clientes_norm[1])\nd_AB_n = distance.euclidean(clientes_norm[0], clientes_norm[1])\nd_AC_n = distance.euclidean(clientes_norm[0], clientes_norm[2])\nprint(f\"  d(A, B) normalizada = {d_AB_n:.3f}\")\nprint(f\"  d(A, C) normalizada = {d_AC_n:.3f}\")\nprint(f\"\\n  ‚Üí Ahora C est√° m√°s cerca de A (comparten nivel de ingresos)\")\nprint(f\"  ‚Üí La normalizaci√≥n restaura el balance entre variables\")"
  },
  {
   "cell_type": "markdown",
   "id": "md_cb7446",
   "metadata": {},
   "source": "**Script de explicaci√≥n:**\n\n*\"Este ejemplo parece trivial pero es uno de los errores m√°s frecuentes en proyectos reales. La distancia euclidiana sin normalizar est√° completamente dominada por los ingresos porque tienen una escala 1000 veces mayor que la edad. Al normalizar, ambas variables contribuyen de forma equilibrada.\"*\n\n*\"La pregunta que deb√©is haceros siempre antes de clusterizar: '¬øMis variables est√°n en la misma escala? ¬øQuiero que contribuyan por igual?' Si la respuesta es s√≠, normalizad. Si quereis que una variable tenga m√°s peso, pod√©is escalarla con un factor diferente.\"*\n\n---\n\n#### Celda 4 ‚Äî Visualizaci√≥n de matrices de distancia"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "code_eae1c7",
   "metadata": {},
   "outputs": [],
   "source": "# Las matrices de distancia revelan la estructura de los datos\n# antes de aplicar ning√∫n algoritmo\n\n# Usamos los blobs (caso simple) para ver qu√© aspecto tiene una matriz \"buena\"\nfrom sklearn.preprocessing import StandardScaler\n\nX_blobs_norm = StandardScaler().fit_transform(X_blobs)\n\n# Calculamos la matriz de distancias\ndist_matrix = distance.cdist(X_blobs_norm[:80], X_blobs_norm[:80], metric='euclidean')\n\n# Ordenamos por etiqueta real para ver la estructura de bloque\nidx_sorted = np.argsort(y_blobs[:80])\ndist_sorted = dist_matrix[np.ix_(idx_sorted, idx_sorted)]\n\nfig, axes = plt.subplots(1, 2, figsize=(14, 6))\n\n# Sin ordenar\nim1 = axes[0].imshow(dist_matrix, cmap='viridis_r', aspect='auto')\naxes[0].set_title(\"Matriz de distancias\\n(puntos en orden original)\", fontsize=11)\naxes[0].set_xlabel(\"√çndice de punto\")\naxes[0].set_ylabel(\"√çndice de punto\")\nplt.colorbar(im1, ax=axes[0], label=\"Distancia euclidiana\")\n\n# Ordenada por cluster real\nim2 = axes[1].imshow(dist_sorted, cmap='viridis_r', aspect='auto')\naxes[1].set_title(\"Matriz de distancias\\n(puntos ordenados por cluster real)\", fontsize=11)\naxes[1].set_xlabel(\"√çndice de punto (ordenado)\")\naxes[1].set_ylabel(\"√çndice de punto (ordenado)\")\nplt.colorbar(im2, ax=axes[1], label=\"Distancia euclidiana\")\n\nplt.suptitle(\"Estructura de bloque diagonal = clusters bien separados\",\n             fontsize=12, fontweight='bold')\nplt.tight_layout()\nplt.show()\n\nprint(\"Interpretaci√≥n:\")\nprint(\"  Colores oscuros = distancia corta (puntos similares)\")\nprint(\"  Colores claros = distancia larga (puntos distintos)\")\nprint(\"  Bloques diagonales oscuros = clusters compactos y separados\")"
  },
  {
   "cell_type": "markdown",
   "id": "md_be2bf9",
   "metadata": {},
   "source": "**Script de explicaci√≥n:**\n\n*\"La imagen de la derecha es lo que queremos ver: bloques oscuros en la diagonal. Cada bloque representa un cluster donde los puntos est√°n cerca entre s√≠, y la zona clara fuera de los bloques indica que los clusters est√°n bien separados entre ellos. En datos reales, esta estructura perfecta raramente aparece ‚Äî veremos se√±ales m√°s ambiguas.\"*\n\n---\n\n#### Celda 5 ‚Äî Comparaci√≥n de m√©tricas de distancia (ejercicio interactivo)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "code_b1216f",
   "metadata": {},
   "outputs": [],
   "source": "# ¬øLa m√©trica de distancia cambia la estructura que percibimos?\n\nfrom scipy.spatial.distance import cdist\n\n# Mismo dataset, tres m√©tricas distintas\nmetricas = {\n    'Euclidiana (L2)': 'euclidean',\n    'Manhattan (L1)':  'cityblock',\n    'Coseno':          'cosine'\n}\n\n# Tomamos una muestra peque√±a de los datos de lunas para visualizar\nsample_idx = np.random.choice(len(X_moons), 60, replace=False)\nX_sample = StandardScaler().fit_transform(X_moons[sample_idx])\ny_sample = y_moons[sample_idx]\nidx_sorted = np.argsort(y_sample)\n\nfig, axes = plt.subplots(1, 3, figsize=(16, 5))\n\nfor ax, (nombre, metrica) in zip(axes, metricas.items()):\n    D = cdist(X_sample, X_sample, metric=metrica)\n    D_sorted = D[np.ix_(idx_sorted, idx_sorted)]\n    im = ax.imshow(D_sorted, cmap='viridis_r', aspect='auto')\n    ax.set_title(f\"Distancia: {nombre}\", fontsize=11)\n    ax.set_xlabel(\"Punto (ordenado por cluster)\")\n    ax.set_ylabel(\"Punto (ordenado por cluster)\")\n    plt.colorbar(im, ax=ax)\n\nplt.suptitle(\"Dataset 'Lunas' ‚Äî La misma estructura, vista con tres m√©tricas distintas\",\n             fontsize=12, fontweight='bold')\nplt.tight_layout()\nplt.show()\n\nprint(\"Reflexi√≥n:\")\nprint(\"  ¬øLas tres m√©tricas revelan la misma estructura de bloques?\")\nprint(\"  ¬øCon cu√°l se ve m√°s claramente la separaci√≥n entre los dos grupos?\")"
  },
  {
   "cell_type": "markdown",
   "id": "md_09889b",
   "metadata": {},
   "source": "**Script de discusi√≥n:**\n\n*\"Mirad las tres matrices. Las tres muestran la misma estructura subyacente ‚Äîdos grupos‚Äî pero con claridad distinta seg√∫n la m√©trica. Este es el punto de partida de cualquier an√°lisis de clustering: antes de elegir un algoritmo, explorad qu√© m√©trica de distancia captura mejor la estructura que os importa.\"*\n\n---\n\n#### Celda 6 ‚Äî Discusi√≥n de cierre + pregunta reflexiva"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "code_de9859",
   "metadata": {},
   "outputs": [],
   "source": "# RESUMEN DEL BLOQUE 1.1\n# =======================\nprint(\"=\" * 55)\nprint(\"PUNTOS CLAVE DEL BLOQUE 1.1\")\nprint(\"=\" * 55)\nprint(\"\"\"\n1. El clustering busca estructura no etiquetada en los datos.\n   No hay 'respuesta correcta' ‚Äî hay soluciones m√°s o menos √∫tiles.\n\n2. Existen 5 paradigmas principales: particional, jer√°rquico,\n   basado en densidad, probabil√≠stico y neuronal.\n   Cada sesi√≥n cubrir√° representantes de cada familia.\n\n3. La elecci√≥n de la m√©trica de distancia es tan importante\n   como la elecci√≥n del algoritmo. Siempre normalizar primero.\n\n4. En alta dimensionalidad, las distancias pierden significado.\n   Reducci√≥n de dimensionalidad (Sesi√≥n 2) es la soluci√≥n.\n\n5. Visualizar los datos ANTES de modelar es obligatorio.\n   Las matrices de distancia son una herramienta infrautilizada.\n\"\"\")\nprint(\"=\" * 55)"
  },
  {
   "cell_type": "markdown",
   "id": "md_a7f779",
   "metadata": {},
   "source": "**Pregunta final de discusi√≥n (3 min):**\n\n*\"Antes de que pasemos al siguiente bloque, quiero que pens√©is en un dataset de vuestro trabajo o sector. ¬øQu√© columnas usar√≠ais para clusterizar? ¬øTendr√≠an todas la misma escala? ¬øQu√© m√©trica de distancia tendr√≠a sentido?\"*\n\n---\n\n## NOTAS DE PRODUCCI√ìN\n\n### Para las slides\n\n- **Slide 1:** Portada del bloque con t√≠tulo, duraci√≥n y objetivo.\n- **Slide 2:** Comparativa supervisado / no supervisado con tabla y ejemplos.\n- **Slide 3:** Los 5 paradigmas de clustering con iconos y ejemplo de output visual de cada uno.\n- **Slide 4:** Aplicaciones reales ‚Äî usar iconos de sector (compras, banco, gen√©tica). M√≠nimo texto, m√°ximo imagen.\n- **Slide 5:** F√≥rmulas de las 4 m√©tricas de distancia + c√≠rculos unitarios L1/L2/L‚àû.\n- **Slide 6:** Tabla resumen de m√©tricas (cu√°ndo usar cada una).\n- **Slide 7:** Advertencia de escalado ‚Äî el ejemplo de edad vs. ingresos.\n\n### Para el handout (papel o PDF)\n\nEl handout de este bloque debe incluir:\n- Tabla resumen de los 5 paradigmas de clustering.\n- Tabla de m√©tricas de distancia con las f√≥rmulas y el contexto de uso.\n- Los tres gr√°ficos de los datasets sint√©ticos (blobs, lunas, c√≠rculos).\n- La imagen de la matriz de distancias ordenada.\n- Checklist pre-clustering: *(1) ¬øDatos normalizados? (2) ¬øM√©trica adecuada? (3) ¬øDatos visualizados?*\n\n### Para el Jupyter Notebook (entrega a alumnos)\n\nEl notebook de este bloque se distribuye con las celdas de c√≥digo completas pero con **celdas de ejercicio** intercaladas con `# TODO:` marcando qu√© deben completar los alumnos:\n\n**Ejercicio 1:** A√±adir la distancia de Minkowski generalizada con p=3 a la comparativa de la Celda 5 e interpretar los resultados.\n\n**Ejercicio 2:** Generar un cuarto dataset con `make_blobs` pero con `cluster_std` muy alto (=3.0). ¬øSe siguen viendo bloques diagonales en la matriz de distancias?\n\n**Ejercicio 3 (opcional/avanzado):** Implementar la funci√≥n de distancia euclidiana desde cero usando solo NumPy (sin scipy) y verificar que produce los mismos resultados.\n\n---\n\n## GESTI√ìN DEL TIEMPO\n\n| Segmento | Duraci√≥n | Indicador de progreso |\n|---|---|---|\n| Apertura y contrato de aprendizaje | 5 min | Los alumnos tienen el notebook abierto |\n| ¬øQu√© es unsupervised learning? | 7 min | Tabla supervisado/no supervisado en pantalla |\n| Tipos de clustering + aplicaciones | 9 min | Diagrama de 5 paradigmas en pantalla |\n| M√©tricas de distancia | 4 min | Tabla de m√©tricas en pantalla |\n| Pr√°ctica Celda 1-2 (setup + datasets) | 8 min | Todos ejecutan sin errores |\n| Pr√°ctica Celda 3 (escala) | 7 min | Discusi√≥n del output |\n| Pr√°ctica Celda 4-5 (matrices) | 10 min | Visualizaciones generadas |\n| Cierre y discusi√≥n | 5 min | Pregunta reflexiva respondida |\n| **Total** | **55 min** | |\n\n---\n\n*Bloque 1.1 desarrollado para el m√≥dulo \"Algoritmos de Clustering\" ‚Äî M√°ster en Ciencia de Datos*"
  },
  {
   "cell_type": "markdown",
   "id": "md_50726c",
   "metadata": {},
   "source": "---\n## üí° Para explorar m√°s ‚Äî Ejercicios propuestos\n\nLos ejercicios pr√°cticos est√°n marcados con comentarios `# EJERCICIO` en el c√≥digo.\n\n**Entrega sugerida:** Exporta este notebook como HTML o PDF (`File ‚Üí Download ‚Üí HTML`)\ny a√±ade tus conclusiones en una celda Markdown al final de cada secci√≥n.\n\n---\n*M√°ster en Ciencia de Datos ¬∑ M√≥dulo Clustering ¬∑ Bloque 1.1*"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}