{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "md_51b553",
   "metadata": {},
   "source": "# Bloque 1.2 ‚Äî K-Means y K-Medoids\n**M√°ster en Ciencia de Datos ¬∑ M√≥dulo: Algoritmos de Clustering**\n**Sesi√≥n 1 ¬∑ Duraci√≥n: 110 min**\n\n---\n> üìå **C√≥mo usar este notebook:**\n> Ejecuta las celdas **en orden**. Cada secci√≥n comienza con explicaci√≥n te√≥rica (en Markdown) seguida del c√≥digo correspondiente.\n> Los comentarios `# ---` delimitan ejercicios opcionales para profundizar.\n"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "code_2d5776",
   "metadata": {},
   "outputs": [],
   "source": "# ============================================================\n# BLOQUE 1.2 ‚Äî K-Means y K-Medoids\n# ============================================================\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as mpatches\nimport seaborn as sns\n\nfrom sklearn.cluster import KMeans\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.datasets import make_blobs\nfrom sklearn.metrics import silhouette_score\n\nplt.rcParams['figure.figsize'] = (10, 6)\nplt.rcParams['font.size'] = 12\nsns.set_style(\"whitegrid\")\nnp.random.seed(42)\n\nprint(\"‚úì Imports correctos\")"
  },
  {
   "cell_type": "markdown",
   "id": "md_3e666d",
   "metadata": {},
   "source": "---\n\n#### Celda 2 ‚Äî Implementaci√≥n manual del algoritmo de Lloyd (paso a paso)\n\n> *Nota: Esta celda tiene un prop√≥sito pedag√≥gico: mostrar el algoritmo desde cero antes de usar scikit-learn. No es la implementaci√≥n que usar√≠an en producci√≥n.*"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "code_28c83a",
   "metadata": {},
   "outputs": [],
   "source": "def kmeans_manual(X, k, n_iter=10, seed=42):\n    \"\"\"\n    Implementaci√≥n did√°ctica del algoritmo de Lloyd (K-Means b√°sico).\n    NO usar en producci√≥n ‚Äî usar sklearn.cluster.KMeans.\n    \"\"\"\n    rng = np.random.RandomState(seed)\n\n    # Paso 0: Inicializaci√≥n aleatoria (Forgy)\n    idx_init = rng.choice(len(X), size=k, replace=False)\n    centroides = X[idx_init].copy()\n\n    historial = [centroides.copy()]  # guardamos la evoluci√≥n\n\n    for iteracion in range(n_iter):\n        # Paso 1: Asignaci√≥n ‚Äî cada punto al centroide m√°s cercano\n        distancias = np.linalg.norm(\n            X[:, np.newaxis, :] - centroides[np.newaxis, :, :], axis=2\n        )\n        asignaciones = np.argmin(distancias, axis=1)\n\n        # Paso 2: Actualizaci√≥n ‚Äî recalcular centroides como media de sus puntos\n        nuevos_centroides = np.array([\n            X[asignaciones == j].mean(axis=0) if (asignaciones == j).any()\n            else centroides[j]  # cluster vac√≠o: mantener centroide\n            for j in range(k)\n        ])\n\n        historial.append(nuevos_centroides.copy())\n\n        # Paso 3: Convergencia\n        if np.allclose(centroides, nuevos_centroides, atol=1e-6):\n            print(f\"  Convergencia alcanzada en iteraci√≥n {iteracion + 1}\")\n            break\n\n        centroides = nuevos_centroides\n\n    wcss = sum(\n        np.sum((X[asignaciones == j] - centroides[j]) ** 2)\n        for j in range(k)\n    )\n\n    return asignaciones, centroides, wcss, historial\n\n\n# --- Generamos datos y ejecutamos ---\nX_demo, y_real = make_blobs(n_samples=200, centers=3, cluster_std=1.0, random_state=42)\nX_demo_norm = StandardScaler().fit_transform(X_demo)\n\nprint(\"Ejecutando K-Means manual con k=3:\")\nlabels, centroides_finales, wcss_final, historial = kmeans_manual(X_demo_norm, k=3)\nprint(f\"  WCSS final: {wcss_final:.4f}\")\nprint(f\"  Puntos por cluster: {[np.sum(labels == j) for j in range(3)]}\")"
  },
  {
   "cell_type": "markdown",
   "id": "md_96e8aa",
   "metadata": {},
   "source": "**Script de explicaci√≥n:**\n\n*\"Fijaos en la funci√≥n: son literalmente tres pasos dentro de un bucle. Paso 1 calcula qu√© centroide est√° m√°s cerca de cada punto. Paso 2 mueve los centroides. El bucle para cuando los centroides ya no se mueven. Eso es todo K-Means. La magia y la limitaci√≥n est√°n en que el centroide es la media aritm√©tica ‚Äîeso es lo que vamos a cuestionar con K-Medoids.\"*\n\n---\n\n#### Celda 3 ‚Äî Visualizaci√≥n de la evoluci√≥n iterativa"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "code_e2bbb9",
   "metadata": {},
   "outputs": [],
   "source": "def plot_evolucion_kmeans(X, historial, labels_finales, k, max_iter_mostrar=5):\n    \"\"\"Muestra c√≥mo evolucionan los centroides a lo largo de las iteraciones.\"\"\"\n    n_iter = min(len(historial), max_iter_mostrar)\n    fig, axes = plt.subplots(1, n_iter, figsize=(4 * n_iter, 4))\n    if n_iter == 1:\n        axes = [axes]\n\n    colores = plt.cm.tab10(np.linspace(0, 0.5, k))\n\n    for idx, ax in enumerate(axes):\n        if idx < len(historial) - 1:\n            # Asignaciones provisionales para esta iteraci√≥n\n            dists = np.linalg.norm(\n                X[:, np.newaxis, :] - historial[idx][np.newaxis, :, :], axis=2\n            )\n            labels_iter = np.argmin(dists, axis=1)\n            titulo = f\"Iteraci√≥n {idx}\" if idx > 0 else \"Inicializaci√≥n\"\n        else:\n            labels_iter = labels_finales\n            titulo = \"Convergencia\"\n\n        ax.scatter(X[:, 0], X[:, 1], c=labels_iter, cmap='tab10',\n                   alpha=0.5, s=20)\n        centroides_iter = historial[idx]\n        ax.scatter(centroides_iter[:, 0], centroides_iter[:, 1],\n                   c='red', marker='X', s=200, zorder=5,\n                   edgecolors='black', linewidths=1.5, label='Centroides')\n        ax.set_title(titulo, fontsize=10, fontweight='bold')\n        ax.set_xticks([])\n        ax.set_yticks([])\n\n    plt.suptitle(\"Evoluci√≥n de K-Means: de inicializaci√≥n a convergencia\",\n                 fontsize=12, fontweight='bold')\n    plt.tight_layout()\n    plt.savefig(\"img_evolucion_kmeans.png\", dpi=150, bbox_inches='tight')\n    plt.show()\n\n\nplot_evolucion_kmeans(X_demo_norm, historial, labels, k=3)"
  },
  {
   "cell_type": "markdown",
   "id": "md_943059",
   "metadata": {},
   "source": "**Script de explicaci√≥n:**\n\n*\"Este es el gr√°fico m√°s importante para entender K-Means intuitivamente. Las X rojas son los centroides. En la inicializaci√≥n est√°n en posiciones aleatorias. En la primera iteraci√≥n, los puntos se asignan a su X m√°s cercana y las X se mueven al centro de sus grupos. En pocas iteraciones el algoritmo converge. Guardad este gr√°fico mentalmente: cuando algo falla en K-Means, normalmente es porque los centroides iniciales estaban en una posici√≥n muy mala.\"*\n\n---\n\n#### Celda 4 ‚Äî K-Means con scikit-learn + m√©todo del codo"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "code_caba96",
   "metadata": {},
   "outputs": [],
   "source": "# ---- CASO PR√ÅCTICO: Dataset Mall Customers ----\n# Segmentaci√≥n de clientes por Annual Income y Spending Score\n# Fuente: Kaggle (incluido en la carpeta datasets/)\n\n# Para la demo en clase usamos datos sint√©ticos que replican la estructura\n# del dataset original. En producci√≥n, cargar con pd.read_csv('mall_customers.csv')\n\nnp.random.seed(0)\nn = 200\ningresos   = np.concatenate([\n    np.random.normal(20,  5,  30),   # bajo ingreso, bajo gasto\n    np.random.normal(20,  5,  30),   # bajo ingreso, alto gasto\n    np.random.normal(55,  8,  40),   # ingreso medio\n    np.random.normal(85,  7,  50),   # alto ingreso, bajo gasto\n    np.random.normal(85,  7,  50),   # alto ingreso, alto gasto\n])\ngasto = np.concatenate([\n    np.random.normal(20,  6,  30),\n    np.random.normal(80,  6,  30),\n    np.random.normal(50,  8,  40),\n    np.random.normal(15,  6,  50),\n    np.random.normal(82,  6,  50),\n])\ndf_mall = pd.DataFrame({'Annual_Income_k': ingresos, 'Spending_Score': gasto})\ndf_mall = df_mall.clip(lower=0)  # sin negativos\n\nprint(f\"Dataset: {df_mall.shape[0]} clientes, {df_mall.shape[1]} variables\")\nprint(df_mall.describe().round(1))"
  },
  {
   "cell_type": "markdown",
   "id": "md_1b0367",
   "metadata": {},
   "source": "---\n\n#### Celda 5 ‚Äî M√©todo del codo"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "code_445943",
   "metadata": {},
   "outputs": [],
   "source": "# Normalizaci√≥n\nscaler = StandardScaler()\nX_mall = scaler.fit_transform(df_mall)\n\n# M√©todo del codo: WCSS para k = 1..10\nwcss_lista = []\nk_range = range(1, 11)\n\nfor k in k_range:\n    km = KMeans(n_clusters=k, init='k-means++', n_init=10, random_state=42)\n    km.fit(X_mall)\n    wcss_lista.append(km.inertia_)\n\n# Visualizaci√≥n\nfig, ax = plt.subplots(figsize=(9, 5))\nax.plot(k_range, wcss_lista, 'bo-', linewidth=2, markersize=8)\nax.set_xlabel(\"N√∫mero de clusters (k)\", fontsize=12)\nax.set_ylabel(\"WCSS (Inercia)\", fontsize=12)\nax.set_title(\"M√©todo del Codo ‚Äî Dataset Mall Customers\", fontsize=13, fontweight='bold')\nax.set_xticks(k_range)\n\n# Anotaci√≥n manual del codo\nax.annotate('Codo ‚âà k=5',\n            xy=(5, wcss_lista[4]),\n            xytext=(6.5, wcss_lista[4] + 0.3 * (wcss_lista[0] - wcss_lista[-1])),\n            arrowprops=dict(arrowstyle='->', color='red', lw=2),\n            fontsize=11, color='red', fontweight='bold')\n\nplt.tight_layout()\nplt.savefig(\"img_codo_mall.png\", dpi=150, bbox_inches='tight')\nplt.show()\n\nprint(f\"\\nReducci√≥n de WCSS al pasar de k=4 a k=5: \"\n      f\"{wcss_lista[3]-wcss_lista[4]:.3f}\")\nprint(f\"Reducci√≥n de WCSS al pasar de k=5 a k=6: \"\n      f\"{wcss_lista[4]-wcss_lista[5]:.3f}\")\nprint(\"‚Üí El salto es mayor en k=4‚Üí5, confirmando k=5 como codo.\")"
  },
  {
   "cell_type": "markdown",
   "id": "md_332361",
   "metadata": {},
   "source": "**Script de explicaci√≥n del codo:**\n\n*\"La curva cae bruscamente de k=1 a k=5 y luego se aplana. Eso es el codo. A√±adir un sexto cluster apenas reduce la WCSS porque ya no estamos capturando estructura real, solo partiendo clusters que ya eran buenos. Fij√©monos en los valores num√©ricos: el salto de k=4 a k=5 es mayor que el de k=5 a k=6.\"*\n\n---\n\n#### Celda 6 ‚Äî Resultado final de K-Means con k=5"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "code_f5e053",
   "metadata": {},
   "outputs": [],
   "source": "# Entrenamiento final\nkm_final = KMeans(n_clusters=5, init='k-means++', n_init=10, random_state=42)\ndf_mall['Cluster'] = km_final.fit_predict(X_mall)\n\n# Centroides en escala original\ncentroides_orig = scaler.inverse_transform(km_final.cluster_centers_)\ndf_centroides = pd.DataFrame(\n    centroides_orig,\n    columns=['Annual_Income_k', 'Spending_Score']\n)\ndf_centroides.index.name = 'Cluster'\n\n# Visualizaci√≥n\ncolores = ['#e41a1c','#377eb8','#4daf4a','#ff7f00','#984ea3']\nfig, ax = plt.subplots(figsize=(10, 7))\n\nfor c in range(5):\n    mask = df_mall['Cluster'] == c\n    ax.scatter(\n        df_mall.loc[mask, 'Annual_Income_k'],\n        df_mall.loc[mask, 'Spending_Score'],\n        color=colores[c], alpha=0.7, s=60, label=f'Cluster {c}'\n    )\n\n# Centroides\nax.scatter(\n    df_centroides['Annual_Income_k'],\n    df_centroides['Spending_Score'],\n    c='black', marker='X', s=250, zorder=5, label='Centroides'\n)\n\nax.set_xlabel(\"Ingresos anuales (k‚Ç¨)\", fontsize=12)\nax.set_ylabel(\"Spending Score (0‚Äì100)\", fontsize=12)\nax.set_title(\"K-Means k=5 ‚Äî Segmentaci√≥n de clientes Mall\", fontsize=13, fontweight='bold')\nax.legend(fontsize=10)\nplt.tight_layout()\nplt.savefig(\"img_kmeans_mall.png\", dpi=150, bbox_inches='tight')\nplt.show()\n\n# Interpretaci√≥n de negocio\nprint(\"\\nPerfil de cada cluster (medias en escala original):\")\nprint(df_mall.groupby('Cluster')[['Annual_Income_k','Spending_Score']].mean().round(1))"
  },
  {
   "cell_type": "markdown",
   "id": "md_72f8bd",
   "metadata": {},
   "source": "**Script de interpretaci√≥n:**\n\n*\"Ahora viene la parte m√°s importante: dar nombre a los clusters. El algoritmo no sabe nada de negocio ‚Äîeso lo ponemos nosotros. Mirando los centroides podemos identificar cinco perfiles:\"*\n- *\"Cluster con ingresos bajos y gasto bajo ‚Üí 'Ahorradores con presupuesto ajustado'\"*\n- *\"Cluster con ingresos bajos y gasto alto ‚Üí 'Compradores impulsivos (alto riesgo de deuda)'\"*\n- *\"Cluster con ingresos medios ‚Üí 'Clientes est√°ndar'\"*\n- *\"Cluster con ingresos altos y gasto bajo ‚Üí 'Ahorradores premium'\"*\n- *\"Cluster con ingresos altos y gasto alto ‚Üí 'VIPs ‚Äî m√°xima prioridad de retenci√≥n'\"*\n\n*\"Esta interpretaci√≥n es la entrega real. No un n√∫mero, sino una narrativa de negocio.\"*\n\n---\n\n#### Celda 7 ‚Äî Comparaci√≥n K-Means vs K-Means++ (demo de inicializaci√≥n)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "code_bf2423",
   "metadata": {},
   "outputs": [],
   "source": "# ¬øCu√°nto importa la inicializaci√≥n?\n\nresultados = []\n\nfor metodo in ['random', 'k-means++']:\n    wcss_runs = []\n    for seed in range(20):\n        km = KMeans(n_clusters=5, init=metodo, n_init=1, random_state=seed)\n        km.fit(X_mall)\n        wcss_runs.append(km.inertia_)\n    resultados.append({\n        'M√©todo': metodo,\n        'WCSS media': np.mean(wcss_runs),\n        'WCSS std':   np.std(wcss_runs),\n        'WCSS min':   np.min(wcss_runs),\n        'WCSS max':   np.max(wcss_runs),\n    })\n\ndf_res = pd.DataFrame(resultados).set_index('M√©todo')\nprint(\"Comparaci√≥n de inicializaci√≥n (20 runs, n_init=1 cada una):\")\nprint(df_res.round(4))\n\n# Visualizaci√≥n como boxplot\nfig, ax = plt.subplots(figsize=(8, 5))\ndata_random   = [KMeans(n_clusters=5, init='random',    n_init=1, random_state=s).fit(X_mall).inertia_ for s in range(30)]\ndata_kpp      = [KMeans(n_clusters=5, init='k-means++', n_init=1, random_state=s).fit(X_mall).inertia_ for s in range(30)]\nax.boxplot([data_random, data_kpp],\n           labels=['Inicializaci√≥n\\naleatoria', 'K-Means++'],\n           patch_artist=True,\n           boxprops=dict(facecolor='lightblue'))\nax.set_ylabel(\"WCSS (Inercia)\", fontsize=12)\nax.set_title(\"Variabilidad de WCSS seg√∫n m√©todo de inicializaci√≥n\\n(30 runs, n_init=1)\",\n             fontsize=12, fontweight='bold')\nplt.tight_layout()\nplt.savefig(\"img_kmeans_vs_kpp.png\", dpi=150, bbox_inches='tight')\nplt.show()"
  },
  {
   "cell_type": "markdown",
   "id": "md_c4f1e4",
   "metadata": {},
   "source": "**Script de explicaci√≥n:**\n\n*\"Con inicializaci√≥n aleatoria, el WCSS var√≠a mucho entre runs: a veces encontramos una buena soluci√≥n, a veces una mala. Con K-Means++ la varianza es mucho menor y el m√≠nimo es mejor. En scikit-learn, el par√°metro `n_init=10` ya ejecuta esto autom√°ticamente y se queda con el mejor resultado ‚Äî por eso es el default.\"*\n\n---\n\n## PARTE B ‚Äî K-MEDOIDS\n\n---\n\n### TEOR√çA K-MEDOIDS (20 min)\n\n---\n\n### [01:05 ‚Äì 01:10] Motivaci√≥n: ¬øPor qu√© K-Medoids?\n\n**Script de transici√≥n:**\n\n*\"K-Means tiene una vulnerabilidad fundamental: el centroide es la media aritm√©tica de los puntos del cluster. Esto tiene un problema grave: la media puede ser un punto que no existe en el dataset. Y si hay outliers, la media se 'contamina'.\"*\n\n**Ejemplo num√©rico inmediato:**\n\n*\"Imaginad un cluster con cuatro clientes con ingresos de 20k, 22k, 21k y 85k euros. La media es (20+22+21+85)/4 = 37k. Ese centroide de 37k no representa bien a nadie del cluster: los tres primeros tienen ~21k y el cuarto es un outlier en 85k. El 'representante' del cluster no es un cliente real.\"*\n\n*\"K-Medoids soluciona esto de forma elegante: en lugar de la media, usa el **medoide** ‚Äî el punto real del dataset que minimiza la distancia media a todos los dem√°s puntos de su cluster. El representante siempre es un cliente que existe.\"*\n\n**Ventajas inmediatas:**\n1. **Robustez ante outliers:** el medoide no puede ser 'tirado' hacia un outlier porque es un punto real del dataset.\n2. **Interpretabilidad:** cada cluster est√° representado por un caso real. Puedes decir: *\"Este segmento se parece al cliente #1234\"*.\n3. **Funciona con cualquier m√©trica de distancia:** no requiere que la media tenga sentido. Funciona con distancias no-euclidianas, datos mixtos o incluso distancias entre strings (edit distance).\n\n---\n\n### [01:10 ‚Äì 01:20] El algoritmo PAM (Partitioning Around Medoids)\n\n**Desarrollado por Kaufman & Rousseeuw (1990). Es el algoritmo de K-Medoids m√°s conocido y sigue siendo la referencia.**\n\n**Notaci√≥n:** Dado un conjunto `S` de `n` puntos y una funci√≥n de distancia `d(i,j)`, PAM busca un conjunto `M` de `k` medoides tal que el coste total sea m√≠nimo:\n\n```\nCOSTE = Œ£·µ¢‚àâM  min_{m‚ààM} d(i, m)\n```\n\n*(La suma de distancias de cada punto no-medoide al medoide m√°s cercano.)*\n\n**Fase BUILD ‚Äî Inicializaci√≥n inteligente:**\n\nA diferencia de K-Means que inicializa aleatoriamente, PAM tiene una fase de inicializaci√≥n determinista:\n\n1. Elige el primer medoide `m‚ÇÅ`: el punto que minimiza la suma de distancias a todos los dem√°s (el punto m√°s \"central\" del dataset completo).\n2. Para cada punto candidato `x` a ser el segundo medoide, calcula cu√°nto reducir√≠a el coste total a√±adirlo. Elige el que m√°s reduce el coste.\n3. Repite hasta tener `k` medoides.\n\n**Fase SWAP ‚Äî Optimizaci√≥n iterativa:**\n\nUna vez inicializados los `k` medoides, PAM intenta mejoras sistem√°ticas:\n\n1. Para cada par `(m·µ¢, x‚±º)` donde `m·µ¢` es un medoide actual y `x‚±º` es un punto no-medoide:\n   - Calcula el coste del swap: ¬øcu√°nto cambiar√≠a el coste total si `x‚±º` reemplazara a `m·µ¢`?\n2. Si existe alg√∫n swap que reduce el coste, realiza el que m√°s lo reduce.\n3. Repite hasta que no haya swaps beneficiosos.\n\n**Complejidad computacional:**\n\n- Fase BUILD: `O(k ¬∑ n¬≤)`\n- Fase SWAP por iteraci√≥n: `O(k ¬∑ (n-k)¬≤)`. Cada iteraci√≥n eval√∫a `k √ó (n-k)` swaps posibles, y cada evaluaci√≥n cuesta `O(n-k)`.\n- Para `n` grande, PAM es significativamente m√°s lento que K-Means. Para `n < 5.000` es perfectamente viable.\n\n**Variantes para datasets grandes:**\n\n| Variante | Idea | Complejidad | Cu√°ndo usar |\n|---|---|---|---|\n| PAM | Exacto, todos los swaps | O(k¬∑(n-k)¬≤) | n < 5.000 |\n| CLARA | Muestrea subconjuntos, aplica PAM a cada uno | O(k¬∑s¬≤) | n ~ 10‚Å¥‚Äì10‚Åµ |\n| CLARANS | B√∫squeda aleatoria de vecinos en el espacio de soluciones | O(n¬≤) | n ~ 10‚Åµ |\n\n**scikit-learn-extra implementa los tres.** El par√°metro `method` de `KMedoids` acepta `'pam'`, `'alternate'` (variante m√°s r√°pida) y `'fastpam1'`.\n\n---\n\n### [01:20 ‚Äì 01:25] K-Means vs. K-Medoids: cu√°ndo elegir cada uno\n\n**Tabla comparativa definitiva:**\n\n| Criterio | K-Means | K-Medoids |\n|---|---|---|\n| Representante del cluster | Media (puede no existir) | Punto real del dataset |\n| Robustez ante outliers | Baja | **Alta** |\n| M√©trica de distancia | Solo euclidiana (nativa) | **Cualquier m√©trica** |\n| Velocidad (n grande) | **Muy r√°pido** O(n¬∑k¬∑d¬∑i) | M√°s lento O(k¬∑(n-k)¬≤) |\n| Interpretabilidad | Media | **Alta** ‚Äî caso real |\n| Datos mixtos / categ√≥ricos | No nativo | **S√≠**, con la m√©trica adecuada |\n| Default scikit | `sklearn.cluster.KMeans` | `sklearn_extra.cluster.KMedoids` |\n\n**Regla pr√°ctica para elegir:**\n\n*\"Usad K-Means cuando teng√°is muchos datos, las variables sean num√©ricas y continuas, y no haya muchos outliers. Usad K-Medoids cuando los outliers sean un problema, cuando necesit√©is que cada cluster est√© representado por un caso real (√∫til para presentaciones a negocio), o cuando est√©is trabajando con distancias que no son euclidianas ‚Äîpor ejemplo, distancias entre perfiles de comportamiento discreto, o datos que incluyen variables categ√≥ricas.\"*\n\n---\n\n## PR√ÅCTICA K-MEDOIDS ‚Äî Jupyter Notebook (25 min)\n\n---\n\n### [01:25 ‚Äì 01:50] Pr√°ctica guiada\n\n---\n\n#### Celda 8 ‚Äî Instalaci√≥n y verificaci√≥n de scikit-learn-extra"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "code_4d0e5e",
   "metadata": {},
   "outputs": [],
   "source": "# scikit-learn-extra no viene con scikit-learn est√°ndar\n# Instalar con: pip install scikit-learn-extra\n\ntry:\n    from sklearn_extra.cluster import KMedoids\n    print(\"‚úì scikit-learn-extra disponible\")\nexcept ImportError:\n    print(\"‚úó Instalando scikit-learn-extra...\")\n    import subprocess\n    subprocess.run([\"pip\", \"install\", \"scikit-learn-extra\", \"-q\"])\n    from sklearn_extra.cluster import KMedoids\n    print(\"‚úì scikit-learn-extra instalado y cargado\")"
  },
  {
   "cell_type": "markdown",
   "id": "md_f65092",
   "metadata": {},
   "source": "---\n\n#### Celda 9 ‚Äî Demostraci√≥n del impacto de outliers: K-Means vs. K-Medoids"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "code_8231cf",
   "metadata": {},
   "outputs": [],
   "source": "# -------------------------------------------------------\n# EXPERIMENTO: ¬øC√≥mo afectan los outliers a K-Means\n# pero no a K-Medoids?\n# -------------------------------------------------------\n\nfrom sklearn_extra.cluster import KMedoids\n\n# Dataset base: 3 clusters bien separados\nnp.random.seed(42)\nX_base, _ = make_blobs(n_samples=120, centers=[[-3, 0], [0, 0], [3, 0]],\n                        cluster_std=0.6, random_state=42)\n\n# A√±adimos 5 outliers extremos artificiales\noutliers = np.array([\n    [-3, 8], [-3, 9],   # outliers sobre el cluster izquierdo\n    [3,  -8], [3, -9],  # outliers sobre el cluster derecho\n    [0,  10]            # outlier sobre el cluster central\n])\n\nX_con_outliers = np.vstack([X_base, outliers])\n\n# Escalado\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X_con_outliers)\nn_outliers = len(outliers)\n\n# Entrenamos ambos algoritmos con k=3\nkm  = KMeans(n_clusters=3, n_init=10, random_state=42)\nkmd = KMedoids(n_clusters=3, method='pam', random_state=42)\n\nlabels_km  = km.fit_predict(X_scaled)\nlabels_kmd = kmd.fit_predict(X_scaled)\n\n# Obtenemos centroides/medoides en escala original\ncentroides_km  = scaler.inverse_transform(km.cluster_centers_)\nmedoides_kmd   = scaler.inverse_transform(kmd.cluster_centers_)\n\n# ---- Visualizaci√≥n comparativa ----\nfig, axes = plt.subplots(1, 2, figsize=(14, 6))\n\nfor ax, labels, representantes, titulo, color_rep in zip(\n    axes,\n    [labels_km, labels_kmd],\n    [centroides_km, medoides_kmd],\n    [\"K-Means (sensible a outliers)\", \"K-Medoids (robusto a outliers)\"],\n    ['red', 'green']\n):\n    # Puntos normales\n    scatter = ax.scatter(\n        X_con_outliers[:-n_outliers, 0],\n        X_con_outliers[:-n_outliers, 1],\n        c=labels[:-n_outliers], cmap='tab10', alpha=0.7, s=40\n    )\n    # Outliers marcados con estrella\n    ax.scatter(\n        outliers[:, 0], outliers[:, 1],\n        c='black', marker='*', s=250, zorder=5, label='Outliers'\n    )\n    # Representantes\n    ax.scatter(\n        representantes[:, 0], representantes[:, 1],\n        c=color_rep, marker='X', s=300, zorder=6,\n        edgecolors='black', linewidths=1.5,\n        label='Centroides' if color_rep=='red' else 'Medoides'\n    )\n    ax.set_title(titulo, fontsize=12, fontweight='bold')\n    ax.legend(fontsize=9)\n    ax.set_xlabel(\"Caracter√≠stica 1\")\n    ax.set_ylabel(\"Caracter√≠stica 2\")\n\nplt.suptitle(\"Impacto de outliers en K-Means vs. K-Medoids\",\n             fontsize=13, fontweight='bold')\nplt.tight_layout()\nplt.savefig(\"img_kmeans_vs_kmedoids_outliers.png\", dpi=150, bbox_inches='tight')\nplt.show()"
  },
  {
   "cell_type": "markdown",
   "id": "md_de18b3",
   "metadata": {},
   "source": "**Script de explicaci√≥n ‚Äî este es el momento clave del bloque:**\n\n*\"Fijaos en las X rojas (K-Means) y en las X verdes (K-Medoids). Los clusters reales son tres grupos horizontales. Los outliers son las estrellas negras arriba y abajo.\"*\n\n*\"En K-Means, los centroides rojos est√°n desplazados hacia los outliers porque la media se contamina. El cluster izquierdo tiene su centroide 'subido' hacia los outliers de arriba. En K-Medoids, los medoides verdes est√°n en el centro real de cada cluster porque son puntos reales del dataset ‚Äî los outliers no pueden moverlos.\"*\n\n*\"En un proyecto real, esto significa que con K-Means vuestro segmento 'cliente t√≠pico' podr√≠a estar representado por un perfil que no existe, distorsionado por cuatro transacciones fraudulentas o por cuatro clientes VIP extremos.\"*\n\n---\n\n#### Celda 10 ‚Äî Cuantificaci√≥n del desplazamiento de representantes"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "code_aef754",
   "metadata": {},
   "outputs": [],
   "source": "# Medimos cu√°nto se desplazan los representantes respecto al centro real\n\n# Calculamos los centros \"reales\" (sin outliers) para comparar\nkm_sin_outliers  = KMeans(n_clusters=3, n_init=10, random_state=42)\nkm_sin_outliers.fit(scaler.transform(X_base))\ncentros_reales = scaler.inverse_transform(km_sin_outliers.cluster_centers_)\n\n# Ordenamos clusters por coordenada X para comparar correctamente\ndef ordenar_clusters(centers):\n    return centers[np.argsort(centers[:, 0])]\n\nreales = ordenar_clusters(centros_reales)\nkm_c   = ordenar_clusters(centroides_km)\nkmd_c  = ordenar_clusters(medoides_kmd)\n\nprint(\"Distancia de cada representante al centro real del cluster:\")\nprint(\"-\" * 55)\nfor i, (r, km_ci, kmd_ci) in enumerate(zip(reales, km_c, kmd_c)):\n    d_km  = np.linalg.norm(km_ci - r)\n    d_kmd = np.linalg.norm(kmd_ci - r)\n    print(f\"Cluster {i+1}:  K-Means desplazado {d_km:.3f} unidades  |\"\n          f\"  K-Medoids desplazado {d_kmd:.3f} unidades\")\n\nprint(\"\\n‚Üí K-Medoids mantiene sus representantes mucho m√°s cerca del centro real.\")"
  },
  {
   "cell_type": "markdown",
   "id": "md_8d5d68",
   "metadata": {},
   "source": "---\n\n#### Celda 11 ‚Äî K-Medoids aplicado al dataset Mall Customers"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "code_cfe85f",
   "metadata": {},
   "outputs": [],
   "source": "# Comparaci√≥n directa sobre el mismo dataset de negocio\n\nkmd_mall = KMedoids(n_clusters=5, method='pam', random_state=42)\ndf_mall['Cluster_KMedoids'] = kmd_mall.fit_predict(X_mall)\n\nmedoides_orig = scaler.inverse_transform(kmd_mall.cluster_centers_)\n\n# Visualizaci√≥n lado a lado\nfig, axes = plt.subplots(1, 2, figsize=(15, 6))\n\nfor ax, col_cluster, representantes, titulo, marker_color in zip(\n    axes,\n    ['Cluster', 'Cluster_KMedoids'],\n    [centroides_orig, medoides_orig],\n    ['K-Means k=5', 'K-Medoids k=5'],\n    ['red', 'green']\n):\n    for c in range(5):\n        mask = df_mall[col_cluster] == c\n        ax.scatter(\n            df_mall.loc[mask, 'Annual_Income_k'],\n            df_mall.loc[mask, 'Spending_Score'],\n            alpha=0.6, s=50\n        )\n    ax.scatter(\n        representantes[:, 0], representantes[:, 1],\n        c=marker_color, marker='X', s=250, zorder=5,\n        edgecolors='black', linewidths=1.5\n    )\n    ax.set_title(titulo, fontsize=12, fontweight='bold')\n    ax.set_xlabel(\"Ingresos anuales (k‚Ç¨)\")\n    ax.set_ylabel(\"Spending Score\")\n\nplt.suptitle(\"Mall Customers ‚Äî Comparaci√≥n K-Means vs K-Medoids\",\n             fontsize=13, fontweight='bold')\nplt.tight_layout()\nplt.savefig(\"img_mall_kmeans_vs_kmedoids.png\", dpi=150, bbox_inches='tight')\nplt.show()\n\n# Mostrar los medoides como filas reales del dataset\nprint(\"\\nMediantas: los 5 clientes 'representativos' seg√∫n K-Medoids:\")\ndf_medoides = df_mall.iloc[kmd_mall.medoid_indices_][\n    ['Annual_Income_k', 'Spending_Score']\n].copy()\ndf_medoides.index = [f'Medoide Cluster {i}' for i in range(5)]\nprint(df_medoides.round(1))\nprint(\"\\n‚Üí Estos son clientes reales del dataset. Existen.\")"
  },
  {
   "cell_type": "markdown",
   "id": "md_fff2fd",
   "metadata": {},
   "source": "**Script de explicaci√≥n:**\n\n*\"Aqu√≠ est√° la gran diferencia pr√°ctica: los medoides son filas reales de vuestro dataset. Si vais a presentar los resultados al equipo de marketing, pod√©is decir: 'Este segmento se parece al cliente 47, que compra as√≠ y gasta as√≠'. Con K-Means, el centroide es un cliente imaginario que quiz√°s no existe en vuestros sistemas.\"*\n\n---\n\n#### Celda 12 ‚Äî Mini-ejercicio: ¬øCu√°ndo escalar importa para K-Medoids?"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "code_45fc3d",
   "metadata": {},
   "outputs": [],
   "source": "# Ejercicio guiado: K-Medoids SIN normalizar vs. CON normalizar\n# (mismo punto que con K-Means pero importante repetirlo)\n\nkmd_sin_norm = KMedoids(n_clusters=5, method='pam', random_state=42)\nkmd_con_norm = KMedoids(n_clusters=5, method='pam', random_state=42)\n\nlabels_sin = kmd_sin_norm.fit_predict(df_mall[['Annual_Income_k','Spending_Score']].values)\nlabels_con = kmd_con_norm.fit_predict(X_mall)\n\nfig, axes = plt.subplots(1, 2, figsize=(13, 5))\n\nfor ax, labels, titulo in zip(\n    axes,\n    [labels_sin, labels_con],\n    ['K-Medoids SIN normalizar', 'K-Medoids CON normalizar']\n):\n    ax.scatter(df_mall['Annual_Income_k'], df_mall['Spending_Score'],\n               c=labels, cmap='tab10', alpha=0.7, s=50)\n    ax.set_title(titulo, fontsize=11, fontweight='bold')\n    ax.set_xlabel(\"Ingresos anuales (k‚Ç¨)\")\n    ax.set_ylabel(\"Spending Score\")\n\nplt.suptitle(\"Impacto de la normalizaci√≥n en K-Medoids\",\n             fontsize=12, fontweight='bold')\nplt.tight_layout()\nplt.show()\n\nprint(\"Conclusi√≥n: K-Medoids tambi√©n requiere normalizaci√≥n.\")\nprint(\"La escala afecta a las distancias, independientemente del algoritmo.\")"
  },
  {
   "cell_type": "markdown",
   "id": "md_fda77c",
   "metadata": {},
   "source": "---\n\n#### Celda 13 ‚Äî Resumen comparativo del bloque"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "code_8f733f",
   "metadata": {},
   "outputs": [],
   "source": "print(\"=\" * 60)\nprint(\"RESUMEN BLOQUE 1.2 ‚Äî K-Means y K-Medoids\")\nprint(\"=\" * 60)\n\nresumen = {\n    \"K-Means\":   {\"Velocidad\": \"‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ\", \"Robustez outliers\": \"‚òÖ‚òÖ‚òÜ‚òÜ‚òÜ\",\n                  \"Interpretabilidad\": \"‚òÖ‚òÖ‚òÖ‚òÜ‚òÜ\", \"M√©tricas flexibles\": \"‚òÖ‚òÖ‚òÜ‚òÜ‚òÜ\"},\n    \"K-Medoids\": {\"Velocidad\": \"‚òÖ‚òÖ‚òÖ‚òÜ‚òÜ\", \"Robustez outliers\": \"‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ\",\n                  \"Interpretabilidad\": \"‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ\", \"M√©tricas flexibles\": \"‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ\"},\n}\n\ndf_resumen = pd.DataFrame(resumen).T\nprint(df_resumen.to_string())\n\nprint(\"\"\"\nCu√°ndo usar K-Means:\n  ‚úì Dataset grande (n > 50.000)\n  ‚úì Variables num√©ricas continuas bien escaladas\n  ‚úì No hay outliers extremos\n  ‚úì Velocidad es prioritaria\n\nCu√°ndo usar K-Medoids:\n  ‚úì Outliers presentes o sospechados\n  ‚úì Necesitas representantes reales (presentaciones, CRM)\n  ‚úì Distancias no-euclidianas (datos mixtos, texto, etc.)\n  ‚úì Dataset peque√±o-mediano (n < 10.000 con PAM)\n\"\"\")"
  },
  {
   "cell_type": "markdown",
   "id": "md_4a0ba1",
   "metadata": {},
   "source": "---\n\n## NOTAS DE PRODUCCI√ìN\n\n### Para las slides\n\n- **Slide 1:** Portada K-Means. Animaci√≥n de los 4 pasos de Lloyd.\n- **Slide 2:** Los 4 pasos del algoritmo con pseudoc√≥digo y f√≥rmulas.\n- **Slide 3:** K-Means++ ‚Äî diagrama mostrando la probabilidad proporcional a D(x)¬≤.\n- **Slide 4:** M√©todo del codo ‚Äî gr√°fico de WCSS con anotaci√≥n del codo.\n- **Slide 5:** Las 5 limitaciones de K-Means ‚Äî tarjetas de advertencia.\n- **Slide 6:** Portada K-Medoids. El ejemplo de ingresos con la media contaminada.\n- **Slide 7:** Algoritmo PAM ‚Äî fases BUILD y SWAP con diagrama de flujo.\n- **Slide 8:** Tabla comparativa K-Means vs. K-Medoids.\n- **Slide 9:** Resultado visual del experimento de outliers (los dos scatter plots lado a lado).\n\n### Para el handout\n\n- Tabla comparativa K-Means vs. K-Medoids (criterios de selecci√≥n).\n- Pseudoc√≥digo de Lloyd (4 pasos) y pseudoc√≥digo de PAM (BUILD + SWAP).\n- Tabla variantes de K-Medoids (PAM, CLARA, CLARANS).\n- Los gr√°ficos: evoluci√≥n de centroides, m√©todo del codo, comparaci√≥n con outliers.\n- Checklist de decisi√≥n: *¬øHay outliers? ‚Üí K-Medoids. ¬øn > 50k? ‚Üí K-Means. ¬øNecesito representantes reales? ‚Üí K-Medoids.*\n\n### Para el Jupyter Notebook (ejercicios a completar por los alumnos)\n\n**Ejercicio 1 (Celda 9 ampliada):** Repetir el experimento de outliers variando el n√∫mero de outliers (0, 2, 5, 10). ¬øA partir de cu√°ntos outliers empieza K-Means a dar resultados claramente peores?\n\n**Ejercicio 2 (Celda 5 ampliada):** A√±adir la curva de Silhouette Score al gr√°fico del codo. ¬øEl k √≥ptimo seg√∫n Silhouette coincide con el del codo? (Anticipaci√≥n al Bloque 2.3.)\n\n**Ejercicio 3 (Celda 11 ampliada):** Usar `method='alternate'` en lugar de `'pam'` para K-Medoids. Comparar los medoides resultantes y el tiempo de ejecuci√≥n con `%%time`.\n\n**Ejercicio 4 (avanzado):** Implementar CLARA manualmente: (1) tomar 5 muestras aleatorias del 20% del dataset, (2) aplicar PAM a cada muestra, (3) asignar todos los puntos al medoide m√°s cercano de la mejor soluci√≥n, (4) comparar con PAM sobre el dataset completo.\n\n---\n\n## GESTI√ìN DEL TIEMPO\n\n| Segmento | Duraci√≥n | Indicador de progreso |\n|---|---|---|\n| Transici√≥n desde Bloque 1.1 | 5 min | Pregunta de conexi√≥n respondida |\n| Algoritmo de Lloyd (4 pasos) | 10 min | Diagrama en pantalla |\n| Inicializaci√≥n y K-Means++ | 5 min | Gr√°fico de variabilidad explicado |\n| M√©todo del codo + limitaciones | 10 min | Tabla de limitaciones en pantalla |\n| Pr√°ctica Celdas 1-3 (manual + evoluci√≥n) | 10 min | Gr√°fico de evoluci√≥n generado |\n| Pr√°ctica Celdas 4-7 (Mall + codo + comparativa) | 25 min | Segmentaci√≥n final interpretada |\n| **Pausa de 5 min** (si el ritmo lo permite) | 5 min | ‚Äî |\n| Motivaci√≥n K-Medoids + ejemplo num√©rico | 5 min | Pregunta ret√≥rica planteada |\n| Algoritmo PAM (BUILD + SWAP) | 10 min | Tabla PAM/CLARA/CLARANS en pantalla |\n| Tabla comparativa K-Means vs. K-Medoids | 5 min | Tabla en pantalla |\n| Pr√°ctica Celdas 8-13 (outliers + Mall + resumen) | 25 min | Gr√°fico comparativo generado |\n| **Total** | **115 min** *(~5 min de margen sobre los 110)* | |\n\n> *Nota: Si el grupo va lento en la pr√°ctica de K-Medoids, omitir la Celda 12 (impacto de normalizaci√≥n) y remitir al ejercicio como tarea.*\n\n---\n\n*Bloque 1.2 desarrollado para el m√≥dulo \"Algoritmos de Clustering\" ‚Äî M√°ster en Ciencia de Datos*"
  },
  {
   "cell_type": "markdown",
   "id": "md_beb8ed",
   "metadata": {},
   "source": "---\n## üí° Para explorar m√°s ‚Äî Ejercicios propuestos\n\nLos ejercicios pr√°cticos est√°n marcados con comentarios `# EJERCICIO` en el c√≥digo.\n\n**Entrega sugerida:** Exporta este notebook como HTML o PDF (`File ‚Üí Download ‚Üí HTML`)\ny a√±ade tus conclusiones en una celda Markdown al final de cada secci√≥n.\n\n---\n*M√°ster en Ciencia de Datos ¬∑ M√≥dulo Clustering ¬∑ Bloque 1.2*"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}