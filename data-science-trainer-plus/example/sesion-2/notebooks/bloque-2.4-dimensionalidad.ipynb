{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "md_f7ac24",
   "metadata": {},
   "source": "# Bloque 2.4 ‚Äî Reducci√≥n de Dimensionalidad para Clustering\n**M√°ster en Ciencia de Datos ¬∑ M√≥dulo: Algoritmos de Clustering**\n**Sesi√≥n 2 ¬∑ Duraci√≥n: 35 min**\n\n---\n> üìå **C√≥mo usar este notebook:**\n> Ejecuta las celdas **en orden**. Cada secci√≥n comienza con explicaci√≥n te√≥rica (en Markdown) seguida del c√≥digo correspondiente.\n> Los comentarios `# ---` delimitan ejercicios opcionales para profundizar.\n"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "code_3b3c8a",
   "metadata": {},
   "outputs": [],
   "source": "import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport matplotlib.gridspec as gridspec\nfrom sklearn.decomposition import PCA\nfrom sklearn.cluster import KMeans\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.datasets import make_blobs\nfrom sklearn.metrics import silhouette_score\nimport warnings\nwarnings.filterwarnings('ignore')\n\nnp.random.seed(42)\n\n# Generamos datos con estructura real en pocas dimensiones, embebidos en alta dim.\nn_samples = 400\nn_features_real = 3   # estructura real en 3 dimensiones\nn_features_total = 50  # datos \"observados\" en 50 dimensiones\n\n# Datos base: 4 clusters en 3D\nX_real, y_true = make_blobs(n_samples=n_samples, n_features=n_features_real,\n                             centers=4, cluster_std=0.8, random_state=42)\n\n# Embebemos en 50D: proyecci√≥n aleatoria + ruido\nproyeccion = np.random.randn(n_features_real, n_features_total)\nX_alto = X_real @ proyeccion + np.random.randn(n_samples, n_features_total) * 2.0\n\nscaler = StandardScaler()\nX_alto_scaled = scaler.fit_transform(X_alto)\n\nprint(f\"Shape datos originales (alta dim.): {X_alto_scaled.shape}\")\nprint(f\"Dimensiones reales con estructura: {n_features_real}\")"
  },
  {
   "cell_type": "markdown",
   "id": "md_ad32c3",
   "metadata": {},
   "source": "### Celda 2 ‚Äî Comparaci√≥n: clustering directo vs. con PCA"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "code_635735",
   "metadata": {},
   "outputs": [],
   "source": "from sklearn.metrics import adjusted_rand_score\n\nresultados = []\n\n# 1. K-Means directo en 50D\nkm_alto = KMeans(n_clusters=4, random_state=42, n_init=10)\nlabels_alto = km_alto.fit_predict(X_alto_scaled)\nsil_alto = silhouette_score(X_alto_scaled, labels_alto)\nari_alto = adjusted_rand_score(y_true, labels_alto)\nresultados.append({'M√©todo': 'K-Means 50D (sin PCA)', 'Silhouette': sil_alto, 'ARI': ari_alto})\n\n# 2. PCA + K-Means con distintos k de componentes\nfor n_comp in [2, 3, 5, 10, 20]:\n    pca = PCA(n_components=n_comp, random_state=42)\n    X_pca = pca.fit_transform(X_alto_scaled)\n    var_exp = pca.explained_variance_ratio_.sum()\n\n    km = KMeans(n_clusters=4, random_state=42, n_init=10)\n    labels = km.fit_predict(X_pca)\n    sil = silhouette_score(X_pca, labels)\n    ari = adjusted_rand_score(y_true, labels)\n    resultados.append({\n        'M√©todo': f'PCA({n_comp}) + K-Means',\n        'Silhouette': sil,\n        'ARI': ari,\n        'Var. explicada': f'{var_exp:.1%}'\n    })\n\ndf_resultados = pd.DataFrame(resultados)\nprint(df_resultados.to_string(index=False))"
  },
  {
   "cell_type": "markdown",
   "id": "md_1cf5d8",
   "metadata": {},
   "source": "### Celda 3 ‚Äî Curva de varianza explicada y elecci√≥n de k"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "code_424208",
   "metadata": {},
   "outputs": [],
   "source": "pca_full = PCA(random_state=42)\npca_full.fit(X_alto_scaled)\n\nvar_acum = np.cumsum(pca_full.explained_variance_ratio_)\nvar_ind = pca_full.explained_variance_ratio_\n\nfig, axes = plt.subplots(1, 2, figsize=(13, 5))\n\n# Varianza individual\nax1 = axes[0]\nax1.bar(range(1, 21), var_ind[:20] * 100, color='steelblue', alpha=0.7, label='Varianza individual')\nax1.set_xlabel('Componente principal')\nax1.set_ylabel('% Varianza explicada')\nax1.set_title('Scree Plot (primeras 20 componentes)')\nax1.axvline(x=3, color='red', linestyle='--', alpha=0.7, label='k real = 3')\nax1.legend()\n\n# Varianza acumulada\nax2 = axes[1]\nax2.plot(range(1, len(var_acum) + 1), var_acum * 100, 'o-', color='steelblue',\n         markersize=4, linewidth=2)\nax2.axhline(y=80, color='orange', linestyle='--', alpha=0.7, label='80% varianza')\nax2.axhline(y=95, color='red', linestyle='--', alpha=0.7, label='95% varianza')\n\n# Marcar los umbrales\nk_80 = np.argmax(var_acum >= 0.80) + 1\nk_95 = np.argmax(var_acum >= 0.95) + 1\nax2.axvline(x=k_80, color='orange', linestyle=':', alpha=0.7)\nax2.axvline(x=k_95, color='red', linestyle=':', alpha=0.7)\nax2.annotate(f'k={k_80}', xy=(k_80, 80), xytext=(k_80+1, 75),\n             fontsize=9, color='orange')\nax2.annotate(f'k={k_95}', xy=(k_95, 95), xytext=(k_95+1, 90),\n             fontsize=9, color='red')\n\nax2.set_xlabel('N√∫mero de componentes')\nax2.set_ylabel('% Varianza acumulada')\nax2.set_title('Varianza acumulada explicada')\nax2.legend()\nax2.set_xlim(1, 30)\n\nplt.tight_layout()\nplt.suptitle('An√°lisis de componentes principales ‚Äî Selecci√≥n de k √≥ptimo',\n             y=1.02, fontsize=13, fontweight='bold')\nplt.show()\n\nprint(f\"\\nComponentes para 80% de varianza: {k_80}\")\nprint(f\"Componentes para 95% de varianza: {k_95}\")"
  },
  {
   "cell_type": "markdown",
   "id": "md_f353fe",
   "metadata": {},
   "source": "---\n\n## Teor√≠a 3 ‚Äî t-SNE y UMAP para visualizaci√≥n (5 min)\n\n### Gui√≥n del instructor\n\n> \"Una vez hemos ejecutado el clustering, queremos *ver* los resultados. Proyectar a 2D con PCA a veces es suficiente, pero si la estructura es no lineal, podemos usar t-SNE o UMAP.\"\n\n**t-SNE (t-distributed Stochastic Neighbor Embedding)**\n\n- Preserva similitudes locales: puntos cercanos en el espacio original ‚Üí cercanos en 2D\n- No preserva distancias globales (la distancia entre clusters en 2D no es interpretable)\n- Par√°metro clave: **perplexity** (rango t√≠pico: 5‚Äì50)\n  - Bajo: estructura muy local, clusters fragmentados\n  - Alto: estructura m√°s global, clusters pueden comprimirse\n- **Computacionalmente costoso**: O(n¬≤ log n). Para n > 10.000, usar PCA primero\n- No determinista por defecto (usar `random_state`)\n- **No sirve para preprocesar clustering**: las distorsiones no lineales lo hacen inadecuado\n\n**UMAP (Uniform Manifold Approximation and Projection)**\n\n- M√°s r√°pido que t-SNE para datasets grandes\n- Mejor preservaci√≥n de la estructura global (grupos de clusters y sus relaciones)\n- Par√°metros clave:\n  - `n_neighbors` (5‚Äì50): balance local/global, an√°logo a perplexity\n  - `min_dist` (0.0‚Äì0.99): compactaci√≥n de los clusters en la proyecci√≥n\n- Tambi√©n puede usarse como reducci√≥n de dimensionalidad (no solo a 2D)\n- M√°s reproducible que t-SNE con `random_state`\n\n**Regla pr√°ctica:**\n\n```\n¬øPara qu√©?          ‚Üí Usar\nPreprocesar clustering  ‚Üí PCA\nVisualizar resultados   ‚Üí t-SNE o UMAP\nExplorar estructura     ‚Üí UMAP (m√°s r√°pido, m√°s fiel a escala global)\n```\n\n---\n\n## Pr√°ctica 2 ‚Äî Visualizaci√≥n comparativa PCA / t-SNE / UMAP (7 min)\n\n### Celda 4 ‚Äî Proyecciones comparativas con labels de clustering"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "code_0a4230",
   "metadata": {},
   "outputs": [],
   "source": "from sklearn.manifold import TSNE\n\ntry:\n    import umap\n    UMAP_DISPONIBLE = True\nexcept ImportError:\n    UMAP_DISPONIBLE = False\n    print(\"UMAP no disponible. Instalar con: pip install umap-learn\")\n\n# Usamos PCA a 3 componentes para clustering\npca3 = PCA(n_components=3, random_state=42)\nX_pca3 = pca3.fit_transform(X_alto_scaled)\n\nkm_final = KMeans(n_clusters=4, random_state=42, n_init=10)\nlabels_final = km_final.fit_predict(X_pca3)\n\ncolores = ['#E74C3C', '#3498DB', '#2ECC71', '#9B59B6']\ncolor_map = [colores[l] for l in labels_final]\n\n# N√∫mero de proyecciones disponibles\nn_plots = 3 if UMAP_DISPONIBLE else 2\nfig, axes = plt.subplots(1, n_plots, figsize=(6 * n_plots, 5))\n\n# --- PCA 2D ---\npca2 = PCA(n_components=2, random_state=42)\nX_pca2 = pca2.fit_transform(X_alto_scaled)\n\naxes[0].scatter(X_pca2[:, 0], X_pca2[:, 1], c=color_map, alpha=0.6, s=30)\naxes[0].set_title(f'PCA (var. explicada: {pca2.explained_variance_ratio_.sum():.1%})',\n                  fontsize=11)\naxes[0].set_xlabel('PC1')\naxes[0].set_ylabel('PC2')\n\n# --- t-SNE ---\nprint(\"Calculando t-SNE... (puede tardar ~10s)\")\ntsne = TSNE(n_components=2, perplexity=30, random_state=42, n_iter=1000)\nX_tsne = tsne.fit_transform(X_alto_scaled)\n\naxes[1].scatter(X_tsne[:, 0], X_tsne[:, 1], c=color_map, alpha=0.6, s=30)\naxes[1].set_title('t-SNE (perplexity=30)', fontsize=11)\naxes[1].set_xlabel('Dim 1')\naxes[1].set_ylabel('Dim 2')\n\n# --- UMAP (si disponible) ---\nif UMAP_DISPONIBLE:\n    reducer = umap.UMAP(n_components=2, n_neighbors=15, min_dist=0.1,\n                        random_state=42)\n    X_umap = reducer.fit_transform(X_alto_scaled)\n\n    axes[2].scatter(X_umap[:, 0], X_umap[:, 1], c=color_map, alpha=0.6, s=30)\n    axes[2].set_title('UMAP (n_neighbors=15, min_dist=0.1)', fontsize=11)\n    axes[2].set_xlabel('Dim 1')\n    axes[2].set_ylabel('Dim 2')\n\n# Leyenda de clusters\nfrom matplotlib.lines import Line2D\nlegend_handles = [Line2D([0], [0], marker='o', color='w',\n                          markerfacecolor=colores[k], markersize=10,\n                          label=f'Cluster {k}')\n                  for k in range(4)]\naxes[0].legend(handles=legend_handles, loc='upper right', fontsize=8)\n\nplt.suptitle('Comparaci√≥n de proyecciones 2D con labels de K-Means\\n(datos 50D ‚Üí 2D)',\n             fontsize=13, fontweight='bold')\nplt.tight_layout()\nplt.show()"
  },
  {
   "cell_type": "markdown",
   "id": "md_7e4b17",
   "metadata": {},
   "source": "### Celda 5 ‚Äî Efecto de perplexity en t-SNE (exploraci√≥n pedag√≥gica)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "code_f4b2f9",
   "metadata": {},
   "outputs": [],
   "source": "fig, axes = plt.subplots(1, 4, figsize=(18, 4))\nperplexities = [5, 15, 30, 50]\n\nfor ax, perp in zip(axes, perplexities):\n    tsne_p = TSNE(n_components=2, perplexity=perp, random_state=42, n_iter=500)\n    X_p = tsne_p.fit_transform(X_pca3)  # usamos PCA3 para acelerar\n\n    ax.scatter(X_p[:, 0], X_p[:, 1], c=color_map, alpha=0.5, s=20)\n    ax.set_title(f'perplexity = {perp}', fontsize=10)\n    ax.set_xticks([])\n    ax.set_yticks([])\n\nplt.suptitle('Efecto del par√°metro perplexity en t-SNE\\n'\n             '(los clusters son reales ‚Äî la distorsi√≥n es del m√©todo)',\n             fontsize=12, fontweight='bold')\nplt.tight_layout()\nplt.show()\n\nprint(\"\\n‚ö†Ô∏è  ADVERTENCIA PEDAG√ìGICA:\")\nprint(\"Las distancias ENTRE clusters en t-SNE NO son interpretables.\")\nprint(\"La distancia DENTRO de un cluster tampoco es directamente comparable entre clusters.\")\nprint(\"t-SNE solo preserva relaciones de vecindad LOCAL.\")"
  },
  {
   "cell_type": "markdown",
   "id": "md_235f3d",
   "metadata": {},
   "source": "### Celda 6 ‚Äî Pipeline completo recomendado"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "code_0145d0",
   "metadata": {},
   "outputs": [],
   "source": "# ============================================================\n# PIPELINE RECOMENDADO: PCA ‚Üí Clustering ‚Üí t-SNE/UMAP para viz\n# ============================================================\n\nfrom sklearn.pipeline import Pipeline\n\n# Paso 1: Estandarizar\nscaler_pipe = StandardScaler()\nX_pipe = scaler_pipe.fit_transform(X_alto)\n\n# Paso 2: PCA para preprocesar clustering\npca_pipe = PCA(n_components=0.90, random_state=42)  # retener 90% varianza\nX_reduced = pca_pipe.fit_transform(X_pipe)\nn_comp_seleccionados = pca_pipe.n_components_\nvar_ret = pca_pipe.explained_variance_ratio_.sum()\nprint(f\"PCA: {X_pipe.shape[1]}D ‚Üí {n_comp_seleccionados}D \"\n      f\"(varianza retenida: {var_ret:.1%})\")\n\n# Paso 3: Clustering en espacio reducido\nkm_pipe = KMeans(n_clusters=4, random_state=42, n_init=10)\nlabels_pipe = km_pipe.fit_predict(X_reduced)\nsil_pipe = silhouette_score(X_reduced, labels_pipe)\nari_pipe = adjusted_rand_score(y_true, labels_pipe)\nprint(f\"K-Means: Silhouette = {sil_pipe:.3f} | ARI = {ari_pipe:.3f}\")\n\n# Paso 4: t-SNE solo para visualizaci√≥n del resultado\ntsne_viz = TSNE(n_components=2, perplexity=30, random_state=42)\nX_viz = tsne_viz.fit_transform(X_pipe)  # proyectamos los datos originales\n\nplt.figure(figsize=(8, 6))\nscatter = plt.scatter(X_viz[:, 0], X_viz[:, 1], c=labels_pipe,\n                      cmap='Set1', alpha=0.7, s=40)\nplt.colorbar(scatter, label='Cluster')\nplt.title(f'Pipeline completo: PCA({n_comp_seleccionados}D) ‚Üí K-Means ‚Üí t-SNE viz\\n'\n          f'ARI={ari_pipe:.3f} | Silhouette={sil_pipe:.3f}',\n          fontsize=11)\nplt.xlabel('t-SNE dim 1')\nplt.ylabel('t-SNE dim 2')\nplt.tight_layout()\nplt.show()\n\nprint(\"\\nüìå RESUMEN DEL PIPELINE:\")\nprint(f\"   Datos originales:  {X_alto.shape}\")\nprint(f\"   Tras PCA (90% var): {X_reduced.shape}\")\nprint(f\"   Clustering: K-Means k=4\")\nprint(f\"   Visualizaci√≥n: t-SNE 2D\")\nprint(f\"   ‚ö†Ô∏è  t-SNE se usa SOLO para visualizar, no afecta al clustering\")"
  },
  {
   "cell_type": "markdown",
   "id": "md_548a46",
   "metadata": {},
   "source": "---"
  },
  {
   "cell_type": "markdown",
   "id": "md_224e34",
   "metadata": {},
   "source": "---\n## üí° Para explorar m√°s ‚Äî Ejercicios propuestos\n\nLos ejercicios pr√°cticos est√°n marcados con comentarios `# EJERCICIO` en el c√≥digo.\n\n**Entrega sugerida:** Exporta este notebook como HTML o PDF (`File ‚Üí Download ‚Üí HTML`)\ny a√±ade tus conclusiones en una celda Markdown al final de cada secci√≥n.\n\n---\n*M√°ster en Ciencia de Datos ¬∑ M√≥dulo Clustering ¬∑ Bloque 2.4*"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}