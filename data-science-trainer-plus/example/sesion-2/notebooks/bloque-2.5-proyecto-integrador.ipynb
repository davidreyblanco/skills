{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "md_da05f9",
   "metadata": {},
   "source": "# Bloque 2.5 â€” Proyecto Integrador: SegmentaciÃ³n de Clientes\n**MÃ¡ster en Ciencia de Datos Â· MÃ³dulo: Algoritmos de Clustering**\n**SesiÃ³n 2 Â· DuraciÃ³n: 55 min**\n\n---\n> ðŸ“Œ **CÃ³mo usar este notebook:**\n> Ejecuta las celdas **en orden**. Cada secciÃ³n comienza con explicaciÃ³n teÃ³rica (en Markdown) seguida del cÃ³digo correspondiente.\n> Los comentarios `# ---` delimitan ejercicios opcionales para profundizar.\n"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "code_978b44",
   "metadata": {},
   "outputs": [],
   "source": "import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom datetime import datetime, timedelta\nimport warnings\nwarnings.filterwarnings('ignore')\n\nnp.random.seed(42)\n\n# -----------------------------------------------\n# GeneraciÃ³n de dataset sintÃ©tico de transacciones\n# -----------------------------------------------\nn_clientes = 1500\nfecha_referencia = datetime(2024, 12, 31)\n\n# Perfiles latentes (5 segmentos reales que intentaremos descubrir)\nperfiles = {\n    'Champions':     {'n': 250, 'recencia': (1, 30),   'freq': (15, 30),  'valor': (500, 2000)},\n    'Leales':        {'n': 350, 'recencia': (15, 60),  'freq': (8, 20),   'valor': (200, 800)},\n    'En riesgo':     {'n': 300, 'recencia': (90, 180), 'freq': (5, 15),   'valor': (150, 600)},\n    'Hibernando':    {'n': 350, 'recencia': (180, 365),'freq': (1, 5),    'valor': (50, 250)},\n    'Perdidos':      {'n': 250, 'recencia': (300, 365),'freq': (1, 3),    'valor': (20, 100)},\n}\n\nfilas = []\nfor perfil, params in perfiles.items():\n    n = params['n']\n    recencias = np.random.randint(*params['recencia'], n)\n    frecuencias = np.random.randint(*params['freq'], n)\n    valores_medios = np.random.uniform(*params['valor'], n)\n    n_tickets = np.random.randint(1, 20, n)\n    devoluciones = np.random.uniform(0, 0.15 if perfil != 'Perdidos' else 0.3, n)\n    categorias_unicas = np.random.randint(1, 8, n)\n\n    for i in range(n):\n        filas.append({\n            'CustomerID': f'C{len(filas)+1:05d}',\n            'Recencia': recencias[i],\n            'Frecuencia': frecuencias[i],\n            'Valor_total': round(valores_medios[i] * n_tickets[i] / 5, 2),\n            'Valor_medio_ticket': round(valores_medios[i], 2),\n            'Tasa_devolucion': round(devoluciones[i], 3),\n            'Categorias_distintas': categorias_unicas[i],\n            'Perfil_real': perfil\n        })\n\ndf = pd.DataFrame(filas).sample(frac=1, random_state=42).reset_index(drop=True)\nprint(\"Shape del dataset:\", df.shape)\nprint(\"\\nDistribuciÃ³n de perfiles reales (solo para validaciÃ³n al final):\")\nprint(df['Perfil_real'].value_counts())\nprint(\"\\nPrimeras filas:\")\ndf.drop(columns='Perfil_real').head()"
  },
  {
   "cell_type": "markdown",
   "id": "md_f0a5ed",
   "metadata": {},
   "source": "### Celda 2 â€” EDA visual"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "code_7d9fa3",
   "metadata": {},
   "outputs": [],
   "source": "features_numericas = ['Recencia', 'Frecuencia', 'Valor_total',\n                      'Valor_medio_ticket', 'Tasa_devolucion', 'Categorias_distintas']\n\nfig, axes = plt.subplots(2, 3, figsize=(15, 8))\naxes = axes.ravel()\n\nfor i, col in enumerate(features_numericas):\n    axes[i].hist(df[col], bins=40, color='steelblue', alpha=0.7, edgecolor='white')\n    axes[i].set_title(f'{col}', fontsize=11)\n    axes[i].set_xlabel('Valor')\n    axes[i].set_ylabel('Frecuencia')\n\n    # AÃ±adir estadÃ­sticos\n    median_val = df[col].median()\n    mean_val = df[col].mean()\n    axes[i].axvline(median_val, color='orange', linestyle='--', linewidth=1.5,\n                    label=f'Mediana: {median_val:.1f}')\n    axes[i].axvline(mean_val, color='red', linestyle='--', linewidth=1.5,\n                    label=f'Media: {mean_val:.1f}')\n    axes[i].legend(fontsize=8)\n\nplt.suptitle('EDA â€” Distribuciones de variables del cliente', fontsize=13, fontweight='bold')\nplt.tight_layout()\nplt.show()\n\n# DetecciÃ³n de outliers\nprint(\"\\nðŸ“Š EstadÃ­sticos descriptivos:\")\nprint(df[features_numericas].describe().round(2).to_string())\n\n# Skewness\nprint(\"\\nðŸ“ AsimetrÃ­a (skewness):\")\nfor col in features_numericas:\n    skew = df[col].skew()\n    flag = \" âš ï¸ Alta asimetrÃ­a\" if abs(skew) > 1 else \"\"\n    print(f\"   {col:30s}: {skew:+.3f}{flag}\")"
  },
  {
   "cell_type": "markdown",
   "id": "md_69ed9d",
   "metadata": {},
   "source": "---\n\n## Fase 2 â€” IngenierÃ­a de features: RFM+ (10 min)\n\n### GuiÃ³n del instructor\n\n> \"La transformaciÃ³n de variables crudas en features significativas es, en muchos casos, la parte mÃ¡s importante del proyecto. AquÃ­ aplicamos la lÃ³gica RFM (Recency, Frequency, Monetary), un framework clÃ¡sico de marketing adaptado.\"\n\n**Â¿QuÃ© es RFM?**\n\n| DimensiÃ³n | Pregunta | Variable en nuestro dataset |\n|---|---|---|\n| **R**ecency | Â¿CuÃ¡ndo comprÃ³ por Ãºltima vez? | `Recencia` (dÃ­as) |\n| **F**requency | Â¿CuÃ¡ntas veces ha comprado? | `Frecuencia` (transacciones) |\n| **M**onetary | Â¿CuÃ¡nto dinero ha gastado? | `Valor_total` (â‚¬) |\n\n**Extensiones (+):**\n- `Valor_medio_ticket`: distingue compradores frecuentes-baratos de infrecuentes-premium\n- `Tasa_devolucion`: indicador de satisfacciÃ³n/fraude\n- `Categorias_distintas`: breadth de interÃ©s del cliente\n\n### Celda 3 â€” TransformaciÃ³n logarÃ­tmica y estandarizaciÃ³n"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "code_6edf78",
   "metadata": {},
   "outputs": [],
   "source": "from sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\n\n# Las variables monetarias suelen tener distribuciÃ³n log-normal\nfeatures_log = ['Recencia', 'Valor_total', 'Valor_medio_ticket']\nfeatures_directas = ['Frecuencia', 'Tasa_devolucion', 'Categorias_distintas']\n\ndf_proc = df.copy()\n\n# TransformaciÃ³n log (suavizar colas largas)\nfor col in features_log:\n    df_proc[f'{col}_log'] = np.log1p(df_proc[col])\n\n# Dataset final de features\nfeatures_modelo = [f'{c}_log' for c in features_log] + features_directas\nX_raw = df_proc[features_modelo].values\n\n# EstandarizaciÃ³n\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X_raw)\n\nprint(\"Features del modelo:\")\nfor i, f in enumerate(features_modelo):\n    print(f\"   [{i+1}] {f}\")\nprint(f\"\\nShape: {X_scaled.shape}\")\n\n# CorrelaciÃ³n entre features\nfig, ax = plt.subplots(figsize=(8, 6))\ncorr = pd.DataFrame(X_scaled, columns=features_modelo).corr()\nmask = np.triu(np.ones_like(corr, dtype=bool))\nsns.heatmap(corr, mask=mask, annot=True, fmt='.2f', cmap='RdBu_r',\n            center=0, ax=ax, square=True, vmin=-1, vmax=1)\nax.set_title('CorrelaciÃ³n entre features (post-escala)', fontsize=12)\nplt.tight_layout()\nplt.show()"
  },
  {
   "cell_type": "markdown",
   "id": "md_7dea42",
   "metadata": {},
   "source": "### Celda 4 â€” DecisiÃ³n: Â¿usar PCA?"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "code_691062",
   "metadata": {},
   "outputs": [],
   "source": "# AnÃ¡lisis de varianza explicada\npca_check = PCA(random_state=42)\npca_check.fit(X_scaled)\n\nvar_acum = np.cumsum(pca_check.explained_variance_ratio_)\nk_90 = np.argmax(var_acum >= 0.90) + 1\n\nprint(f\"Dimensiones originales: {X_scaled.shape[1]}\")\nprint(f\"Componentes para retener 90% varianza: {k_90}\")\nprint(f\"Varianza retenida con todas las componentes: 100%\")\n\n# Con 6 features no hay maldiciÃ³n severa, pero PCA puede ayudar\n# si hay alta correlaciÃ³n entre variables\nif k_90 <= 4:\n    print(\"\\nâ†’ Usaremos PCA para reducir a\", k_90, \"componentes\")\n    pca_final = PCA(n_components=k_90, random_state=42)\n    X_modelo = pca_final.fit_transform(X_scaled)\nelse:\n    print(f\"\\nâ†’ Con solo {X_scaled.shape[1]} features y {k_90} comp. para 90%, \"\n          \"trabajamos directamente con las features originales estandarizadas\")\n    X_modelo = X_scaled\n    pca_final = None\n\nprint(f\"Shape datos para clustering: {X_modelo.shape}\")"
  },
  {
   "cell_type": "markdown",
   "id": "md_ebb48d",
   "metadata": {},
   "source": "---\n\n## Fase 3 â€” SelecciÃ³n del algoritmo (10 min)\n\n### GuiÃ³n del instructor\n\n> \"Ahora aplicamos lo aprendido en el Bloque 2.3: no elegimos un algoritmo arbitrariamente, sino que comparamos varios con mÃ©tricas objetivas.\"\n\n### Celda 5 â€” ComparaciÃ³n multialgoritmo"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "code_144096",
   "metadata": {},
   "outputs": [],
   "source": "from sklearn.cluster import KMeans, AgglomerativeClustering, DBSCAN\nfrom sklearn.mixture import GaussianMixture\nfrom sklearn.metrics import silhouette_score, davies_bouldin_score, calinski_harabasz_score\nfrom sklearn_extra.cluster import KMedoids\n\nresultados_comparacion = []\n\nk_candidatos = [3, 4, 5, 6]\n\nprint(\"Evaluando algoritmos...\\n\")\n\nfor k in k_candidatos:\n    # K-Means\n    km = KMeans(n_clusters=k, random_state=42, n_init=15)\n    lbl = km.fit_predict(X_modelo)\n    resultados_comparacion.append({\n        'Algoritmo': f'K-Means k={k}',\n        'k': k,\n        'Silhouette': silhouette_score(X_modelo, lbl),\n        'DBI': davies_bouldin_score(X_modelo, lbl),\n        'CHI': calinski_harabasz_score(X_modelo, lbl),\n        'labels': lbl\n    })\n\n    # GMM\n    gmm = GaussianMixture(n_components=k, covariance_type='full',\n                          random_state=42, n_init=5)\n    lbl = gmm.fit_predict(X_modelo)\n    resultados_comparacion.append({\n        'Algoritmo': f'GMM k={k}',\n        'k': k,\n        'Silhouette': silhouette_score(X_modelo, lbl),\n        'DBI': davies_bouldin_score(X_modelo, lbl),\n        'CHI': calinski_harabasz_score(X_modelo, lbl),\n        'labels': lbl\n    })\n\ndf_comp = pd.DataFrame(resultados_comparacion)\n\n# Normalizar para score compuesto\nfrom sklearn.preprocessing import MinMaxScaler\n\nmms = MinMaxScaler()\ndf_comp['sil_norm'] = mms.fit_transform(df_comp[['Silhouette']])\ndf_comp['dbi_norm_inv'] = 1 - mms.fit_transform(df_comp[['DBI']])\ndf_comp['chi_norm'] = mms.fit_transform(df_comp[['CHI']])\ndf_comp['Score_compuesto'] = (df_comp['sil_norm'] +\n                               df_comp['dbi_norm_inv'] +\n                               df_comp['chi_norm']) / 3\n\n# Mostrar ranking\ncols_display = ['Algoritmo', 'Silhouette', 'DBI', 'CHI', 'Score_compuesto']\nprint(df_comp[cols_display].sort_values('Score_compuesto', ascending=False)\n      .round(4).to_string(index=False))\n\n# SelecciÃ³n del mejor\nmejor = df_comp.sort_values('Score_compuesto', ascending=False).iloc[0]\nprint(f\"\\nâœ… Mejor configuraciÃ³n: {mejor['Algoritmo']} \"\n      f\"(Score compuesto: {mejor['Score_compuesto']:.4f})\")"
  },
  {
   "cell_type": "markdown",
   "id": "md_7c36e8",
   "metadata": {},
   "source": "### Celda 6 â€” Ajuste fino del modelo seleccionado"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "code_05cf9c",
   "metadata": {},
   "outputs": [],
   "source": "# Ajustamos K-Means con el k Ã³ptimo encontrado\nk_optimo = int(df_comp.sort_values('Score_compuesto', ascending=False).iloc[0]['k'])\nalgoritmo_optimo = df_comp.sort_values('Score_compuesto', ascending=False).iloc[0]['Algoritmo']\n\nprint(f\"Modelo final: {algoritmo_optimo}\")\nprint(f\"k Ã³ptimo: {k_optimo}\")\n\n# Re-entrenar con mÃ¡s inits para mayor estabilidad\nif 'K-Means' in algoritmo_optimo:\n    modelo_final = KMeans(n_clusters=k_optimo, random_state=42, n_init=30)\nelse:\n    modelo_final = GaussianMixture(n_components=k_optimo, covariance_type='full',\n                                   random_state=42, n_init=15)\n\nlabels_finales = modelo_final.fit_predict(X_modelo)\ndf_proc['Segmento'] = labels_finales\n\nprint(f\"\\nDistribuciÃ³n de segmentos:\")\nprint(df_proc['Segmento'].value_counts().sort_index())\n\nsil = silhouette_score(X_modelo, labels_finales)\ndbi = davies_bouldin_score(X_modelo, labels_finales)\nchi = calinski_harabasz_score(X_modelo, labels_finales)\nprint(f\"\\nMÃ©tricas modelo final:\")\nprint(f\"   Silhouette:  {sil:.4f}\")\nprint(f\"   Davies-Bouldin: {dbi:.4f}\")\nprint(f\"   Calinski-Harabasz: {chi:.1f}\")"
  },
  {
   "cell_type": "markdown",
   "id": "md_d03218",
   "metadata": {},
   "source": "---\n\n## Fase 4 â€” Perfilado e interpretaciÃ³n (10 min)\n\n### GuiÃ³n del instructor\n\n> \"Tenemos los segmentos. Ahora viene la parte que mÃ¡s le importa al negocio: Â¿quÃ© significa cada nÃºmero? Un clustering sin interpretaciÃ³n es inÃºtil.\"\n\n### Celda 7 â€” Perfiles estadÃ­sticos"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "code_19487c",
   "metadata": {},
   "outputs": [],
   "source": "# Perfil estadÃ­stico por segmento\nperfil = df_proc.groupby('Segmento')[features_numericas].agg(['mean', 'median']).round(2)\n\n# Tabla de medias para interpretaciÃ³n\nmedias = df_proc.groupby('Segmento')[features_numericas].mean().round(2)\nmedias['N_clientes'] = df_proc.groupby('Segmento').size()\nmedias['%_clientes'] = (medias['N_clientes'] / len(df_proc) * 100).round(1)\n\nprint(\"ðŸ“Š Perfil medio por segmento:\\n\")\nprint(medias.to_string())"
  },
  {
   "cell_type": "markdown",
   "id": "md_0d9224",
   "metadata": {},
   "source": "### Celda 8 â€” Radar chart y etiquetado de segmentos"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "code_e6ea52",
   "metadata": {},
   "outputs": [],
   "source": "from matplotlib.patches import FancyArrowPatch\n\n# Normalizar features para radar chart (0-1)\nmedias_norm = medias[features_numericas].copy()\n\n# Recencia: invertir (menor recencia = mejor â†’ mÃ¡s activo)\nmedias_norm['Recencia'] = 1 - (medias_norm['Recencia'] - medias_norm['Recencia'].min()) / \\\n                              (medias_norm['Recencia'].max() - medias_norm['Recencia'].min())\n# Tasa devoluciÃ³n: invertir\nmedias_norm['Tasa_devolucion'] = 1 - (medias_norm['Tasa_devolucion'] -\n                                       medias_norm['Tasa_devolucion'].min()) / \\\n                                      (medias_norm['Tasa_devolucion'].max() -\n                                       medias_norm['Tasa_devolucion'].min() + 1e-9)\n\nfor col in ['Frecuencia', 'Valor_total', 'Valor_medio_ticket', 'Categorias_distintas']:\n    min_v = medias_norm[col].min()\n    max_v = medias_norm[col].max()\n    medias_norm[col] = (medias_norm[col] - min_v) / (max_v - min_v + 1e-9)\n\n# Renombrar para el grÃ¡fico\nlabels_radar = ['Actividad\\nreciente', 'Frecuencia', 'Valor\\ntotal',\n                'Ticket\\nmedio', 'Fidelidad\\n(dev. inv.)', 'Amplitud\\ncategorÃ­as']\n\nn_seg = k_optimo\ncolores_seg = plt.cm.Set2(np.linspace(0, 1, n_seg))\n\nfig = plt.figure(figsize=(14, 6))\n\n# Radar chart\nax_radar = fig.add_subplot(121, polar=True)\nangles = np.linspace(0, 2 * np.pi, len(labels_radar), endpoint=False).tolist()\nangles += angles[:1]\n\nfor seg in range(n_seg):\n    vals = medias_norm.iloc[seg][features_numericas].tolist()\n    # Reordenar para el radar\n    vals_radar = [medias_norm.iloc[seg]['Recencia'],\n                  medias_norm.iloc[seg]['Frecuencia'],\n                  medias_norm.iloc[seg]['Valor_total'],\n                  medias_norm.iloc[seg]['Valor_medio_ticket'],\n                  medias_norm.iloc[seg]['Tasa_devolucion'],\n                  medias_norm.iloc[seg]['Categorias_distintas']]\n    vals_radar += vals_radar[:1]\n    ax_radar.plot(angles, vals_radar, 'o-', linewidth=2,\n                  color=colores_seg[seg], label=f'Segmento {seg}')\n    ax_radar.fill(angles, vals_radar, alpha=0.15, color=colores_seg[seg])\n\nax_radar.set_xticks(angles[:-1])\nax_radar.set_xticklabels(labels_radar, size=9)\nax_radar.set_title('Radar: perfil por segmento', fontsize=11, pad=20)\nax_radar.legend(loc='upper right', bbox_to_anchor=(1.3, 1.1), fontsize=9)\n\n# Barras de tamaÃ±o\nax_bar = fig.add_subplot(122)\nsizes = [medias.loc[s, 'N_clientes'] for s in range(n_seg)]\nbars = ax_bar.barh(range(n_seg),\n                   sizes,\n                   color=colores_seg,\n                   edgecolor='white', linewidth=1.5)\nax_bar.set_yticks(range(n_seg))\nax_bar.set_yticklabels([f'Segmento {s}' for s in range(n_seg)], fontsize=10)\nax_bar.set_xlabel('NÃºmero de clientes')\nax_bar.set_title('TamaÃ±o de cada segmento', fontsize=11)\n\nfor bar, size in zip(bars, sizes):\n    pct = size / len(df_proc) * 100\n    ax_bar.text(bar.get_width() + 5, bar.get_y() + bar.get_height()/2,\n                f'{size} ({pct:.1f}%)', va='center', fontsize=9)\n\nplt.tight_layout()\nplt.suptitle('CaracterizaciÃ³n de segmentos de clientes',\n             fontsize=13, fontweight='bold', y=1.02)\nplt.show()"
  },
  {
   "cell_type": "markdown",
   "id": "md_d89aec",
   "metadata": {},
   "source": "### Celda 9 â€” AsignaciÃ³n de nombres y validaciÃ³n"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "code_6f1745",
   "metadata": {},
   "outputs": [],
   "source": "# El instructor facilita la interpretaciÃ³n con los estudiantes\n# basÃ¡ndose en el radar chart anterior\n\n# Ejemplo de asignaciÃ³n de nombres (ajustar segÃºn los resultados reales)\n# Los estudiantes deben proponer sus propios nombres basÃ¡ndose en los datos\n\n# Tabla de interpretaciÃ³n guiada\nprint(\"=\" * 65)\nprint(\"GUÃA DE INTERPRETACIÃ“N DE SEGMENTOS\")\nprint(\"=\" * 65)\nprint()\nprint(\"Para asignar nombre a cada segmento, considera:\")\nprint()\nprint(\"  Alta Actividad Reciente + Alta Frecuencia + Alto Valor\")\nprint(\"  â†’ Champions / Clientes VIP activos\")\nprint()\nprint(\"  Media Actividad + Media Frecuencia + Medio Valor\")\nprint(\"  â†’ Clientes Leales / Base estable\")\nprint()\nprint(\"  Baja Actividad Reciente + Alta Frecuencia histÃ³rica\")\nprint(\"  â†’ En riesgo de abandono / Despertar necesario\")\nprint()\nprint(\"  Muy Baja Actividad + Baja Frecuencia + Bajo Valor\")\nprint(\"  â†’ Hibernando / PrÃ¡cticamente perdidos\")\nprint()\nprint(\"  Alta Tasa DevoluciÃ³n\")\nprint(\"  â†’ Perfil de riesgo / InsatisfacciÃ³n o fraude\")\nprint()\n\n# Mapeo propuesto (adaptable)\nnombres_segmentos_base = {\n    0: 'Segmento A', 1: 'Segmento B', 2: 'Segmento C',\n    3: 'Segmento D', 4: 'Segmento E'\n}\n\n# ValidaciÃ³n con ARI (usando los perfiles reales que solo el instructor conoce)\nfrom sklearn.metrics import adjusted_rand_score\nari = adjusted_rand_score(df['Perfil_real'], labels_finales)\nprint(f\"ARI vs. perfiles reales (solo para validaciÃ³n interna): {ari:.4f}\")\nprint(\"(Un ARI > 0.6 indica que el clustering captura bien los segmentos reales)\")"
  },
  {
   "cell_type": "markdown",
   "id": "md_df2f3a",
   "metadata": {},
   "source": "---\n\n## Fase 5 â€” PresentaciÃ³n de resultados (10 min)\n\n### GuiÃ³n del instructor\n\n> \"El Ãºltimo paso, y uno que se subestima en la formaciÃ³n tÃ©cnica: comunicar los resultados de forma que el equipo de marketing pueda actuar. Un cluster sin nombre y sin acciÃ³n es ruido.\"\n\n### Celda 10 â€” Dashboard final y recomendaciones"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "code_75e4a3",
   "metadata": {},
   "outputs": [],
   "source": "# VisualizaciÃ³n TSNE final de los segmentos\nfrom sklearn.manifold import TSNE\n\n# AsignaciÃ³n de nombres ilustrativos (el instructor ajusta tras ver los perfiles)\n# En el aula: pedir a los estudiantes que propongan nombres\nnombres_ejemplo = {\n    0: 'â­ Champions',\n    1: 'ðŸ’™ Leales',\n    2: 'âš ï¸ En riesgo',\n    3: 'ðŸ’¤ Hibernando',\n    4: 'ðŸš« Perdidos'\n}\n# Solo aplicamos si k_optimo == 5\nif k_optimo == 5:\n    df_proc['Nombre_segmento'] = df_proc['Segmento'].map(nombres_ejemplo)\nelse:\n    df_proc['Nombre_segmento'] = 'Segmento ' + df_proc['Segmento'].astype(str)\n\n# t-SNE para visualizaciÃ³n\nprint(\"Calculando proyecciÃ³n t-SNE para visualizaciÃ³n final...\")\ntsne = TSNE(n_components=2, perplexity=40, random_state=42)\nX_tsne = tsne.fit_transform(X_scaled)\n\nfig, axes = plt.subplots(1, 2, figsize=(15, 6))\n\n# Scatter con segmentos\ncolores_finales = plt.cm.Set2(np.linspace(0, 1, k_optimo))\nfor seg in range(k_optimo):\n    mask = df_proc['Segmento'] == seg\n    nombre = df_proc.loc[mask, 'Nombre_segmento'].iloc[0]\n    axes[0].scatter(X_tsne[mask, 0], X_tsne[mask, 1],\n                    c=[colores_finales[seg]], alpha=0.6, s=20, label=nombre)\n\naxes[0].set_title('SegmentaciÃ³n de clientes (proyecciÃ³n t-SNE)', fontsize=11)\naxes[0].set_xlabel('DimensiÃ³n 1')\naxes[0].set_ylabel('DimensiÃ³n 2')\naxes[0].legend(fontsize=9, markerscale=2)\n\n# Heatmap de perfiles\nperfil_heatmap = medias[features_numericas].copy()\n# Normalizar por columna para el heatmap\nperfil_heatmap_norm = (perfil_heatmap - perfil_heatmap.min()) / \\\n                      (perfil_heatmap.max() - perfil_heatmap.min() + 1e-9)\n\nsns.heatmap(perfil_heatmap_norm, annot=perfil_heatmap.values.round(1),\n            fmt='.1f', cmap='YlOrRd', ax=axes[1],\n            linewidths=0.5, linecolor='white',\n            xticklabels=[c.replace('_', '\\n') for c in features_numericas],\n            yticklabels=[df_proc.loc[df_proc['Segmento']==s, 'Nombre_segmento'].iloc[0]\n                         for s in range(k_optimo)])\n\naxes[1].set_title('Perfil de segmentos (valores reales, escala por color)', fontsize=11)\naxes[1].set_xlabel('')\n\nplt.suptitle('Dashboard final â€” SegmentaciÃ³n de clientes e-commerce',\n             fontsize=14, fontweight='bold')\nplt.tight_layout()\nplt.show()"
  },
  {
   "cell_type": "markdown",
   "id": "md_978d5a",
   "metadata": {},
   "source": "### Celda 11 â€” Tabla de acciones de marketing"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "code_2a28bf",
   "metadata": {},
   "outputs": [],
   "source": "# Tabla de recomendaciones (para generar PDF/presentaciÃ³n)\nacciones = pd.DataFrame({\n    'Segmento': ['Champions', 'Leales', 'En riesgo', 'Hibernando', 'Perdidos'],\n    'DescripciÃ³n': [\n        'Alta recencia, frecuencia y valor',\n        'Compran regularmente, valor medio',\n        'Activos antes, inactivos Ãºltimos meses',\n        'Sin compras desde 6-12 meses',\n        'Sin compras desde >12 meses, bajo valor'\n    ],\n    'AcciÃ³n recomendada': [\n        'Programa de fidelizaciÃ³n premium, early access',\n        'Descuentos por volumen, newsletter VIP',\n        'CampaÃ±a de reactivaciÃ³n, encuesta de satisfacciÃ³n',\n        'Oferta win-back agresiva, recordatorio personalizado',\n        'Evaluar coste-beneficio de campaÃ±a vs. baja de BBDD'\n    ],\n    'Canal sugerido': [\n        'Email + App push + Gestor personal',\n        'Email + SMS',\n        'Email personalizado + Retargeting',\n        'Email + SMS oferta limitada',\n        'Email (coste mÃ­nimo)'\n    ],\n    'KPI objetivo': [\n        'Mantener ticket medio, NPS > 8',\n        'Aumentar frecuencia +20%',\n        'Reactivar >30% en 90 dÃ­as',\n        'Reactivar >15% en 60 dÃ­as',\n        'ROI positivo o depurar'\n    ]\n})\n\nprint(\"=\" * 90)\nprint(\"PLAN DE ACCIÃ“N DE MARKETING POR SEGMENTO\")\nprint(\"=\" * 90)\nprint(acciones.to_string(index=False))\nprint()\nprint(f\"ðŸ“ˆ InversiÃ³n recomendada:\")\nprint(f\"   Champions ({medias.loc[0,'N_clientes'] if k_optimo>0 else 'N/A'} clientes): Alta\")\nprint(f\"   Leales: Media-Alta\")\nprint(f\"   En riesgo: Media (ROI potencial alto)\")\nprint(f\"   Hibernando: Baja-Media\")\nprint(f\"   Perdidos: Muy baja (evaluar individualmente)\")"
  },
  {
   "cell_type": "markdown",
   "id": "md_548a46",
   "metadata": {},
   "source": "---"
  },
  {
   "cell_type": "markdown",
   "id": "md_9ffbf8",
   "metadata": {},
   "source": "---\n## ðŸ’¡ Para explorar mÃ¡s â€” Ejercicios propuestos\n\nLos ejercicios prÃ¡cticos estÃ¡n marcados con comentarios `# EJERCICIO` en el cÃ³digo.\n\n**Entrega sugerida:** Exporta este notebook como HTML o PDF (`File â†’ Download â†’ HTML`)\ny aÃ±ade tus conclusiones en una celda Markdown al final de cada secciÃ³n.\n\n---\n*MÃ¡ster en Ciencia de Datos Â· MÃ³dulo Clustering Â· Bloque 2.5*"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}