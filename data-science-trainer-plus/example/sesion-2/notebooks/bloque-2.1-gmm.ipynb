{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "md_59f638",
   "metadata": {},
   "source": "# Bloque 2.1 ‚Äî Modelos de Mezcla Gaussiana (GMM + EM)\n**M√°ster en Ciencia de Datos ¬∑ M√≥dulo: Algoritmos de Clustering**\n**Sesi√≥n 2 ¬∑ Duraci√≥n: 70 min**\n\n---\n> üìå **C√≥mo usar este notebook:**\n> Ejecuta las celdas **en orden**. Cada secci√≥n comienza con explicaci√≥n te√≥rica (en Markdown) seguida del c√≥digo correspondiente.\n> Los comentarios `# ---` delimitan ejercicios opcionales para profundizar.\n"
  },
  {
   "cell_type": "markdown",
   "id": "md_a3e26a",
   "metadata": {},
   "source": "## üîß Setup y verificaci√≥n del entorno"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "code_b81cdd",
   "metadata": {},
   "outputs": [],
   "source": "# ============================================================\n# SETUP ‚Äî ejecutar siempre en primer lugar\n# ============================================================\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Verificar librer√≠as clave\nimport importlib, sys\n\nrequired = {\n    'numpy': 'numpy',\n    'pandas': 'pandas',\n    'matplotlib': 'matplotlib',\n    'seaborn': 'seaborn',\n    'sklearn': 'scikit-learn',\n    'scipy': 'scipy',\n}\n\noptional = {\n    'sklearn_extra': 'scikit-learn-extra  # pip install scikit-learn-extra',\n    'minisom': 'minisom               # pip install minisom',\n    'umap': 'umap-learn             # pip install umap-learn',\n    'hdbscan': 'hdbscan               # pip install hdbscan',\n    'yellowbrick': 'yellowbrick          # pip install yellowbrick',\n}\n\nprint(\"Librer√≠as requeridas:\")\nfor mod, pkg in required.items():\n    ok = importlib.util.find_spec(mod) is not None\n    print(f\"  {'‚úÖ' if ok else '‚ùå'} {pkg}\")\n\nprint(\"\\nLibrer√≠as opcionales:\")\nfor mod, pkg in optional.items():\n    ok = importlib.util.find_spec(mod) is not None\n    print(f\"  {'‚úÖ' if ok else '‚ö†Ô∏è '} {pkg}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "code_0cdb5d",
   "metadata": {},
   "outputs": [],
   "source": "# ============================================================\n# BLOQUE 2.1 ‚Äî Gaussian Mixture Models y el Algoritmo EM\n# ============================================================\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as mpatches\nfrom matplotlib.patches import Ellipse\nimport seaborn as sns\n\nfrom sklearn.mixture import GaussianMixture\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.datasets import make_blobs\n\nplt.rcParams['figure.figsize'] = (11, 6)\nplt.rcParams['font.size'] = 12\nsns.set_style(\"whitegrid\")\nnp.random.seed(42)\n\nprint(\"‚úì Imports correctos\")"
  },
  {
   "cell_type": "markdown",
   "id": "md_cb75fb",
   "metadata": {},
   "source": "---\n\n#### Celda 2 ‚Äî Visualizaci√≥n intuitiva: asignaci√≥n dura vs. suave"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "code_080f79",
   "metadata": {},
   "outputs": [],
   "source": "# -------------------------------------------------------\n# K-Means (duro) vs. GMM (suave) en el mismo dataset\n# -------------------------------------------------------\n\nfrom sklearn.cluster import KMeans\n\n# Dataset con zona de solapamiento entre dos clusters\nnp.random.seed(3)\nX_overlap = np.vstack([\n    np.random.multivariate_normal([0, 0], [[1.5, 0.8],[0.8, 0.6]], 200),\n    np.random.multivariate_normal([3, 2], [[1.0, -0.5],[-0.5, 0.8]], 200),\n])\n\nX_norm = StandardScaler().fit_transform(X_overlap)\n\n# K-Means\nkm = KMeans(n_clusters=2, n_init=10, random_state=0)\nlabels_km = km.fit_predict(X_norm)\n\n# GMM\ngmm = GaussianMixture(n_components=2, covariance_type='full',\n                      n_init=5, random_state=0)\ngmm.fit(X_norm)\nlabels_gmm   = gmm.predict(X_norm)\nproba_gmm    = gmm.predict_proba(X_norm)  # probabilidades de pertenencia\nincertidumbre = 1 - proba_gmm.max(axis=1)  # 0 = seguro, 0.5 = m√°xima incertidumbre\n\nfig, axes = plt.subplots(1, 3, figsize=(16, 5))\n\n# K-Means: asignaci√≥n dura\nax = axes[0]\nax.scatter(X_norm[:, 0], X_norm[:, 1], c=labels_km,\n           cmap='bwr', alpha=0.6, s=25)\nax.scatter(km.cluster_centers_[:, 0], km.cluster_centers_[:, 1],\n           c='black', marker='X', s=200, zorder=5, label='Centroides')\nax.set_title(\"K-Means ‚Äî Asignaci√≥n dura\\n(cada punto = un color, sin matices)\",\n             fontsize=10, fontweight='bold')\nax.legend(fontsize=9)\n\n# GMM: probabilidad de pertenencia al componente 0\nax = axes[1]\nsc = ax.scatter(X_norm[:, 0], X_norm[:, 1],\n                c=proba_gmm[:, 0], cmap='RdBu', alpha=0.8, s=25,\n                vmin=0, vmax=1)\nplt.colorbar(sc, ax=ax, label='P(componente 0 | x)')\nax.set_title(\"GMM ‚Äî Probabilidad de pertenencia\\n(gradiente = incertidumbre)\",\n             fontsize=10, fontweight='bold')\n\n# GMM: incertidumbre (zona de frontera)\nax = axes[2]\nsc2 = ax.scatter(X_norm[:, 0], X_norm[:, 1],\n                 c=incertidumbre, cmap='hot_r', alpha=0.8, s=25,\n                 vmin=0, vmax=0.5)\nplt.colorbar(sc2, ax=ax, label='Incertidumbre (0=seguro, 0.5=m√°x)')\nax.set_title(\"GMM ‚Äî Mapa de incertidumbre\\n(rojo = zona de frontera ambigua)\",\n             fontsize=10, fontweight='bold')\n\nplt.suptitle(\"K-Means vs. GMM: asignaci√≥n dura vs. asignaci√≥n probabil√≠stica\",\n             fontsize=13, fontweight='bold')\nplt.tight_layout()\nplt.savefig(\"img_gmm_vs_kmeans_soft.png\", dpi=150, bbox_inches='tight')\nplt.show()\n\nprint(f\"Puntos con incertidumbre > 0.3: {(incertidumbre > 0.3).sum()} \"\n      f\"({(incertidumbre > 0.3).mean()*100:.1f}%)\")\nprint(\"‚Üí Estos son los puntos 'frontera' que K-Means clasifica con falsa certeza.\")"
  },
  {
   "cell_type": "markdown",
   "id": "md_d3c154",
   "metadata": {},
   "source": "**Script de explicaci√≥n:**\n\n*\"La imagen central es la clave: el gradiente de color muestra la probabilidad de pertenecer al componente azul. Los puntos totalmente rojos son con certeza del componente rojo; los totalmente azules, del azul. Pero hay una zona intermedia donde los puntos son violetas ‚Äîpertenecen a ambos en distintas proporciones. Ese gradiente es informaci√≥n que K-Means descarta completamente.\"*\n\n*\"El tercer gr√°fico muestra la incertidumbre: los puntos m√°s calientes son los m√°s ambiguos. En un proyecto real, esos son los clientes 'en la frontera' entre dos segmentos ‚Äîlos m√°s interesantes para estrategias de cross-selling o para campa√±as de reactivaci√≥n.\"*\n\n---\n\n#### Celda 3 ‚Äî Visualizaci√≥n de las elipses de covarianza"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "code_82f7bb",
   "metadata": {},
   "outputs": [],
   "source": "def plot_elipses_gmm(gmm, ax, n_std=2.0, alpha=0.25, colores=None):\n    \"\"\"\n    Dibuja las elipses de covarianza de un GMM entrenado.\n    n_std: n√∫mero de desviaciones est√°ndar para el radio de la elipse.\n    \"\"\"\n    if colores is None:\n        colores = plt.cm.tab10(np.linspace(0, 0.5, gmm.n_components))\n\n    for k, (mean, cov, color) in enumerate(\n        zip(gmm.means_, gmm.covariances_, colores)\n    ):\n        # Descomposici√≥n propia para obtener ejes y √°ngulo\n        if gmm.covariance_type == 'full':\n            cov_2d = cov\n        elif gmm.covariance_type == 'diag':\n            cov_2d = np.diag(cov)\n        elif gmm.covariance_type in ('spherical', 'tied'):\n            cov_2d = np.eye(2) * (cov if gmm.covariance_type == 'spherical'\n                                   else cov[0, 0])\n        else:\n            cov_2d = cov\n\n        vals, vecs = np.linalg.eigh(cov_2d[:2, :2])\n        angle = np.degrees(np.arctan2(*vecs[:, 0][::-1]))\n        width, height = 2 * n_std * np.sqrt(np.abs(vals))\n\n        elipse = Ellipse(\n            xy=mean[:2], width=width, height=height, angle=angle,\n            edgecolor=color, facecolor=color, alpha=alpha, linewidth=2\n        )\n        ax.add_patch(elipse)\n        ax.scatter(*mean[:2], c=[color], s=150, marker='X',\n                   zorder=5, edgecolors='black', linewidths=1)\n\n\n# Comparaci√≥n de tipos de covarianza\nfig, axes = plt.subplots(1, 4, figsize=(18, 5))\n\ncov_types = ['full', 'tied', 'diag', 'spherical']\ntitulos   = [\n    \"full\\n(elipses libres por cluster)\",\n    \"tied\\n(misma forma, distintos centros)\",\n    \"diag\\n(ejes alineados, sin rotaci√≥n)\",\n    \"spherical\\n(c√≠rculos, similar a K-Means)\"\n]\n\n# Dataset con clusters de distinta forma\nnp.random.seed(7)\nX_elip = np.vstack([\n    np.random.multivariate_normal([-2, 0], [[2.0, 1.2],[1.2, 0.4]], 150),\n    np.random.multivariate_normal([2,  1], [[0.5, -0.3],[-0.3, 1.5]], 150),\n    np.random.multivariate_normal([0, -3], [[0.3, 0],[0, 0.3]], 100),\n])\nX_elip_norm = StandardScaler().fit_transform(X_elip)\n\ncolores_elip = ['#e41a1c','#377eb8','#4daf4a']\n\nfor ax, ctype, titulo in zip(axes, cov_types, titulos):\n    gmm_c = GaussianMixture(n_components=3, covariance_type=ctype,\n                             n_init=5, random_state=0)\n    gmm_c.fit(X_elip_norm)\n    labels_c = gmm_c.predict(X_elip_norm)\n\n    ax.scatter(X_elip_norm[:, 0], X_elip_norm[:, 1],\n               c=labels_c, cmap='tab10', alpha=0.5, s=20)\n    plot_elipses_gmm(gmm_c, ax, colores=colores_elip)\n    ax.set_title(f\"covariance_type='{ctype}'\\n{titulo}\", fontsize=9, fontweight='bold')\n    ax.set_xlim(-3, 3)\n    ax.set_ylim(-3, 3)\n\nplt.suptitle(\"Impacto de covariance_type en las formas de los clusters\",\n             fontsize=13, fontweight='bold')\nplt.tight_layout()\nplt.savefig(\"img_gmm_covariance_types.png\", dpi=150, bbox_inches='tight')\nplt.show()"
  },
  {
   "cell_type": "markdown",
   "id": "md_244cf0",
   "metadata": {},
   "source": "**Script de explicaci√≥n:**\n\n*\"Cada panel muestra el mismo dataset con un tipo de covarianza distinto. Con `full` cada cluster puede ser la elipse que le corresponde ‚Äîlibre en forma, tama√±o y orientaci√≥n‚Äî. Con `spherical` los clusters son c√≠rculos: si lo record√°is, eso es b√°sicamente K-Means suave. La elecci√≥n de `covariance_type` afecta profundamente la soluci√≥n, y tambi√©n el n√∫mero de par√°metros que hay que estimar ‚Äîm√°s par√°metros requieren m√°s datos.\"*\n\n---\n\n#### Celda 4 ‚Äî Selecci√≥n de K con BIC y AIC"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "code_125710",
   "metadata": {},
   "outputs": [],
   "source": "# -------------------------------------------------------\n# Curvas BIC y AIC para elegir el n√∫mero de componentes\n# -------------------------------------------------------\n\n# Dataset de churn de telecomunicaciones (sint√©tico)\nnp.random.seed(0)\nn = 500\n\n# Simulamos 4 perfiles distintos de cliente de telecom\nantiguedad = np.concatenate([\n    np.random.normal(24,  6, 120),   # clientes nuevos\n    np.random.normal(48, 10, 150),   # clientes medios\n    np.random.normal(72, 12, 130),   # clientes veteranos no fieles\n    np.random.normal(60,  8, 100),   # clientes veteranos fieles\n])\nllamadas = np.concatenate([\n    np.random.normal(150, 30, 120),\n    np.random.normal(200, 40, 150),\n    np.random.normal(80,  20, 130),\n    np.random.normal(300, 35, 100),\n])\nfactura = np.concatenate([\n    np.random.normal(30, 8,  120),\n    np.random.normal(55, 12, 150),\n    np.random.normal(40, 10, 130),\n    np.random.normal(90, 15, 100),\n])\nchurn_prob = np.concatenate([\n    np.random.beta(3, 2, 120),\n    np.random.beta(2, 4, 150),\n    np.random.beta(5, 2, 130),\n    np.random.beta(1, 6, 100),\n])\n\ndf_telecom = pd.DataFrame({\n    'antiguedad_meses': antiguedad,\n    'llamadas_mes':     llamadas,\n    'factura_media':    factura,\n    'prob_churn':       churn_prob,\n})\ndf_telecom = df_telecom.clip(lower=0)\n\nX_tel = StandardScaler().fit_transform(df_telecom)\n\n# Calculamos BIC y AIC para K = 1..10\nks_range = range(1, 11)\nbic_vals, aic_vals, ll_vals = [], [], []\n\nfor k in ks_range:\n    gmm_k = GaussianMixture(n_components=k, covariance_type='full',\n                             n_init=5, random_state=42)\n    gmm_k.fit(X_tel)\n    bic_vals.append(gmm_k.bic(X_tel))\n    aic_vals.append(gmm_k.aic(X_tel))\n    ll_vals.append(gmm_k.score(X_tel))  # log-verosimilitud media\n\n# Visualizaci√≥n\nfig, axes = plt.subplots(1, 2, figsize=(14, 5))\n\n# BIC y AIC\nax1 = axes[0]\nax1.plot(ks_range, bic_vals, 'bs-', linewidth=2, markersize=8, label='BIC')\nax1.plot(ks_range, aic_vals, 'r^--', linewidth=2, markersize=8, label='AIC')\nk_bic = np.argmin(bic_vals) + 1\nk_aic = np.argmin(aic_vals) + 1\nax1.axvline(x=k_bic, color='blue', linestyle=':', alpha=0.7,\n            label=f'M√≠n. BIC ‚Üí k={k_bic}')\nax1.axvline(x=k_aic, color='red', linestyle=':', alpha=0.7,\n            label=f'M√≠n. AIC ‚Üí k={k_aic}')\nax1.set_xlabel(\"N√∫mero de componentes (K)\", fontsize=11)\nax1.set_ylabel(\"Criterio de informaci√≥n (menor = mejor)\", fontsize=11)\nax1.set_title(\"BIC y AIC para seleccionar K\\n(Dataset Telecom Churn)\",\n              fontsize=11, fontweight='bold')\nax1.legend(fontsize=10)\nax1.set_xticks(ks_range)\n\n# Log-verosimilitud\nax2 = axes[1]\nax2.plot(ks_range, ll_vals, 'go-', linewidth=2, markersize=8)\nax2.set_xlabel(\"N√∫mero de componentes (K)\", fontsize=11)\nax2.set_ylabel(\"Log-verosimilitud media\", fontsize=11)\nax2.set_title(\"Log-verosimilitud vs. K\\n(siempre crece ‚Äî no sirve sola)\",\n              fontsize=11, fontweight='bold')\nax2.set_xticks(ks_range)\n\nplt.suptitle(\"Selecci√≥n del n√∫mero de componentes GMM\",\n             fontsize=13, fontweight='bold')\nplt.tight_layout()\nplt.savefig(\"img_gmm_bic_aic.png\", dpi=150, bbox_inches='tight')\nplt.show()\n\nprint(f\"K √≥ptimo seg√∫n BIC: {k_bic}\")\nprint(f\"K √≥ptimo seg√∫n AIC: {k_aic}\")"
  },
  {
   "cell_type": "markdown",
   "id": "md_7d7691",
   "metadata": {},
   "source": "**Script de explicaci√≥n:**\n\n*\"El gr√°fico de la derecha ilustra el problema central: la log-verosimilitud siempre crece al a√±adir componentes. Si la us√°semos sola, siempre elegir√≠amos K = n. Por eso necesitamos BIC y AIC: penalizan el n√∫mero de par√°metros. La curva del BIC tiene un m√≠nimo claro ‚Äîah√≠ est√° el K √≥ptimo seg√∫n BIC. Si BIC y AIC coinciden, es un resultado robusto.\"*\n\n---\n\n#### Celda 5 ‚Äî GMM entrenado y perfilado de segmentos"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "code_355e8f",
   "metadata": {},
   "outputs": [],
   "source": "# Entrenamos el GMM final con el K elegido por BIC\nk_final = k_bic\ngmm_final = GaussianMixture(n_components=k_final, covariance_type='full',\n                             n_init=10, random_state=42)\ngmm_final.fit(X_tel)\n\ndf_telecom['cluster_gmm'] = gmm_final.predict(X_tel)\nproba_final = gmm_final.predict_proba(X_tel)\n\n# Perfil de cada componente\nprint(\"Perfil medio de cada componente GMM:\")\nperfil_gmm = df_telecom.groupby('cluster_gmm')[df_telecom.columns[:-1]].mean().round(1)\nperfil_gmm['peso (%)'] = (\n    df_telecom['cluster_gmm'].value_counts(normalize=True) * 100\n).sort_index().round(1)\nprint(perfil_gmm)\n\n# Visualizaci√≥n: probabilidades de los 5 puntos m√°s ambiguos\nincert = 1 - proba_final.max(axis=1)\ntop_ambiguos = np.argsort(incert)[-5:][::-1]\nprint(\"\\nLos 5 clientes m√°s ambiguos (mayor incertidumbre):\")\ndf_ambiguos = pd.DataFrame(\n    proba_final[top_ambiguos],\n    columns=[f'P(cluster {k})' for k in range(k_final)],\n    index=[f'Cliente {i}' for i in top_ambiguos]\n).round(3)\nprint(df_ambiguos)\nprint(\"\\n‚Üí Estos clientes no pertenecen claramente a ning√∫n segmento.\")\nprint(\"  Son candidatos a campa√±as de 'definici√≥n de perfil' (encuestas, A/B tests).\")"
  },
  {
   "cell_type": "markdown",
   "id": "md_c51368",
   "metadata": {},
   "source": "---\n\n#### Celda 6 ‚Äî Visualizaci√≥n del resultado con elipses"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "code_f15f1c",
   "metadata": {},
   "outputs": [],
   "source": "# Proyecci√≥n 2D para visualizar (usamos las dos primeras features)\nfig, axes = plt.subplots(1, 2, figsize=(14, 6))\n\ncolores_gmm = ['#e41a1c','#377eb8','#4daf4a','#ff7f00']\n\n# Panel izquierdo: scatter con asignaci√≥n hard\nax1 = axes[0]\nfor c in range(k_final):\n    mask = df_telecom['cluster_gmm'] == c\n    ax1.scatter(\n        df_telecom.loc[mask, 'antiguedad_meses'],\n        df_telecom.loc[mask, 'factura_media'],\n        c=colores_gmm[c % len(colores_gmm)], alpha=0.5, s=30,\n        label=f'Componente {c} (n={mask.sum()})'\n    )\nax1.set_xlabel(\"Antig√ºedad (meses)\")\nax1.set_ylabel(\"Factura media (‚Ç¨)\")\nax1.set_title(f\"GMM k={k_final} ‚Äî Asignaci√≥n hard\\n(argmax de probabilidades)\",\n              fontsize=10, fontweight='bold')\nax1.legend(fontsize=9)\n\n# Panel derecho: incertidumbre\nax2 = axes[1]\nsc = ax2.scatter(\n    df_telecom['antiguedad_meses'],\n    df_telecom['factura_media'],\n    c=incert, cmap='YlOrRd', s=30, alpha=0.8\n)\nplt.colorbar(sc, ax=ax2, label='Incertidumbre de asignaci√≥n')\nax2.set_xlabel(\"Antig√ºedad (meses)\")\nax2.set_ylabel(\"Factura media (‚Ç¨)\")\nax2.set_title(\"Mapa de incertidumbre\\n(amarillo = seguro, rojo = ambiguo)\",\n              fontsize=10, fontweight='bold')\n\nplt.suptitle(\"GMM aplicado a segmentaci√≥n de clientes de telecom\",\n             fontsize=13, fontweight='bold')\nplt.tight_layout()\nplt.savefig(\"img_gmm_telecom_resultado.png\", dpi=150, bbox_inches='tight')\nplt.show()"
  },
  {
   "cell_type": "markdown",
   "id": "md_5d0456",
   "metadata": {},
   "source": "---\n\n#### Celda 7 ‚Äî Interpretaci√≥n de negocio de los segmentos"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "code_639b20",
   "metadata": {},
   "outputs": [],
   "source": "# Nomenclatura de los segmentos basada en el perfil medio\nnombres_segmento = {\n    0: \"Clientes nuevos de bajo valor\",\n    1: \"Clientes consolidados activos\",\n    2: \"Clientes veteranos en riesgo\",\n    3: \"Clientes VIP fieles\",\n}\n\nacciones = {\n    0: \"Onboarding mejorado, ofertas de bienvenida\",\n    1: \"Cross-selling de productos premium\",\n    2: \"Programa de retenci√≥n urgente, llamada proactiva\",\n    3: \"Programa de fidelidad exclusivo, upselling\",\n}\n\nprint(\"=\" * 60)\nprint(\"INTERPRETACI√ìN DE NEGOCIO ‚Äî GMM Segmentaci√≥n Telecom\")\nprint(\"=\" * 60)\nfor c in range(k_final):\n    n_seg = (df_telecom['cluster_gmm'] == c).sum()\n    pct   = n_seg / len(df_telecom) * 100\n    print(f\"\\nComponente {c}: '{nombres_segmento.get(c, 'Por definir')}'\")\n    print(f\"  Tama√±o: {n_seg} clientes ({pct:.1f}%)\")\n    print(f\"  Acci√≥n: {acciones.get(c, 'Pendiente de definir')}\")\n\nprint(\"\\nVentaja del GMM sobre K-Means:\")\nprint(\"  Los clientes ambiguos no reciben una etiqueta forzada.\")\nprint(\"  Se pueden tratar con estrategias mixtas o como prioridad de an√°lisis.\")"
  },
  {
   "cell_type": "markdown",
   "id": "md_944d06",
   "metadata": {},
   "source": "---\n\n## NOTAS DE PRODUCCI√ìN\n\n### Para las slides\n\n- **Slide 1:** Portada. Pregunta: *\"¬øUn cliente pertenece al 100% a un √∫nico segmento?\"*\n- **Slide 2:** Clustering duro vs. suave ‚Äî diagrama con la misma frontera vista con K-Means (l√≠nea dura) y GMM (gradiente de probabilidad).\n- **Slide 3:** F√≥rmula `p(x) = Œ£ œÄ‚Çñ ùí©(x|Œº‚Çñ,Œ£‚Çñ)` descompuesta visualmente: tres gaussianas coloreadas que se suman.\n- **Slide 4:** El algoritmo EM ‚Äî tabla comparativa con K-Means, paso a paso.\n- **Slide 5:** Los cuatro tipos de covarianza ‚Äî los cuatro paneles de la Celda 3.\n- **Slide 6:** BIC y AIC ‚Äî gr√°fica con los m√≠nimos se√±alados y la explicaci√≥n de la penalizaci√≥n.\n- **Slide 7:** Tabla comparativa K-Means vs. GMM (cu√°ndo usar cada uno).\n\n### Para el handout\n\n- Tabla comparativa K-Means vs. GMM con f√≥rmulas del paso E y paso M.\n- Tabla de `covariance_type`: descripci√≥n, par√°metros, cu√°ndo usar.\n- Los gr√°ficos de elipses de covarianza (Celda 3).\n- El mapa de incertidumbre (Celda 2 y Celda 6) con gu√≠a de interpretaci√≥n.\n- Gu√≠a de decisi√≥n BIC vs. AIC.\n\n### Para el Jupyter Notebook (ejercicios a completar)\n\n**Ejercicio 1:** Aplicar GMM con los cuatro tipos de covarianza al dataset de pa√≠ses del Bloque 1.3. ¬øCu√°l produce clusters m√°s interpretables? ¬øCu√°l minimiza el BIC?\n\n**Ejercicio 2:** Para el dataset de telecom, a√±adir la columna `probabilidad_maxima` al DataFrame y filtrar los clientes con `max_prob < 0.6`. ¬øCu√°ntos son? ¬øA qu√© cluster pertenecen mayoritariamente?\n\n**Ejercicio 3 (avanzado):** Implementar una iteraci√≥n del algoritmo EM manualmente: dado un GMM ya inicializado con `gmm.fit()`, programar el paso E (responsabilidades) usando NumPy y verificar que coincide con `gmm.predict_proba()`.\n\n---\n\n## GESTI√ìN DEL TIEMPO\n\n| Segmento | Duraci√≥n | Indicador |\n|---|---|---|\n| Apertura Sesi√≥n 2 + recapitulaci√≥n | 6 min | Preguntas respondidas |\n| Limitaciones del clustering duro | 8 min | Ejemplo altura/peso en pantalla |\n| El modelo GMM (f√≥rmula + par√°metros) | 8 min | F√≥rmula descompuesta en pantalla |\n| El algoritmo EM (pasos E y M) | 9 min | Tabla comparativa con K-Means |\n| BIC y AIC | 4 min | F√≥rmulas en pantalla |\n| Celda 1-2 (imports + soft vs. hard) | 8 min | Mapa de incertidumbre generado |\n| Celda 3 (elipses de covarianza) | 7 min | Los 4 paneles generados |\n| Celda 4 (BIC y AIC) | 7 min | K √≥ptimo identificado |\n| Celda 5-7 (telecom + interpretaci√≥n) | 13 min | Tabla de negocio impresa |\n| **Total** | **70 min** | |\n\n---\n\n*Bloque 2.1 desarrollado para el m√≥dulo \"Algoritmos de Clustering\" ‚Äî M√°ster en Ciencia de Datos*"
  },
  {
   "cell_type": "markdown",
   "id": "md_e1f91b",
   "metadata": {},
   "source": "---\n## üí° Para explorar m√°s ‚Äî Ejercicios propuestos\n\nLos ejercicios pr√°cticos est√°n marcados con comentarios `# EJERCICIO` en el c√≥digo.\n\n**Entrega sugerida:** Exporta este notebook como HTML o PDF (`File ‚Üí Download ‚Üí HTML`)\ny a√±ade tus conclusiones en una celda Markdown al final de cada secci√≥n.\n\n---\n*M√°ster en Ciencia de Datos ¬∑ M√≥dulo Clustering ¬∑ Bloque 2.1*"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}