{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "md_173bc0",
   "metadata": {},
   "source": "# Bloque 2.6 â€” Buenas PrÃ¡cticas, Escalabilidad y Cierre\n**MÃ¡ster en Ciencia de Datos Â· MÃ³dulo: Algoritmos de Clustering**\n**SesiÃ³n 2 Â· DuraciÃ³n: 15 min**\n\n---\n> ğŸ“Œ **CÃ³mo usar este notebook:**\n> Ejecuta las celdas **en orden**. Cada secciÃ³n comienza con explicaciÃ³n teÃ³rica (en Markdown) seguida del cÃ³digo correspondiente.\n> Los comentarios `# ---` delimitan ejercicios opcionales para profundizar.\n"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "code_dda3fc",
   "metadata": {},
   "outputs": [],
   "source": "# ============================================================\n# CHECKLIST DE BUENAS PRÃCTICAS EN CLUSTERING\n# Guarda este notebook como referencia para proyectos reales\n# ============================================================\n\nchecklist = {\n    \"PREPROCESADO\": [\n        (\"âœ…\", \"Estandarizar siempre antes de clustering (media 0, std 1)\"),\n        (\"âœ…\", \"Revisar outliers ANTES de clustering, no despuÃ©s\"),\n        (\"âœ…\", \"Aplicar log-transform a variables monetarias/de conteo con cola larga\"),\n        (\"âœ…\", \"Comprobar multicolinealidad â€” features correladas distorsionan las distancias\"),\n        (\"âŒ\", \"NUNCA incluir el target (si existe) en las features de clustering\"),\n        (\"âŒ\", \"NUNCA imputar nulos con la media si hay muchos â€” pueden crear un cluster espurio\"),\n    ],\n    \"SELECCIÃ“N DE ALGORITMO\": [\n        (\"âœ…\", \"Probar al menos 2-3 algoritmos antes de decidir\"),\n        (\"âœ…\", \"Usar siempre mÃºltiples mÃ©tricas (no solo Silhouette)\"),\n        (\"âœ…\", \"Considerar la interpretabilidad del negocio al elegir k\"),\n        (\"âŒ\", \"NUNCA elegir k basÃ¡ndose en una sola mÃ©trica ni por defecto (k=3 o k=5)\"),\n        (\"âŒ\", \"NUNCA usar K-Means si los clusters no son convexos o hay ruido significativo\"),\n        (\"âŒ\", \"NUNCA usar DBSCAN sin explorar el k-distance graph para calibrar Îµ\"),\n    ],\n    \"INTERPRETACIÃ“N\": [\n        (\"âœ…\", \"Asignar nombres de negocio a cada cluster â€” los nÃºmeros no comunican\"),\n        (\"âœ…\", \"Validar con expertos de dominio que los segmentos tienen sentido\"),\n        (\"âœ…\", \"Comprobar estabilidad: re-ejecutar con distintas semillas\"),\n        (\"âœ…\", \"Verificar que cada cluster es accionable (se puede hacer algo diferente con Ã©l)\"),\n        (\"âŒ\", \"NUNCA presentar el clustering como 'la verdad' â€” es una aproximaciÃ³n\"),\n        (\"âŒ\", \"NUNCA confundir el score de Silhouette alto con 'el clustering es correcto'\"),\n    ],\n    \"VISUALIZACIÃ“N\": [\n        (\"âœ…\", \"Usar PCA para preprocesar, t-SNE/UMAP solo para visualizar\"),\n        (\"âœ…\", \"Aclarar en cualquier grÃ¡fico t-SNE que las distancias entre clusters NO son interpretables\"),\n        (\"âœ…\", \"Incluir siempre el tamaÃ±o de cada cluster en los grÃ¡ficos\"),\n        (\"âŒ\", \"NUNCA sacar conclusiones sobre relaciones entre clusters a partir de t-SNE\"),\n    ],\n    \"PRODUCCIÃ“N\": [\n        (\"âœ…\", \"Guardar el scaler y el modelo entrenado (pickle/joblib) para predecir nuevos clientes\"),\n        (\"âœ…\", \"Documentar el pipeline completo (parÃ¡metros, versiones de librerÃ­as)\"),\n        (\"âœ…\", \"Establecer un proceso de re-entrenamiento periÃ³dico\"),\n        (\"âŒ\", \"NUNCA asumir que el clustering de hace 6 meses es vÃ¡lido hoy sin validarlo\"),\n    ]\n}\n\nfor categoria, items in checklist.items():\n    print(f\"\\n{'='*55}\")\n    print(f\"  {categoria}\")\n    print(f\"{'='*55}\")\n    for simbolo, texto in items:\n        print(f\"  {simbolo}  {texto}\")\n\nprint(\"\\n\\nğŸ’¾ Guarda este notebook â€” es tu referencia para proyectos reales.\")"
  },
  {
   "cell_type": "markdown",
   "id": "md_b6f0d8",
   "metadata": {},
   "source": "---\n\n## Parte 2 â€” Escalabilidad: Â¿quÃ© usar con datos grandes? (3 min)\n\n### GuiÃ³n del instructor\n\n> \"Todo lo que hemos visto funciona bien hasta ~50.000-100.000 registros. En proyectos con millones de clientes hay que hacer ajustes.\"\n\n### Tabla de escalabilidad (para la presentaciÃ³n)\n\n| Algoritmo | Complejidad | Escala a 1M+ | Alternativa escalable |\n|---|---|---|---|\n| K-Means | O(nÂ·kÂ·iter) | âœ… Con `MiniBatchKMeans` | `MiniBatchKMeans` (sklearn) |\n| K-Medoids (PAM) | O(nÂ²) | âŒ | CLARA, CLARANS |\n| JerÃ¡rquico | O(nÂ² log n) â€“ O(nÂ³) | âŒ | Bisecting K-Means |\n| DBSCAN | O(n log n) con Ã­ndice | âš ï¸ Con HDBSCAN | HDBSCAN (hdbscan library) |\n| GMM (EM) | O(nÂ·kÂ·dÂ²Â·iter) | âš ï¸ Con muchos features | Variational Bayes GMM |\n| SOM | O(nÂ·mÂ·iter) | âœ… Con m moderado | `minisom` + submuestras |"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "code_7046b5",
   "metadata": {},
   "outputs": [],
   "source": "# Demo rÃ¡pida: MiniBatchKMeans vs KMeans en tiempo\nimport time\nfrom sklearn.cluster import MiniBatchKMeans\nfrom sklearn.datasets import make_blobs\nimport numpy as np\n\n# Dataset grande simulado\nX_grande, _ = make_blobs(n_samples=200_000, n_features=10,\n                         centers=6, random_state=42)\n\n# KMeans estÃ¡ndar\nt0 = time.time()\nkm = KMeans(n_clusters=6, random_state=42, n_init=5, max_iter=100)\nkm.fit(X_grande)\nt_km = time.time() - t0\n\n# MiniBatchKMeans\nt0 = time.time()\nmbkm = MiniBatchKMeans(n_clusters=6, random_state=42, n_init=5,\n                        batch_size=2048, max_iter=100)\nmbkm.fit(X_grande)\nt_mbkm = time.time() - t0\n\nprint(f\"KMeans estÃ¡ndar (200k muestras):  {t_km:.2f}s\")\nprint(f\"MiniBatchKMeans (200k muestras):  {t_mbkm:.2f}s\")\nprint(f\"AceleraciÃ³n: {t_km/t_mbkm:.1f}x\")\n\n# Comparar inercia (aproximaciÃ³n de calidad)\nprint(f\"\\nKMeans inertia:          {km.inertia_:,.0f}\")\nprint(f\"MiniBatchKMeans inertia: {mbkm.inertia_:,.0f}\")\nprint(f\"Diferencia relativa: {abs(km.inertia_-mbkm.inertia_)/km.inertia_:.2%}\")"
  },
  {
   "cell_type": "markdown",
   "id": "md_eb3f71",
   "metadata": {},
   "source": "---\n\n## Parte 3 â€” Mapa de decisiÃ³n final (3 min)\n\n### GuiÃ³n del instructor\n\n> \"Cierro con el Ã¡rbol de decisiÃ³n que podÃ©is usar en cualquier proyecto. No es un orÃ¡culo, pero es un punto de partida sÃ³lido.\"\n\n### Celda 3 â€” Ãrbol de decisiÃ³n como texto ejecutable"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "code_6a5c9d",
   "metadata": {},
   "outputs": [],
   "source": "# ============================================================\n# ÃRBOL DE DECISIÃ“N PARA SELECCIÃ“N DE ALGORITMO DE CLUSTERING\n# ============================================================\n\narbol = \"\"\"\nÂ¿CuÃ¡ntas muestras tengo?\nâ”‚\nâ”œâ”€â”€ < 10.000 â†’ Puedo usar cualquier algoritmo\nâ”‚\nâ””â”€â”€ > 100.000 â†’ Evitar: K-Medoids PAM, JerÃ¡rquico completo\n               Preferir: MiniBatchKMeans, HDBSCAN, SOM con submuestra\n\nÂ¿Conozco el nÃºmero de clusters k de antemano?\nâ”‚\nâ”œâ”€â”€ SÃ­ â†’ K-Means / K-Medoids / GMM\nâ”‚\nâ””â”€â”€ No â†’ Explorar k con: Elbow + Silhouette + DBI\n         O usar: DBSCAN / HDBSCAN (determinan k automÃ¡ticamente)\n\nÂ¿Esperan clusters esfÃ©ricos y bien separados?\nâ”‚\nâ”œâ”€â”€ SÃ­ â†’ K-Means (rÃ¡pido, interpretable)\nâ”‚\nâ””â”€â”€ No â†’ Â¿Hay ruido/outliers significativos?\n          â”‚\n          â”œâ”€â”€ SÃ­ â†’ DBSCAN / HDBSCAN\n          â”‚\n          â””â”€â”€ No â†’ Â¿Clusters con forma irregular pero sin ruido?\n                    â”‚\n                    â”œâ”€â”€ SÃ­ â†’ JerÃ¡rquico (Ward) o GMM\n                    â””â”€â”€ No â†’ SOM (exploraciÃ³n topolÃ³gica)\n\nÂ¿Necesito probabilidades de pertenencia (soft assignment)?\nâ”‚\nâ”œâ”€â”€ SÃ­ â†’ GMM\nâ””â”€â”€ No â†’ K-Means, K-Medoids, JerÃ¡rquico, DBSCAN\n\nÂ¿Los outliers son datos importantes (no errores)?\nâ”‚\nâ”œâ”€â”€ SÃ­ (ej. detecciÃ³n de anomalÃ­as) â†’ DBSCAN / HDBSCAN\nâ””â”€â”€ No â†’ K-Means o K-Medoids (mÃ¡s robusto que K-Means)\n\nÂ¿Necesito visualizar relaciones topolÃ³gicas entre clusters?\nâ”‚\nâ””â”€â”€ SÃ­ â†’ SOM (U-Matrix, component planes)\n\"\"\"\n\nprint(arbol)"
  },
  {
   "cell_type": "markdown",
   "id": "md_c17839",
   "metadata": {},
   "source": "---\n\n## Parte 4 â€” Cierre, recursos y prÃ³ximos pasos (4 min)\n\n### GuiÃ³n del instructor\n\n> \"Hemos cubierto en 10 horas lo que muchos proyectos de ciencia de datos requieren: desde los fundamentos matemÃ¡ticos hasta la puesta en producciÃ³n. Pero este es el punto de partida, no el final.\"\n\n### Celda 4 â€” Resumen del mÃ³dulo y recursos"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "code_5132fa",
   "metadata": {},
   "outputs": [],
   "source": "resumen = \"\"\"\nâ•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\nâ•‘          RESUMEN DEL MÃ“DULO â€” ALGORITMOS DE CLUSTERING          â•‘\nâ• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£\nâ•‘                                                                  â•‘\nâ•‘  SESIÃ“N 1 (5h)                                                   â•‘\nâ•‘  â”œâ”€â”€ Bloque 1.1: Fundamentos y mÃ©tricas de distancia            â•‘\nâ•‘  â”œâ”€â”€ Bloque 1.2: K-Means y K-Medoids                            â•‘\nâ•‘  â”œâ”€â”€ Bloque 1.3: Clustering jerÃ¡rquico                          â•‘\nâ•‘  â””â”€â”€ Bloque 1.4: DBSCAN                                         â•‘\nâ•‘                                                                  â•‘\nâ•‘  SESIÃ“N 2 (5h)                                                   â•‘\nâ•‘  â”œâ”€â”€ Bloque 2.1: Modelos de mezcla gaussiana (GMM + EM)         â•‘\nâ•‘  â”œâ”€â”€ Bloque 2.2: Mapas auto-organizados de Kohonen (SOM)        â•‘\nâ•‘  â”œâ”€â”€ Bloque 2.3: MÃ©tricas de evaluaciÃ³n                         â•‘\nâ•‘  â”œâ”€â”€ Bloque 2.4: ReducciÃ³n de dimensionalidad                   â•‘\nâ•‘  â”œâ”€â”€ Bloque 2.5: Proyecto integrador (RFM e-commerce)           â•‘\nâ•‘  â””â”€â”€ Bloque 2.6: Buenas prÃ¡cticas y cierre â† aquÃ­ estamos      â•‘\nâ•‘                                                                  â•‘\nâ• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£\nâ•‘  ALGORITMOS CUBIERTOS:                                           â•‘\nâ•‘   K-Means  Â·  K-Medoids  Â·  Hierarchical  Â·  DBSCAN             â•‘\nâ•‘   GMM (EM)  Â·  Kohonen SOM                                      â•‘\nâ•‘                                                                  â•‘\nâ•‘  MÃ‰TRICAS: Silhouette Â· DBI Â· CHI Â· WCSS Â· ARI Â· NMI            â•‘\nâ•‘                                                                  â•‘\nâ•‘  PREPROCESADO: Escala Â· Log-transform Â· PCA                     â•‘\nâ•‘  VISUALIZACIÃ“N: t-SNE Â· UMAP Â· Radar Â· Heatmap Â· U-Matrix       â•‘\nâ•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n\"\"\"\nprint(resumen)\n\nrecursos = \"\"\"\nğŸ“š RECURSOS RECOMENDADOS\n\nLIBROS\n  â€¢ \"The Elements of Statistical Learning\" â€” Hastie, Tibshirani, Friedman\n    Cap. 14: Unsupervised Learning (disponible free en web oficial)\n  â€¢ \"Pattern Recognition and Machine Learning\" â€” Bishop\n    Cap. 9: Mixture Models and EM\n  â€¢ \"Introduction to Machine Learning with Python\" â€” MÃ¼ller & Guido\n    (enfoque prÃ¡ctico, scikit-learn)\n\nPAPERS CLAVE\n  â€¢ Lloyd (1982) â€” \"Least squares quantization in PCM\" (K-Means original)\n  â€¢ Arthur & Vassilvitskii (2007) â€” \"k-means++: The advantages of careful seeding\"\n  â€¢ Ester et al. (1996) â€” \"A density-based algorithm for discovering clusters\" (DBSCAN)\n  â€¢ Kohonen (1982) â€” \"Self-organized formation of topologically correct feature maps\"\n  â€¢ McInnes et al. (2018) â€” \"UMAP: Uniform Manifold Approximation and Projection\" (arXiv)\n\nDOCUMENTACIÃ“N\n  â€¢ scikit-learn clustering: https://scikit-learn.org/stable/modules/clustering.html\n  â€¢ minisom: https://github.com/JustGlowing/minisom\n  â€¢ UMAP: https://umap-learn.readthedocs.io\n  â€¢ HDBSCAN: https://hdbscan.readthedocs.io\n\nDATASETS PARA PRACTICAR\n  â€¢ UCI ML Repository â€” Online Retail, Customer Segmentation\n  â€¢ Kaggle â€” Mall Customers, Credit Card Dataset, E-Commerce Data\n  â€¢ scikit-learn â€” make_blobs, make_moons, make_circles (sintÃ©ticos)\n\"\"\"\nprint(recursos)"
  },
  {
   "cell_type": "markdown",
   "id": "md_f7e7e4",
   "metadata": {},
   "source": "### Celda 5 â€” PrÃ³ximos pasos (continuaciÃ³n del mÃ¡ster)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "code_42ce91",
   "metadata": {},
   "outputs": [],
   "source": "proximos_pasos = \"\"\"\nğŸ¯ PRÃ“XIMOS PASOS EN EL MÃSTER\n\nEste mÃ³dulo conecta con:\n\n  â†’ MÃ“DULO SIGUIENTE: DetecciÃ³n de anomalÃ­as\n     Isolation Forest, LOF, Autoencoders\n     (Clustering como baseline para anomaly detection)\n\n  â†’ MÃ“DULO: Aprendizaje semi-supervisado\n     Usar clustering para generar pseudo-labels\n     Label propagation, self-training\n\n  â†’ MÃ“DULO: Series temporales\n     Clustering de series: DTW, k-Shape, temporal SOM\n     SegmentaciÃ³n de comportamiento a lo largo del tiempo\n\n  â†’ PROYECTO FINAL\n     Aplicar el pipeline completo a datos reales de la empresa\n     Deliverable: notebook documentado + presentaciÃ³n ejecutiva\n\nğŸ’¡ CONSEJO FINAL\n\nEl clustering es tanto ciencia como arte.\nLas mÃ©tricas guÃ­an, pero el juicio del dominio decide.\nEl mejor clustering no es el que tiene el Silhouette mÃ¡s alto,\nsino el que lleva a las mejores decisiones de negocio.\n\"\"\"\nprint(proximos_pasos)"
  },
  {
   "cell_type": "markdown",
   "id": "md_548a46",
   "metadata": {},
   "source": "---"
  },
  {
   "cell_type": "markdown",
   "id": "md_2c09bf",
   "metadata": {},
   "source": "---\n## ğŸ’¡ Para explorar mÃ¡s â€” Ejercicios propuestos\n\nLos ejercicios prÃ¡cticos estÃ¡n marcados con comentarios `# EJERCICIO` en el cÃ³digo.\n\n**Entrega sugerida:** Exporta este notebook como HTML o PDF (`File â†’ Download â†’ HTML`)\ny aÃ±ade tus conclusiones en una celda Markdown al final de cada secciÃ³n.\n\n---\n*MÃ¡ster en Ciencia de Datos Â· MÃ³dulo Clustering Â· Bloque 2.6*"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}