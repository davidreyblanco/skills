# Bloque 2.1 ‚Äî Gaussian Mixture Models (GMM) y el Algoritmo EM
## Gui√≥n detallado del instructor

**Duraci√≥n:** 70 minutos (35 min teor√≠a + 35 min pr√°ctica en Jupyter Notebook)
**Posici√≥n en la sesi√≥n:** Primer bloque de la Sesi√≥n 2

---

## PARTE TE√ìRICA (35 min)

---

### [00:00 ‚Äì 00:06] Apertura de la Sesi√≥n 2 y transici√≥n

**Script de apertura:**

*"Bienvenidos a la Sesi√≥n 2. En la sesi√≥n anterior construimos la base: K-Means, K-Medoids, clustering jer√°rquico y DBSCAN. Hoy vamos a subir un nivel. Los algoritmos de ayer producen lo que se llama clustering duro: cada punto pertenece exactamente a un cluster con certeza absoluta. El mundo real rara vez funciona as√≠."*

*"Pensad en un cliente de vuestra empresa. ¬øPertenece al segmento 'cazador de ofertas' o al segmento 'comprador de conveniencia'? Probablemente a los dos, en proporciones distintas seg√∫n el momento. O pensad en una transacci√≥n financiera: ¬øes normal o fraudulenta? A veces no hay una respuesta binaria ‚Äîhay un grado de sospecha. Los Gaussian Mixture Models son la respuesta matem√°tica a esta necesidad: asignan probabilidades de pertenencia, no etiquetas."*

**Recapitulaci√≥n r√°pida de la Sesi√≥n 1 (2 min):**

*"Antes de arrancar: ¬øalguna pregunta de la sesi√≥n anterior? ¬øAlgo que no qued√≥ claro con K-Means, K-Medoids, el dendrograma o DBSCAN?"*

---

### [00:06 ‚Äì 00:14] Limitaciones del clustering duro y la necesidad de probabilidades

**El problema del clustering duro:**

K-Means asigna cada punto `x·µ¢` al cluster `k` seg√∫n:
```
z·µ¢ = argmin_k ||x·µ¢ - Œº‚Çñ||¬≤
```
Es una asignaci√≥n binaria: `z·µ¢ ‚àà {1, 2, ..., K}`. No hay matices.

**Situaciones donde esto falla:**

1. **Puntos en la frontera entre clusters:** Un punto equidistante de dos centroides recibe la misma etiqueta definitiva que un punto en el centro de un cluster. No hay forma de saber que es un caso ambiguo.

2. **Clusters el√≠pticos o con correlaci√≥n:** K-Means asume que todos los clusters tienen la misma forma esf√©rica (misma covarianza). Si los datos tienen clusters elongados en distintas direcciones, K-Means los parte incorrectamente.

3. **Clusters con tama√±os muy distintos:** K-Means tiende a producir clusters del mismo tama√±o aunque la realidad sea que el 80% de los puntos pertenece a un solo grupo.

**Ejemplo visual que funciona muy bien en clase:**

*"Imaginad que med√≠s la altura y el peso de una poblaci√≥n. Hay claramente dos grupos: hombres y mujeres. Pero los grupos no son esf√©ricos ‚Äîhay correlaci√≥n positiva entre altura y peso, y los dos grupos tienen formas el√≠pticas distintas. K-Means intentar√° dividir esto con c√≠rculos y fallar√° en la zona de superposici√≥n. Un GMM pondr√° una elipse diferente sobre cada grupo y asignar√° probabilidades en la zona de solapamiento."*

---

### [00:14 ‚Äì 00:22] El modelo: mezcla de distribuciones gaussianas

**Definici√≥n formal:**

Un Gaussian Mixture Model asume que los datos han sido generados por `K` distribuciones gaussianas multivariantes. La densidad de probabilidad del modelo completo es:

```
p(x) = Œ£‚Çñ‚Çå‚ÇÅ·¥∑  œÄ‚Çñ ¬∑ ùí©(x | Œº‚Çñ, Œ£‚Çñ)
```

donde:
- `œÄ‚Çñ` es el **peso** del componente `k`: la probabilidad a priori de que un punto aleatorio pertenezca al componente `k`. Cumple `Œ£œÄ‚Çñ = 1`, `œÄ‚Çñ ‚â• 0`.
- `ùí©(x | Œº‚Çñ, Œ£‚Çñ)` es la densidad de una gaussiana multivariante con **media** `Œº‚Çñ` y **covarianza** `Œ£‚Çñ`.
- La densidad gaussiana multivariante en `d` dimensiones es:

```
ùí©(x | Œº, Œ£) = (1 / ((2œÄ)^(d/2) |Œ£|^(1/2))) ¬∑ exp(-¬Ω (x-Œº)·µÄ Œ£‚Åª¬π (x-Œº))
```

**Los tres par√°metros que describe cada gaussiana:**

- `Œº‚Çñ` ‚Äî el centro del cluster (vector de medias): equivalente al centroide de K-Means.
- `Œ£‚Çñ` ‚Äî la matriz de covarianza: describe la forma y orientaci√≥n del cluster. Una diagonal grande ‚Üí cluster alargado. Un t√©rmino off-diagonal ‚Üí cluster rotado.
- `œÄ‚Çñ` ‚Äî el peso: qu√© fracci√≥n del dataset pertenece a este componente.

**Tipos de covarianza en scikit-learn (par√°metro `covariance_type`):**

| Tipo | Descripci√≥n | Par√°metros | Cu√°ndo |
|---|---|---|---|
| `'full'` | Cada cluster tiene su propia matriz de covarianza completa | K¬∑d¬≤/2 | Clusters con forma y orientaci√≥n distintas |
| `'tied'` | Todos los clusters comparten la misma covarianza | d¬≤/2 | Clusters con misma forma, distintos centros |
| `'diag'` | Covarianzas diagonales (sin correlaci√≥n entre features) | K¬∑d | Clusters el√≠pticos alineados con los ejes |
| `'spherical'` | Una varianza por cluster, clusters esf√©ricos | K | Equivalente suave a K-Means |

**Script de explicaci√≥n:**

*"Hay un par√°metro que os cambia por completo el comportamiento del GMM: `covariance_type`. Con `full` cada cluster puede tener la elipse que quiera ‚Äîrotada, alargada, aplastada‚Äî. Con `spherical` cada cluster es un c√≠rculo de tama√±o distinto, que es b√°sicamente K-Means con asignaci√≥n suave. La elecci√≥n depende de cu√°ntos datos ten√©is y de lo compleja que sea la forma de vuestros clusters."*

---

### [00:22 ‚Äì 00:30] El algoritmo EM: Expectation-Maximization

**EM es el algoritmo de optimizaci√≥n que entrena el GMM. La analog√≠a con K-Means es directa.**

**El problema de optimizaci√≥n:**

Queremos encontrar los par√°metros `Œ∏ = {œÄ‚Çñ, Œº‚Çñ, Œ£‚Çñ}` que maximizan la log-verosimilitud de los datos:

```
log L(Œ∏) = Œ£·µ¢ log p(x·µ¢ | Œ∏) = Œ£·µ¢ log (Œ£‚Çñ œÄ‚Çñ ¬∑ ùí©(x·µ¢ | Œº‚Çñ, Œ£‚Çñ))
```

El problema es que la suma dentro del logaritmo hace la optimizaci√≥n directa intratable.

**La soluci√≥n EM ‚Äî dos pasos que se alternan:**

**Paso E (Expectation):** Dada la estimaci√≥n actual de los par√°metros `Œ∏`, calcula la **responsabilidad** `r·µ¢‚Çñ`: la probabilidad posterior de que el punto `x·µ¢` haya sido generado por el componente `k`.

```
r·µ¢‚Çñ = P(k | x·µ¢, Œ∏) = (œÄ‚Çñ ¬∑ ùí©(x·µ¢ | Œº‚Çñ, Œ£‚Çñ)) / Œ£‚±º (œÄ‚±º ¬∑ ùí©(x·µ¢ | Œº‚±º, Œ£‚±º))
```

Cada punto `x·µ¢` recibe un vector de K responsabilidades que suman 1: `[r·µ¢‚ÇÅ, r·µ¢‚ÇÇ, ..., r·µ¢‚Çñ]`. Esta es la **asignaci√≥n suave** (soft assignment). Comparad con K-Means donde el punto se asignaba a un solo cluster con responsabilidad 1.

**Paso M (Maximization):** Dadas las responsabilidades, actualiza los par√°metros maximizando la verosimilitud esperada:

```
œÄ‚Çñ_nuevo  = (1/n) Œ£·µ¢ r·µ¢‚Çñ                                  ‚Üê peso = fracci√≥n efectiva del cluster
Œº‚Çñ_nuevo  = Œ£·µ¢ r·µ¢‚Çñ ¬∑ x·µ¢ / Œ£·µ¢ r·µ¢‚Çñ                         ‚Üê media ponderada
Œ£‚Çñ_nuevo  = Œ£·µ¢ r·µ¢‚Çñ ¬∑ (x·µ¢ - Œº‚Çñ)(x·µ¢ - Œº‚Çñ)·µÄ / Œ£·µ¢ r·µ¢‚Çñ       ‚Üê covarianza ponderada
```

**Convergencia:** Se repite E-M hasta que la log-verosimilitud deja de crecer (o el cambio es menor que un umbral `tol`). Garantiza convergencia a un m√°ximo local, no global.

**Analog√≠a con K-Means:**

| K-Means | GMM + EM |
|---|---|
| Inicializaci√≥n con K-Means++ | Inicializaci√≥n por defecto con K-Means |
| Paso Asignaci√≥n: distancia al centroide m√°s cercano (duro) | Paso E: responsabilidades probabil√≠sticas (suave) |
| Paso Actualizaci√≥n: media aritm√©tica | Paso M: media ponderada, covarianza ponderada |
| Objetivo: minimizar WCSS | Objetivo: maximizar log-verosimilitud |
| Resultado: etiquetas duras | Resultado: probabilidades de pertenencia |

*"EM es K-Means con asignaciones suaves y forma el√≠ptica. Si hac√©is `covariance_type='spherical'` y aplic√°is argmax a las responsabilidades, obten√©is exactamente K-Means."*

---

### [00:30 ‚Äì 00:35] Selecci√≥n del n√∫mero de componentes: BIC y AIC

**El problema:**

Al igual que K-Means necesita elegir `k`, GMM necesita elegir el n√∫mero de componentes `K`. Pero en GMM tenemos una ventaja: la verosimilitud es una funci√≥n de coste natural que podemos comparar.

**El riesgo del sobreajuste:**

Con K = n (un componente por punto), la log-verosimilitud es m√°xima pero el modelo no generaliza ‚Äîha memorizando los datos. Necesitamos penalizar la complejidad.

**AIC (Akaike Information Criterion):**
```
AIC = 2¬∑p ‚àí 2¬∑log L
```
donde `p` es el n√∫mero de par√°metros libres del modelo. Menor AIC = mejor.

**BIC (Bayesian Information Criterion):**
```
BIC = p¬∑log(n) ‚àí 2¬∑log L
```
BIC penaliza m√°s fuerte que AIC porque el factor `log(n)` crece con el tama√±o del dataset. Para datasets grandes, BIC tiende a seleccionar modelos m√°s simples (menos componentes).

**Regla pr√°ctica:**
- Usar BIC como criterio principal cuando el dataset es grande (n > 500).
- Usar AIC cuando se prefiere capturar m√°s estructura aunque el modelo sea m√°s complejo.
- Cuando BIC y AIC coinciden, hay consenso claro.
- Cuando difieren, usar el criterio de negocio: ¬øcu√°ntos segmentos accionables puede gestionar el equipo?

---

## PARTE PR√ÅCTICA ‚Äî Jupyter Notebook (35 min)

---

### [00:35 ‚Äì 01:10] Pr√°ctica guiada

---

#### Celda 1 ‚Äî Imports

```python
# ============================================================
# BLOQUE 2.1 ‚Äî Gaussian Mixture Models y el Algoritmo EM
# ============================================================

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import matplotlib.patches as mpatches
from matplotlib.patches import Ellipse
import seaborn as sns

from sklearn.mixture import GaussianMixture
from sklearn.preprocessing import StandardScaler
from sklearn.datasets import make_blobs

plt.rcParams['figure.figsize'] = (11, 6)
plt.rcParams['font.size'] = 12
sns.set_style("whitegrid")
np.random.seed(42)

print("‚úì Imports correctos")
```

---

#### Celda 2 ‚Äî Visualizaci√≥n intuitiva: asignaci√≥n dura vs. suave

```python
# -------------------------------------------------------
# K-Means (duro) vs. GMM (suave) en el mismo dataset
# -------------------------------------------------------

from sklearn.cluster import KMeans

# Dataset con zona de solapamiento entre dos clusters
np.random.seed(3)
X_overlap = np.vstack([
    np.random.multivariate_normal([0, 0], [[1.5, 0.8],[0.8, 0.6]], 200),
    np.random.multivariate_normal([3, 2], [[1.0, -0.5],[-0.5, 0.8]], 200),
])

X_norm = StandardScaler().fit_transform(X_overlap)

# K-Means
km = KMeans(n_clusters=2, n_init=10, random_state=0)
labels_km = km.fit_predict(X_norm)

# GMM
gmm = GaussianMixture(n_components=2, covariance_type='full',
                      n_init=5, random_state=0)
gmm.fit(X_norm)
labels_gmm   = gmm.predict(X_norm)
proba_gmm    = gmm.predict_proba(X_norm)  # probabilidades de pertenencia
incertidumbre = 1 - proba_gmm.max(axis=1)  # 0 = seguro, 0.5 = m√°xima incertidumbre

fig, axes = plt.subplots(1, 3, figsize=(16, 5))

# K-Means: asignaci√≥n dura
ax = axes[0]
ax.scatter(X_norm[:, 0], X_norm[:, 1], c=labels_km,
           cmap='bwr', alpha=0.6, s=25)
ax.scatter(km.cluster_centers_[:, 0], km.cluster_centers_[:, 1],
           c='black', marker='X', s=200, zorder=5, label='Centroides')
ax.set_title("K-Means ‚Äî Asignaci√≥n dura\n(cada punto = un color, sin matices)",
             fontsize=10, fontweight='bold')
ax.legend(fontsize=9)

# GMM: probabilidad de pertenencia al componente 0
ax = axes[1]
sc = ax.scatter(X_norm[:, 0], X_norm[:, 1],
                c=proba_gmm[:, 0], cmap='RdBu', alpha=0.8, s=25,
                vmin=0, vmax=1)
plt.colorbar(sc, ax=ax, label='P(componente 0 | x)')
ax.set_title("GMM ‚Äî Probabilidad de pertenencia\n(gradiente = incertidumbre)",
             fontsize=10, fontweight='bold')

# GMM: incertidumbre (zona de frontera)
ax = axes[2]
sc2 = ax.scatter(X_norm[:, 0], X_norm[:, 1],
                 c=incertidumbre, cmap='hot_r', alpha=0.8, s=25,
                 vmin=0, vmax=0.5)
plt.colorbar(sc2, ax=ax, label='Incertidumbre (0=seguro, 0.5=m√°x)')
ax.set_title("GMM ‚Äî Mapa de incertidumbre\n(rojo = zona de frontera ambigua)",
             fontsize=10, fontweight='bold')

plt.suptitle("K-Means vs. GMM: asignaci√≥n dura vs. asignaci√≥n probabil√≠stica",
             fontsize=13, fontweight='bold')
plt.tight_layout()
plt.savefig("img_gmm_vs_kmeans_soft.png", dpi=150, bbox_inches='tight')
plt.show()

print(f"Puntos con incertidumbre > 0.3: {(incertidumbre > 0.3).sum()} "
      f"({(incertidumbre > 0.3).mean()*100:.1f}%)")
print("‚Üí Estos son los puntos 'frontera' que K-Means clasifica con falsa certeza.")
```

**Script de explicaci√≥n:**

*"La imagen central es la clave: el gradiente de color muestra la probabilidad de pertenecer al componente azul. Los puntos totalmente rojos son con certeza del componente rojo; los totalmente azules, del azul. Pero hay una zona intermedia donde los puntos son violetas ‚Äîpertenecen a ambos en distintas proporciones. Ese gradiente es informaci√≥n que K-Means descarta completamente."*

*"El tercer gr√°fico muestra la incertidumbre: los puntos m√°s calientes son los m√°s ambiguos. En un proyecto real, esos son los clientes 'en la frontera' entre dos segmentos ‚Äîlos m√°s interesantes para estrategias de cross-selling o para campa√±as de reactivaci√≥n."*

---

#### Celda 3 ‚Äî Visualizaci√≥n de las elipses de covarianza

```python
def plot_elipses_gmm(gmm, ax, n_std=2.0, alpha=0.25, colores=None):
    """
    Dibuja las elipses de covarianza de un GMM entrenado.
    n_std: n√∫mero de desviaciones est√°ndar para el radio de la elipse.
    """
    if colores is None:
        colores = plt.cm.tab10(np.linspace(0, 0.5, gmm.n_components))

    for k, (mean, cov, color) in enumerate(
        zip(gmm.means_, gmm.covariances_, colores)
    ):
        # Descomposici√≥n propia para obtener ejes y √°ngulo
        if gmm.covariance_type == 'full':
            cov_2d = cov
        elif gmm.covariance_type == 'diag':
            cov_2d = np.diag(cov)
        elif gmm.covariance_type in ('spherical', 'tied'):
            cov_2d = np.eye(2) * (cov if gmm.covariance_type == 'spherical'
                                   else cov[0, 0])
        else:
            cov_2d = cov

        vals, vecs = np.linalg.eigh(cov_2d[:2, :2])
        angle = np.degrees(np.arctan2(*vecs[:, 0][::-1]))
        width, height = 2 * n_std * np.sqrt(np.abs(vals))

        elipse = Ellipse(
            xy=mean[:2], width=width, height=height, angle=angle,
            edgecolor=color, facecolor=color, alpha=alpha, linewidth=2
        )
        ax.add_patch(elipse)
        ax.scatter(*mean[:2], c=[color], s=150, marker='X',
                   zorder=5, edgecolors='black', linewidths=1)


# Comparaci√≥n de tipos de covarianza
fig, axes = plt.subplots(1, 4, figsize=(18, 5))

cov_types = ['full', 'tied', 'diag', 'spherical']
titulos   = [
    "full\n(elipses libres por cluster)",
    "tied\n(misma forma, distintos centros)",
    "diag\n(ejes alineados, sin rotaci√≥n)",
    "spherical\n(c√≠rculos, similar a K-Means)"
]

# Dataset con clusters de distinta forma
np.random.seed(7)
X_elip = np.vstack([
    np.random.multivariate_normal([-2, 0], [[2.0, 1.2],[1.2, 0.4]], 150),
    np.random.multivariate_normal([2,  1], [[0.5, -0.3],[-0.3, 1.5]], 150),
    np.random.multivariate_normal([0, -3], [[0.3, 0],[0, 0.3]], 100),
])
X_elip_norm = StandardScaler().fit_transform(X_elip)

colores_elip = ['#e41a1c','#377eb8','#4daf4a']

for ax, ctype, titulo in zip(axes, cov_types, titulos):
    gmm_c = GaussianMixture(n_components=3, covariance_type=ctype,
                             n_init=5, random_state=0)
    gmm_c.fit(X_elip_norm)
    labels_c = gmm_c.predict(X_elip_norm)

    ax.scatter(X_elip_norm[:, 0], X_elip_norm[:, 1],
               c=labels_c, cmap='tab10', alpha=0.5, s=20)
    plot_elipses_gmm(gmm_c, ax, colores=colores_elip)
    ax.set_title(f"covariance_type='{ctype}'\n{titulo}", fontsize=9, fontweight='bold')
    ax.set_xlim(-3, 3)
    ax.set_ylim(-3, 3)

plt.suptitle("Impacto de covariance_type en las formas de los clusters",
             fontsize=13, fontweight='bold')
plt.tight_layout()
plt.savefig("img_gmm_covariance_types.png", dpi=150, bbox_inches='tight')
plt.show()
```

**Script de explicaci√≥n:**

*"Cada panel muestra el mismo dataset con un tipo de covarianza distinto. Con `full` cada cluster puede ser la elipse que le corresponde ‚Äîlibre en forma, tama√±o y orientaci√≥n‚Äî. Con `spherical` los clusters son c√≠rculos: si lo record√°is, eso es b√°sicamente K-Means suave. La elecci√≥n de `covariance_type` afecta profundamente la soluci√≥n, y tambi√©n el n√∫mero de par√°metros que hay que estimar ‚Äîm√°s par√°metros requieren m√°s datos."*

---

#### Celda 4 ‚Äî Selecci√≥n de K con BIC y AIC

```python
# -------------------------------------------------------
# Curvas BIC y AIC para elegir el n√∫mero de componentes
# -------------------------------------------------------

# Dataset de churn de telecomunicaciones (sint√©tico)
np.random.seed(0)
n = 500

# Simulamos 4 perfiles distintos de cliente de telecom
antiguedad = np.concatenate([
    np.random.normal(24,  6, 120),   # clientes nuevos
    np.random.normal(48, 10, 150),   # clientes medios
    np.random.normal(72, 12, 130),   # clientes veteranos no fieles
    np.random.normal(60,  8, 100),   # clientes veteranos fieles
])
llamadas = np.concatenate([
    np.random.normal(150, 30, 120),
    np.random.normal(200, 40, 150),
    np.random.normal(80,  20, 130),
    np.random.normal(300, 35, 100),
])
factura = np.concatenate([
    np.random.normal(30, 8,  120),
    np.random.normal(55, 12, 150),
    np.random.normal(40, 10, 130),
    np.random.normal(90, 15, 100),
])
churn_prob = np.concatenate([
    np.random.beta(3, 2, 120),
    np.random.beta(2, 4, 150),
    np.random.beta(5, 2, 130),
    np.random.beta(1, 6, 100),
])

df_telecom = pd.DataFrame({
    'antiguedad_meses': antiguedad,
    'llamadas_mes':     llamadas,
    'factura_media':    factura,
    'prob_churn':       churn_prob,
})
df_telecom = df_telecom.clip(lower=0)

X_tel = StandardScaler().fit_transform(df_telecom)

# Calculamos BIC y AIC para K = 1..10
ks_range = range(1, 11)
bic_vals, aic_vals, ll_vals = [], [], []

for k in ks_range:
    gmm_k = GaussianMixture(n_components=k, covariance_type='full',
                             n_init=5, random_state=42)
    gmm_k.fit(X_tel)
    bic_vals.append(gmm_k.bic(X_tel))
    aic_vals.append(gmm_k.aic(X_tel))
    ll_vals.append(gmm_k.score(X_tel))  # log-verosimilitud media

# Visualizaci√≥n
fig, axes = plt.subplots(1, 2, figsize=(14, 5))

# BIC y AIC
ax1 = axes[0]
ax1.plot(ks_range, bic_vals, 'bs-', linewidth=2, markersize=8, label='BIC')
ax1.plot(ks_range, aic_vals, 'r^--', linewidth=2, markersize=8, label='AIC')
k_bic = np.argmin(bic_vals) + 1
k_aic = np.argmin(aic_vals) + 1
ax1.axvline(x=k_bic, color='blue', linestyle=':', alpha=0.7,
            label=f'M√≠n. BIC ‚Üí k={k_bic}')
ax1.axvline(x=k_aic, color='red', linestyle=':', alpha=0.7,
            label=f'M√≠n. AIC ‚Üí k={k_aic}')
ax1.set_xlabel("N√∫mero de componentes (K)", fontsize=11)
ax1.set_ylabel("Criterio de informaci√≥n (menor = mejor)", fontsize=11)
ax1.set_title("BIC y AIC para seleccionar K\n(Dataset Telecom Churn)",
              fontsize=11, fontweight='bold')
ax1.legend(fontsize=10)
ax1.set_xticks(ks_range)

# Log-verosimilitud
ax2 = axes[1]
ax2.plot(ks_range, ll_vals, 'go-', linewidth=2, markersize=8)
ax2.set_xlabel("N√∫mero de componentes (K)", fontsize=11)
ax2.set_ylabel("Log-verosimilitud media", fontsize=11)
ax2.set_title("Log-verosimilitud vs. K\n(siempre crece ‚Äî no sirve sola)",
              fontsize=11, fontweight='bold')
ax2.set_xticks(ks_range)

plt.suptitle("Selecci√≥n del n√∫mero de componentes GMM",
             fontsize=13, fontweight='bold')
plt.tight_layout()
plt.savefig("img_gmm_bic_aic.png", dpi=150, bbox_inches='tight')
plt.show()

print(f"K √≥ptimo seg√∫n BIC: {k_bic}")
print(f"K √≥ptimo seg√∫n AIC: {k_aic}")
```

**Script de explicaci√≥n:**

*"El gr√°fico de la derecha ilustra el problema central: la log-verosimilitud siempre crece al a√±adir componentes. Si la us√°semos sola, siempre elegir√≠amos K = n. Por eso necesitamos BIC y AIC: penalizan el n√∫mero de par√°metros. La curva del BIC tiene un m√≠nimo claro ‚Äîah√≠ est√° el K √≥ptimo seg√∫n BIC. Si BIC y AIC coinciden, es un resultado robusto."*

---

#### Celda 5 ‚Äî GMM entrenado y perfilado de segmentos

```python
# Entrenamos el GMM final con el K elegido por BIC
k_final = k_bic
gmm_final = GaussianMixture(n_components=k_final, covariance_type='full',
                             n_init=10, random_state=42)
gmm_final.fit(X_tel)

df_telecom['cluster_gmm'] = gmm_final.predict(X_tel)
proba_final = gmm_final.predict_proba(X_tel)

# Perfil de cada componente
print("Perfil medio de cada componente GMM:")
perfil_gmm = df_telecom.groupby('cluster_gmm')[df_telecom.columns[:-1]].mean().round(1)
perfil_gmm['peso (%)'] = (
    df_telecom['cluster_gmm'].value_counts(normalize=True) * 100
).sort_index().round(1)
print(perfil_gmm)

# Visualizaci√≥n: probabilidades de los 5 puntos m√°s ambiguos
incert = 1 - proba_final.max(axis=1)
top_ambiguos = np.argsort(incert)[-5:][::-1]
print("\nLos 5 clientes m√°s ambiguos (mayor incertidumbre):")
df_ambiguos = pd.DataFrame(
    proba_final[top_ambiguos],
    columns=[f'P(cluster {k})' for k in range(k_final)],
    index=[f'Cliente {i}' for i in top_ambiguos]
).round(3)
print(df_ambiguos)
print("\n‚Üí Estos clientes no pertenecen claramente a ning√∫n segmento.")
print("  Son candidatos a campa√±as de 'definici√≥n de perfil' (encuestas, A/B tests).")
```

---

#### Celda 6 ‚Äî Visualizaci√≥n del resultado con elipses

```python
# Proyecci√≥n 2D para visualizar (usamos las dos primeras features)
fig, axes = plt.subplots(1, 2, figsize=(14, 6))

colores_gmm = ['#e41a1c','#377eb8','#4daf4a','#ff7f00']

# Panel izquierdo: scatter con asignaci√≥n hard
ax1 = axes[0]
for c in range(k_final):
    mask = df_telecom['cluster_gmm'] == c
    ax1.scatter(
        df_telecom.loc[mask, 'antiguedad_meses'],
        df_telecom.loc[mask, 'factura_media'],
        c=colores_gmm[c % len(colores_gmm)], alpha=0.5, s=30,
        label=f'Componente {c} (n={mask.sum()})'
    )
ax1.set_xlabel("Antig√ºedad (meses)")
ax1.set_ylabel("Factura media (‚Ç¨)")
ax1.set_title(f"GMM k={k_final} ‚Äî Asignaci√≥n hard\n(argmax de probabilidades)",
              fontsize=10, fontweight='bold')
ax1.legend(fontsize=9)

# Panel derecho: incertidumbre
ax2 = axes[1]
sc = ax2.scatter(
    df_telecom['antiguedad_meses'],
    df_telecom['factura_media'],
    c=incert, cmap='YlOrRd', s=30, alpha=0.8
)
plt.colorbar(sc, ax=ax2, label='Incertidumbre de asignaci√≥n')
ax2.set_xlabel("Antig√ºedad (meses)")
ax2.set_ylabel("Factura media (‚Ç¨)")
ax2.set_title("Mapa de incertidumbre\n(amarillo = seguro, rojo = ambiguo)",
              fontsize=10, fontweight='bold')

plt.suptitle("GMM aplicado a segmentaci√≥n de clientes de telecom",
             fontsize=13, fontweight='bold')
plt.tight_layout()
plt.savefig("img_gmm_telecom_resultado.png", dpi=150, bbox_inches='tight')
plt.show()
```

---

#### Celda 7 ‚Äî Interpretaci√≥n de negocio de los segmentos

```python
# Nomenclatura de los segmentos basada en el perfil medio
nombres_segmento = {
    0: "Clientes nuevos de bajo valor",
    1: "Clientes consolidados activos",
    2: "Clientes veteranos en riesgo",
    3: "Clientes VIP fieles",
}

acciones = {
    0: "Onboarding mejorado, ofertas de bienvenida",
    1: "Cross-selling de productos premium",
    2: "Programa de retenci√≥n urgente, llamada proactiva",
    3: "Programa de fidelidad exclusivo, upselling",
}

print("=" * 60)
print("INTERPRETACI√ìN DE NEGOCIO ‚Äî GMM Segmentaci√≥n Telecom")
print("=" * 60)
for c in range(k_final):
    n_seg = (df_telecom['cluster_gmm'] == c).sum()
    pct   = n_seg / len(df_telecom) * 100
    print(f"\nComponente {c}: '{nombres_segmento.get(c, 'Por definir')}'")
    print(f"  Tama√±o: {n_seg} clientes ({pct:.1f}%)")
    print(f"  Acci√≥n: {acciones.get(c, 'Pendiente de definir')}")

print("\nVentaja del GMM sobre K-Means:")
print("  Los clientes ambiguos no reciben una etiqueta forzada.")
print("  Se pueden tratar con estrategias mixtas o como prioridad de an√°lisis.")
```

---

## NOTAS DE PRODUCCI√ìN

### Para las slides

- **Slide 1:** Portada. Pregunta: *"¬øUn cliente pertenece al 100% a un √∫nico segmento?"*
- **Slide 2:** Clustering duro vs. suave ‚Äî diagrama con la misma frontera vista con K-Means (l√≠nea dura) y GMM (gradiente de probabilidad).
- **Slide 3:** F√≥rmula `p(x) = Œ£ œÄ‚Çñ ùí©(x|Œº‚Çñ,Œ£‚Çñ)` descompuesta visualmente: tres gaussianas coloreadas que se suman.
- **Slide 4:** El algoritmo EM ‚Äî tabla comparativa con K-Means, paso a paso.
- **Slide 5:** Los cuatro tipos de covarianza ‚Äî los cuatro paneles de la Celda 3.
- **Slide 6:** BIC y AIC ‚Äî gr√°fica con los m√≠nimos se√±alados y la explicaci√≥n de la penalizaci√≥n.
- **Slide 7:** Tabla comparativa K-Means vs. GMM (cu√°ndo usar cada uno).

### Para el handout

- Tabla comparativa K-Means vs. GMM con f√≥rmulas del paso E y paso M.
- Tabla de `covariance_type`: descripci√≥n, par√°metros, cu√°ndo usar.
- Los gr√°ficos de elipses de covarianza (Celda 3).
- El mapa de incertidumbre (Celda 2 y Celda 6) con gu√≠a de interpretaci√≥n.
- Gu√≠a de decisi√≥n BIC vs. AIC.

### Para el Jupyter Notebook (ejercicios a completar)

**Ejercicio 1:** Aplicar GMM con los cuatro tipos de covarianza al dataset de pa√≠ses del Bloque 1.3. ¬øCu√°l produce clusters m√°s interpretables? ¬øCu√°l minimiza el BIC?

**Ejercicio 2:** Para el dataset de telecom, a√±adir la columna `probabilidad_maxima` al DataFrame y filtrar los clientes con `max_prob < 0.6`. ¬øCu√°ntos son? ¬øA qu√© cluster pertenecen mayoritariamente?

**Ejercicio 3 (avanzado):** Implementar una iteraci√≥n del algoritmo EM manualmente: dado un GMM ya inicializado con `gmm.fit()`, programar el paso E (responsabilidades) usando NumPy y verificar que coincide con `gmm.predict_proba()`.

---

## GESTI√ìN DEL TIEMPO

| Segmento | Duraci√≥n | Indicador |
|---|---|---|
| Apertura Sesi√≥n 2 + recapitulaci√≥n | 6 min | Preguntas respondidas |
| Limitaciones del clustering duro | 8 min | Ejemplo altura/peso en pantalla |
| El modelo GMM (f√≥rmula + par√°metros) | 8 min | F√≥rmula descompuesta en pantalla |
| El algoritmo EM (pasos E y M) | 9 min | Tabla comparativa con K-Means |
| BIC y AIC | 4 min | F√≥rmulas en pantalla |
| Celda 1-2 (imports + soft vs. hard) | 8 min | Mapa de incertidumbre generado |
| Celda 3 (elipses de covarianza) | 7 min | Los 4 paneles generados |
| Celda 4 (BIC y AIC) | 7 min | K √≥ptimo identificado |
| Celda 5-7 (telecom + interpretaci√≥n) | 13 min | Tabla de negocio impresa |
| **Total** | **70 min** | |

---

*Bloque 2.1 desarrollado para el m√≥dulo "Algoritmos de Clustering" ‚Äî M√°ster en Ciencia de Datos*
