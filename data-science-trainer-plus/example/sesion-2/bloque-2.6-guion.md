# Bloque 2.6 â€” Buenas PrÃ¡cticas, Escalabilidad y Cierre del MÃ³dulo
**SesiÃ³n 2 | DuraciÃ³n: 15 minutos**

---

## Tabla de tiempos

| Segmento | Contenido | Tiempo |
|---|---|---|
| Parte 1 | Errores frecuentes y cÃ³mo evitarlos | 5 min |
| Parte 2 | Escalabilidad: Â¿quÃ© usar con datos grandes? | 3 min |
| Parte 3 | Mapa de decisiÃ³n final | 3 min |
| Parte 4 | Cierre, recursos y prÃ³ximos pasos | 4 min |
| **Total** | | **15 min** |

---

## Parte 1 â€” Errores frecuentes y cÃ³mo evitarlos (5 min)

### GuiÃ³n del instructor

> "Antes de cerrar, quiero compartir un inventario de los errores mÃ¡s comunes que se cometen en proyectos reales de clustering. Los he visto en proyectos de consultorÃ­a y en trabajos de fin de mÃ¡ster. Saber cuÃ¡les son ya es evitar la mitad."

### Celda 1 â€” Checklist de errores frecuentes (ejecutable como recordatorio)

```python
# ============================================================
# CHECKLIST DE BUENAS PRÃCTICAS EN CLUSTERING
# Guarda este notebook como referencia para proyectos reales
# ============================================================

checklist = {
    "PREPROCESADO": [
        ("âœ…", "Estandarizar siempre antes de clustering (media 0, std 1)"),
        ("âœ…", "Revisar outliers ANTES de clustering, no despuÃ©s"),
        ("âœ…", "Aplicar log-transform a variables monetarias/de conteo con cola larga"),
        ("âœ…", "Comprobar multicolinealidad â€” features correladas distorsionan las distancias"),
        ("âŒ", "NUNCA incluir el target (si existe) en las features de clustering"),
        ("âŒ", "NUNCA imputar nulos con la media si hay muchos â€” pueden crear un cluster espurio"),
    ],
    "SELECCIÃ“N DE ALGORITMO": [
        ("âœ…", "Probar al menos 2-3 algoritmos antes de decidir"),
        ("âœ…", "Usar siempre mÃºltiples mÃ©tricas (no solo Silhouette)"),
        ("âœ…", "Considerar la interpretabilidad del negocio al elegir k"),
        ("âŒ", "NUNCA elegir k basÃ¡ndose en una sola mÃ©trica ni por defecto (k=3 o k=5)"),
        ("âŒ", "NUNCA usar K-Means si los clusters no son convexos o hay ruido significativo"),
        ("âŒ", "NUNCA usar DBSCAN sin explorar el k-distance graph para calibrar Îµ"),
    ],
    "INTERPRETACIÃ“N": [
        ("âœ…", "Asignar nombres de negocio a cada cluster â€” los nÃºmeros no comunican"),
        ("âœ…", "Validar con expertos de dominio que los segmentos tienen sentido"),
        ("âœ…", "Comprobar estabilidad: re-ejecutar con distintas semillas"),
        ("âœ…", "Verificar que cada cluster es accionable (se puede hacer algo diferente con Ã©l)"),
        ("âŒ", "NUNCA presentar el clustering como 'la verdad' â€” es una aproximaciÃ³n"),
        ("âŒ", "NUNCA confundir el score de Silhouette alto con 'el clustering es correcto'"),
    ],
    "VISUALIZACIÃ“N": [
        ("âœ…", "Usar PCA para preprocesar, t-SNE/UMAP solo para visualizar"),
        ("âœ…", "Aclarar en cualquier grÃ¡fico t-SNE que las distancias entre clusters NO son interpretables"),
        ("âœ…", "Incluir siempre el tamaÃ±o de cada cluster en los grÃ¡ficos"),
        ("âŒ", "NUNCA sacar conclusiones sobre relaciones entre clusters a partir de t-SNE"),
    ],
    "PRODUCCIÃ“N": [
        ("âœ…", "Guardar el scaler y el modelo entrenado (pickle/joblib) para predecir nuevos clientes"),
        ("âœ…", "Documentar el pipeline completo (parÃ¡metros, versiones de librerÃ­as)"),
        ("âœ…", "Establecer un proceso de re-entrenamiento periÃ³dico"),
        ("âŒ", "NUNCA asumir que el clustering de hace 6 meses es vÃ¡lido hoy sin validarlo"),
    ]
}

for categoria, items in checklist.items():
    print(f"\n{'='*55}")
    print(f"  {categoria}")
    print(f"{'='*55}")
    for simbolo, texto in items:
        print(f"  {simbolo}  {texto}")

print("\n\nğŸ’¾ Guarda este notebook â€” es tu referencia para proyectos reales.")
```

---

## Parte 2 â€” Escalabilidad: Â¿quÃ© usar con datos grandes? (3 min)

### GuiÃ³n del instructor

> "Todo lo que hemos visto funciona bien hasta ~50.000-100.000 registros. En proyectos con millones de clientes hay que hacer ajustes."

### Tabla de escalabilidad (para la presentaciÃ³n)

| Algoritmo | Complejidad | Escala a 1M+ | Alternativa escalable |
|---|---|---|---|
| K-Means | O(nÂ·kÂ·iter) | âœ… Con `MiniBatchKMeans` | `MiniBatchKMeans` (sklearn) |
| K-Medoids (PAM) | O(nÂ²) | âŒ | CLARA, CLARANS |
| JerÃ¡rquico | O(nÂ² log n) â€“ O(nÂ³) | âŒ | Bisecting K-Means |
| DBSCAN | O(n log n) con Ã­ndice | âš ï¸ Con HDBSCAN | HDBSCAN (hdbscan library) |
| GMM (EM) | O(nÂ·kÂ·dÂ²Â·iter) | âš ï¸ Con muchos features | Variational Bayes GMM |
| SOM | O(nÂ·mÂ·iter) | âœ… Con m moderado | `minisom` + submuestras |

```python
# Demo rÃ¡pida: MiniBatchKMeans vs KMeans en tiempo
import time
from sklearn.cluster import MiniBatchKMeans
from sklearn.datasets import make_blobs
import numpy as np

# Dataset grande simulado
X_grande, _ = make_blobs(n_samples=200_000, n_features=10,
                         centers=6, random_state=42)

# KMeans estÃ¡ndar
t0 = time.time()
km = KMeans(n_clusters=6, random_state=42, n_init=5, max_iter=100)
km.fit(X_grande)
t_km = time.time() - t0

# MiniBatchKMeans
t0 = time.time()
mbkm = MiniBatchKMeans(n_clusters=6, random_state=42, n_init=5,
                        batch_size=2048, max_iter=100)
mbkm.fit(X_grande)
t_mbkm = time.time() - t0

print(f"KMeans estÃ¡ndar (200k muestras):  {t_km:.2f}s")
print(f"MiniBatchKMeans (200k muestras):  {t_mbkm:.2f}s")
print(f"AceleraciÃ³n: {t_km/t_mbkm:.1f}x")

# Comparar inercia (aproximaciÃ³n de calidad)
print(f"\nKMeans inertia:          {km.inertia_:,.0f}")
print(f"MiniBatchKMeans inertia: {mbkm.inertia_:,.0f}")
print(f"Diferencia relativa: {abs(km.inertia_-mbkm.inertia_)/km.inertia_:.2%}")
```

---

## Parte 3 â€” Mapa de decisiÃ³n final (3 min)

### GuiÃ³n del instructor

> "Cierro con el Ã¡rbol de decisiÃ³n que podÃ©is usar en cualquier proyecto. No es un orÃ¡culo, pero es un punto de partida sÃ³lido."

### Celda 3 â€” Ãrbol de decisiÃ³n como texto ejecutable

```python
# ============================================================
# ÃRBOL DE DECISIÃ“N PARA SELECCIÃ“N DE ALGORITMO DE CLUSTERING
# ============================================================

arbol = """
Â¿CuÃ¡ntas muestras tengo?
â”‚
â”œâ”€â”€ < 10.000 â†’ Puedo usar cualquier algoritmo
â”‚
â””â”€â”€ > 100.000 â†’ Evitar: K-Medoids PAM, JerÃ¡rquico completo
               Preferir: MiniBatchKMeans, HDBSCAN, SOM con submuestra

Â¿Conozco el nÃºmero de clusters k de antemano?
â”‚
â”œâ”€â”€ SÃ­ â†’ K-Means / K-Medoids / GMM
â”‚
â””â”€â”€ No â†’ Explorar k con: Elbow + Silhouette + DBI
         O usar: DBSCAN / HDBSCAN (determinan k automÃ¡ticamente)

Â¿Esperan clusters esfÃ©ricos y bien separados?
â”‚
â”œâ”€â”€ SÃ­ â†’ K-Means (rÃ¡pido, interpretable)
â”‚
â””â”€â”€ No â†’ Â¿Hay ruido/outliers significativos?
          â”‚
          â”œâ”€â”€ SÃ­ â†’ DBSCAN / HDBSCAN
          â”‚
          â””â”€â”€ No â†’ Â¿Clusters con forma irregular pero sin ruido?
                    â”‚
                    â”œâ”€â”€ SÃ­ â†’ JerÃ¡rquico (Ward) o GMM
                    â””â”€â”€ No â†’ SOM (exploraciÃ³n topolÃ³gica)

Â¿Necesito probabilidades de pertenencia (soft assignment)?
â”‚
â”œâ”€â”€ SÃ­ â†’ GMM
â””â”€â”€ No â†’ K-Means, K-Medoids, JerÃ¡rquico, DBSCAN

Â¿Los outliers son datos importantes (no errores)?
â”‚
â”œâ”€â”€ SÃ­ (ej. detecciÃ³n de anomalÃ­as) â†’ DBSCAN / HDBSCAN
â””â”€â”€ No â†’ K-Means o K-Medoids (mÃ¡s robusto que K-Means)

Â¿Necesito visualizar relaciones topolÃ³gicas entre clusters?
â”‚
â””â”€â”€ SÃ­ â†’ SOM (U-Matrix, component planes)
"""

print(arbol)
```

---

## Parte 4 â€” Cierre, recursos y prÃ³ximos pasos (4 min)

### GuiÃ³n del instructor

> "Hemos cubierto en 10 horas lo que muchos proyectos de ciencia de datos requieren: desde los fundamentos matemÃ¡ticos hasta la puesta en producciÃ³n. Pero este es el punto de partida, no el final."

### Celda 4 â€” Resumen del mÃ³dulo y recursos

```python
resumen = """
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘          RESUMEN DEL MÃ“DULO â€” ALGORITMOS DE CLUSTERING          â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘                                                                  â•‘
â•‘  SESIÃ“N 1 (5h)                                                   â•‘
â•‘  â”œâ”€â”€ Bloque 1.1: Fundamentos y mÃ©tricas de distancia            â•‘
â•‘  â”œâ”€â”€ Bloque 1.2: K-Means y K-Medoids                            â•‘
â•‘  â”œâ”€â”€ Bloque 1.3: Clustering jerÃ¡rquico                          â•‘
â•‘  â””â”€â”€ Bloque 1.4: DBSCAN                                         â•‘
â•‘                                                                  â•‘
â•‘  SESIÃ“N 2 (5h)                                                   â•‘
â•‘  â”œâ”€â”€ Bloque 2.1: Modelos de mezcla gaussiana (GMM + EM)         â•‘
â•‘  â”œâ”€â”€ Bloque 2.2: Mapas auto-organizados de Kohonen (SOM)        â•‘
â•‘  â”œâ”€â”€ Bloque 2.3: MÃ©tricas de evaluaciÃ³n                         â•‘
â•‘  â”œâ”€â”€ Bloque 2.4: ReducciÃ³n de dimensionalidad                   â•‘
â•‘  â”œâ”€â”€ Bloque 2.5: Proyecto integrador (RFM e-commerce)           â•‘
â•‘  â””â”€â”€ Bloque 2.6: Buenas prÃ¡cticas y cierre â† aquÃ­ estamos      â•‘
â•‘                                                                  â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘  ALGORITMOS CUBIERTOS:                                           â•‘
â•‘   K-Means  Â·  K-Medoids  Â·  Hierarchical  Â·  DBSCAN             â•‘
â•‘   GMM (EM)  Â·  Kohonen SOM                                      â•‘
â•‘                                                                  â•‘
â•‘  MÃ‰TRICAS: Silhouette Â· DBI Â· CHI Â· WCSS Â· ARI Â· NMI            â•‘
â•‘                                                                  â•‘
â•‘  PREPROCESADO: Escala Â· Log-transform Â· PCA                     â•‘
â•‘  VISUALIZACIÃ“N: t-SNE Â· UMAP Â· Radar Â· Heatmap Â· U-Matrix       â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
"""
print(resumen)

recursos = """
ğŸ“š RECURSOS RECOMENDADOS

LIBROS
  â€¢ "The Elements of Statistical Learning" â€” Hastie, Tibshirani, Friedman
    Cap. 14: Unsupervised Learning (disponible free en web oficial)
  â€¢ "Pattern Recognition and Machine Learning" â€” Bishop
    Cap. 9: Mixture Models and EM
  â€¢ "Introduction to Machine Learning with Python" â€” MÃ¼ller & Guido
    (enfoque prÃ¡ctico, scikit-learn)

PAPERS CLAVE
  â€¢ Lloyd (1982) â€” "Least squares quantization in PCM" (K-Means original)
  â€¢ Arthur & Vassilvitskii (2007) â€” "k-means++: The advantages of careful seeding"
  â€¢ Ester et al. (1996) â€” "A density-based algorithm for discovering clusters" (DBSCAN)
  â€¢ Kohonen (1982) â€” "Self-organized formation of topologically correct feature maps"
  â€¢ McInnes et al. (2018) â€” "UMAP: Uniform Manifold Approximation and Projection" (arXiv)

DOCUMENTACIÃ“N
  â€¢ scikit-learn clustering: https://scikit-learn.org/stable/modules/clustering.html
  â€¢ minisom: https://github.com/JustGlowing/minisom
  â€¢ UMAP: https://umap-learn.readthedocs.io
  â€¢ HDBSCAN: https://hdbscan.readthedocs.io

DATASETS PARA PRACTICAR
  â€¢ UCI ML Repository â€” Online Retail, Customer Segmentation
  â€¢ Kaggle â€” Mall Customers, Credit Card Dataset, E-Commerce Data
  â€¢ scikit-learn â€” make_blobs, make_moons, make_circles (sintÃ©ticos)
"""
print(recursos)
```

### Celda 5 â€” PrÃ³ximos pasos (continuaciÃ³n del mÃ¡ster)

```python
proximos_pasos = """
ğŸ¯ PRÃ“XIMOS PASOS EN EL MÃSTER

Este mÃ³dulo conecta con:

  â†’ MÃ“DULO SIGUIENTE: DetecciÃ³n de anomalÃ­as
     Isolation Forest, LOF, Autoencoders
     (Clustering como baseline para anomaly detection)

  â†’ MÃ“DULO: Aprendizaje semi-supervisado
     Usar clustering para generar pseudo-labels
     Label propagation, self-training

  â†’ MÃ“DULO: Series temporales
     Clustering de series: DTW, k-Shape, temporal SOM
     SegmentaciÃ³n de comportamiento a lo largo del tiempo

  â†’ PROYECTO FINAL
     Aplicar el pipeline completo a datos reales de la empresa
     Deliverable: notebook documentado + presentaciÃ³n ejecutiva

ğŸ’¡ CONSEJO FINAL

El clustering es tanto ciencia como arte.
Las mÃ©tricas guÃ­an, pero el juicio del dominio decide.
El mejor clustering no es el que tiene el Silhouette mÃ¡s alto,
sino el que lleva a las mejores decisiones de negocio.
"""
print(proximos_pasos)
```

---

## Notas de producciÃ³n

### Para la presentaciÃ³n (slides)
- **Slide 1**: Checklist en 4 categorÃ­as â€” presentar como "errores que cuestan dinero"
- **Slide 2**: Tabla de escalabilidad â€” resaltar MiniBatchKMeans y HDBSCAN
- **Slide 3**: Ãrbol de decisiÃ³n como diagrama visual (no texto)
- **Slide 4**: Mapa visual de los 6 algoritmos con sus dominios de aplicaciÃ³n
- **Slide 5**: Lista de recursos + QR code al repositorio del curso
- **Slide 6** (Ãºltima): Frase de cierre â€” "El mejor clustering lleva a la mejor decisiÃ³n"

### Para el handout
- Checklist en una pÃ¡gina (A4 doble cara) para uso en proyectos
- Ãrbol de decisiÃ³n como diagrama (printable)
- Tabla comparativa final de los 6 algoritmos cubiertos
- Lista de recursos con URLs

### Encuesta de cierre (sugerida)
Pedir a los estudiantes que respondan en papel o digital:
1. Â¿QuÃ© algoritmo crees que usarÃ¡s mÃ¡s en tu trabajo?
2. Â¿QuÃ© concepto te ha costado mÃ¡s entender?
3. Â¿QuÃ© echas en falta en el mÃ³dulo?

---

## Tabla de tiempos (verificaciÃ³n)

| Segmento | DuraciÃ³n real |
|---|---|
| Parte 1: Errores frecuentes | 5 min |
| Parte 2: Escalabilidad | 3 min |
| Parte 3: Ãrbol de decisiÃ³n | 3 min |
| Parte 4: Cierre y recursos | 4 min |
| **Total** | **15 min** âœ“ |
